{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functorch succesfully imported\n"
     ]
    }
   ],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer\n",
    "from cfollmer.drifts import ResNetScoreNetwork, AbstractDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "# import sklearn.linear_model\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# mod = sklearn.neural_network.MLPClassifier((100,100),random_state=0).fit(X_train, y_train)\n",
    "# print(mod.score(X_train, y_train))\n",
    "# print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x, t):\n",
    "#         x = torch.cat((x, t), dim=-1)\n",
    "#         return self.nn(x)\n",
    "\n",
    "\n",
    "# class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "#     def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "#         super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "#         self.nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + 1, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, input_dim )\n",
    "#         )\n",
    "        \n",
    "#         self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "##     def forward(self, x):\n",
    "##         return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "# net = ClassificationNetwork(\n",
    "#     dim,1, device=device, depth=2, width=100, activation=F.softplus\n",
    "# )\n",
    "\n",
    "\n",
    "net = LinearClassifier(\n",
    "    dim,1, device=device\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 123)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim,  X_train.shape[1] #, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cfollmer.layers import ResBlock, get_timestep_embedding\n",
    "\n",
    "# class ResNetScoreNetwork_(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  input_dim: int = 123,\n",
    "#                  pos_dim: int = 16,\n",
    "#                  res_dim: int = 650,\n",
    "#                  res_block_initial_widths=None,\n",
    "#                  res_block_final_widths=None,\n",
    "#                  res_block_inner_layers=None,\n",
    "#                  activation=torch.nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         if res_block_initial_widths is None:\n",
    "#             res_block_initial_widths = [res_dim, res_dim, res_dim,res_dim]\n",
    "#         if res_block_final_widths is None:\n",
    "#             res_block_final_widths = [res_dim, res_dim, res_dim, res_dim]\n",
    "#         if res_block_inner_layers is None:\n",
    "#             res_block_inner_layers = [128, 128]\n",
    "\n",
    "#         self.temb_dim = pos_dim\n",
    "\n",
    "#         # ResBlock Sequence\n",
    "#         res_layers = []\n",
    "#         initial_dim = input_dim\n",
    "#         for initial, final in zip(res_block_initial_widths, res_block_final_widths):\n",
    "#             res_layers.append(ResBlock(initial_dim, initial, final, res_block_inner_layers, activation))\n",
    "#             initial_dim = initial + final\n",
    "#         self.res_sequence = torch.nn.Sequential(*res_layers)\n",
    "\n",
    "#         # Time FCBlock\n",
    "#         self.time_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim, self.temb_dim * 2), activation)\n",
    "\n",
    "#         # Final_block\n",
    "#         self.final_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim * 2 + initial_dim, input_dim))\n",
    "\n",
    "#     def forward(self, x, t):\n",
    "#         # t needs the same shape as x (except for the final dim, which is 1)\n",
    "#         t_emb = get_timestep_embedding(t, self.temb_dim)\n",
    "#         t_emb = self.time_block(t_emb)\n",
    "#         x_emb = self.res_sequence(x)\n",
    "#         h = torch.cat([x_emb, t_emb], -1)\n",
    "#         return self.final_block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ResNetScoreNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79edcf75d78a452a8ad2f58c34e00e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2378: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::batch_norm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch.batch_norm(\n",
      "/tmp/ipykernel_1898/3495608044.py:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9016560912132263\n",
      "0.7858908772468567\n",
      "0.7955307364463806\n",
      "0.6245638728141785\n",
      "0.49218639731407166\n",
      "0.5208956599235535\n",
      "0.64518803358078\n",
      "0.503056526184082\n",
      "0.49570998549461365\n",
      "0.4333386719226837\n",
      "0.4574110209941864\n",
      "0.449645072221756\n",
      "0.46681851148605347\n",
      "0.46798256039619446\n",
      "0.4521825313568115\n",
      "0.46005979180336\n",
      "0.4276396334171295\n",
      "0.43452852964401245\n",
      "0.45451417565345764\n",
      "0.4438866078853607\n",
      "0.44326794147491455\n",
      "0.44955554604530334\n",
      "0.421499103307724\n",
      "0.45189169049263\n",
      "0.4312673509120941\n",
      "0.4438117742538452\n",
      "0.41842103004455566\n",
      "0.41901183128356934\n",
      "0.42015302181243896\n",
      "0.42534106969833374\n",
      "0.41299861669540405\n",
      "0.4450279474258423\n",
      "0.4319508373737335\n",
      "0.41628560423851013\n",
      "0.4310545325279236\n",
      "0.42803049087524414\n",
      "0.4268939793109894\n",
      "0.4174824059009552\n",
      "0.45602527260780334\n",
      "0.43009093403816223\n",
      "0.43264931440353394\n",
      "0.4091968536376953\n",
      "0.44313010573387146\n",
      "0.45005524158477783\n",
      "0.40731415152549744\n",
      "0.40561431646347046\n",
      "0.4142289459705353\n",
      "0.41309916973114014\n",
      "0.42645812034606934\n",
      "0.4066483974456787\n",
      "0.41812485456466675\n",
      "0.40806445479393005\n",
      "0.4229845702648163\n",
      "0.4182996451854706\n",
      "0.43708571791648865\n",
      "0.42332205176353455\n",
      "0.4049936830997467\n",
      "0.40723368525505066\n",
      "0.4166981875896454\n",
      "0.42381641268730164\n",
      "0.4027542173862457\n",
      "0.41360896825790405\n",
      "0.4311537444591522\n",
      "0.40759679675102234\n",
      "0.4111192226409912\n",
      "0.40606966614723206\n",
      "0.39295586943626404\n",
      "0.4276207387447357\n",
      "0.40218740701675415\n",
      "0.4245770275592804\n",
      "0.42469602823257446\n",
      "0.41626647114753723\n",
      "0.4109540283679962\n",
      "0.412657767534256\n",
      "0.40242770314216614\n",
      "0.3974241316318512\n",
      "0.4154124855995178\n",
      "0.40533632040023804\n",
      "0.39964309334754944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=2500, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.nn(x)\n",
    "\n",
    "\n",
    "\n",
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"linear\",\n",
    "    γ_min=0.2**2, γ_max=0.5**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9017),\n",
       " tensor(0.7859),\n",
       " tensor(0.7955),\n",
       " tensor(0.6246),\n",
       " tensor(0.4922),\n",
       " tensor(0.5209),\n",
       " tensor(0.6452),\n",
       " tensor(0.5031),\n",
       " tensor(0.4957),\n",
       " tensor(0.4333),\n",
       " tensor(0.4574),\n",
       " tensor(0.4496),\n",
       " tensor(0.4668),\n",
       " tensor(0.4680),\n",
       " tensor(0.4522),\n",
       " tensor(0.4601),\n",
       " tensor(0.4276),\n",
       " tensor(0.4345),\n",
       " tensor(0.4545),\n",
       " tensor(0.4439),\n",
       " tensor(0.4433),\n",
       " tensor(0.4496),\n",
       " tensor(0.4215),\n",
       " tensor(0.4519),\n",
       " tensor(0.4313),\n",
       " tensor(0.4438),\n",
       " tensor(0.4184),\n",
       " tensor(0.4190),\n",
       " tensor(0.4202),\n",
       " tensor(0.4253),\n",
       " tensor(0.4130),\n",
       " tensor(0.4450),\n",
       " tensor(0.4320),\n",
       " tensor(0.4163),\n",
       " tensor(0.4311),\n",
       " tensor(0.4280),\n",
       " tensor(0.4269),\n",
       " tensor(0.4175),\n",
       " tensor(0.4560),\n",
       " tensor(0.4301),\n",
       " tensor(0.4326),\n",
       " tensor(0.4092),\n",
       " tensor(0.4431),\n",
       " tensor(0.4501),\n",
       " tensor(0.4073),\n",
       " tensor(0.4056),\n",
       " tensor(0.4142),\n",
       " tensor(0.4131),\n",
       " tensor(0.4265),\n",
       " tensor(0.4066),\n",
       " tensor(0.4181),\n",
       " tensor(0.4081),\n",
       " tensor(0.4230),\n",
       " tensor(0.4183),\n",
       " tensor(0.4371),\n",
       " tensor(0.4233),\n",
       " tensor(0.4050),\n",
       " tensor(0.4072),\n",
       " tensor(0.4167),\n",
       " tensor(0.4238),\n",
       " tensor(0.4028),\n",
       " tensor(0.4136),\n",
       " tensor(0.4312),\n",
       " tensor(0.4076),\n",
       " tensor(0.4111),\n",
       " tensor(0.4061),\n",
       " tensor(0.3930),\n",
       " tensor(0.4276),\n",
       " tensor(0.4022),\n",
       " tensor(0.4246),\n",
       " tensor(0.4247),\n",
       " tensor(0.4163),\n",
       " tensor(0.4110),\n",
       " tensor(0.4127),\n",
       " tensor(0.4024),\n",
       " tensor(0.3974),\n",
       " tensor(0.4154),\n",
       " tensor(0.4053),\n",
       " tensor(0.3996)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f614e59f850>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvEElEQVR4nO3deXxU1f3/8dcny2Qne9gCJIR9X8LqChZFtNjiilaxVq0LXWytX7GtP7WL2lqtW1vRuiuIaC1VFFDAjTXsayBASEKAbCQhIevM+f0xMyErmWwmM/N5Ph48mLlzc+eTTPKeM+eec64YY1BKKeX+fDq7AKWUUu1DA10ppTyEBrpSSnkIDXSllPIQGuhKKeUh/DrriWNiYkxCQkJnPb1SSrmlLVu25BljYht7rNMCPSEhgZSUlM56eqWUcksicrSpx7TLRSmlPIQGulJKeQgNdKWU8hAa6Eop5SE00JVSykNooCullIdwKdBFZKaIpIpImog82Mjj/UTkCxHZKSJrRSS+/UtVSil1Ls0Guoj4Ai8ClwPDgLkiMqzebk8BbxpjRgGPAY+3d6FOm9MLePKz/eiyv0opVZcrLfSJQJox5rAxphJYDFxVb59hwGrH7TWNPN5udmQW8s+1hyguq+6op1BKKbfkSqD3BjJr3c9ybKttBzDHcfuHQJiIRNc/kIjcKSIpIpKSm5vbmnqJCQ0AILekolVfr5RSnqq9ToreD1wkItuAi4BjgLX+TsaYhcaYZGNMcmxso0sRNMsZ6Pka6EopVYcra7kcA/rUuh/v2FbDGJONo4UuIqHA1caYwnaqsY7oUAsA+aWVHXF4pZRyW6600DcDA0UkUUQswA3Asto7iEiMiDiPtQB4tX3LPKsm0LWFrpRSdTQb6MaYamA+sALYBywxxuwRkcdEZLZjt4uBVBE5AHQH/tRB9RIVbEEE8kq0ha6UUrW5tHyuMWY5sLzetodr3V4KLG3f0hrn5+tDZLCFPG2hK6VUHW45UzQ6xEK+ttCVUqoO9wz0UAv5pdpCV0qp2tw00AO0ha6UUvW4ZaDHhGgfulJK1eeegR4aQHF5NRXVDeYuKaWU13LLQI92zBYt0MlFSilVw00D3Tm5SANdKaWc3DLQYxyBrv3oSil1llsGenSIvctFZ4sqpdRZbhnoMWG64qJSStXnloEeYvElwM9HV1xUSqla3DLQRYSY0ADtQ1dKqVrcMtDBMf1f+9CVUqqG2wa6ttCVUqoutw10XXFRKaXqct9ADw0gv7QCY0xnl6KUUl2C2wZ6TKiFKquhuLy6s0tRSqkuwW0DXa8tqpRSdbltoMeE6mxRpZSqzW0D3Tn9X1voSill57aBXrNAl84WVUopwI0DPTJE+9CVUqo2tw10f18fIoP9dSy6Uko5uG2gg30sus4WVUopO/cOdJ0tqpRSNdw60GNCA8gr1Ra6UkqBmwe6rriolFJnuXWgx4QGUFRWRWW1rbNLUUqpTufWge6c/l+gY9GVUsrNA73mYtHaj66UUi4FuojMFJFUEUkTkQcbebyviKwRkW0islNEZrV/qQ05Z4vqtUWVUsqFQBcRX+BF4HJgGDBXRIbV2+13wBJjzFjgBuAf7V1oY6JDdT0XpZRycqWFPhFIM8YcNsZUAouBq+rtY4BujtvhQHb7ldi0mvVcNNCVUsqlQO8NZNa6n+XYVtsjwI9EJAtYDvyssQOJyJ0ikiIiKbm5ua0ot67QAD8sfj51hi4Wnakiu7CszcdWSil3014nRecCrxtj4oFZwFsi0uDYxpiFxphkY0xybGxsm59URIgJsdSsiX40v5RZz33NT95IafOxlVLK3fi5sM8xoE+t+/GObbX9BJgJYIxZLyKBQAyQ0x5Fnovz2qJpOSXc9MoGThZXUFFt7einVUqpLseVFvpmYKCIJIqIBftJz2X19skALgEQkaFAIND2PhUXxIRaSD1xmutfWo/VBpcMidPrjCqlvFKzgW6MqQbmAyuAfdhHs+wRkcdEZLZjt18Dd4jIDmARcKsxxnRU0bVFhwZwvKgci58PS346mbF9I6istmkrXSnldVzpcsEYsxz7yc7a2x6udXsvcF77luaa8f0i2X2siJdvSaZPVDBhgf4AnC6vJiDUtzNKUkqpTuFSoHdlcyf2Ze7EvjX3wwLt39Lp8uqaC0krpZQ3cOup/40520Kv6uRKlFLqu+WBgX62ha6UUt7EgwNdW+hKKe/icYHezdHlokMXlVLexmMDXbtclFLexuMCPVS7XJRSXsrjAt3XRwix+GoLXSnldTwu0ME+dFFb6Eopb+Ohge6nLXSllNfRQFdKKQ/hoYGuXS5KKe/joYGuLXSllPfx0ED314lFSimv45GB3i3QT7tclFJexyMDPSzQj4pqG5XVts4uRSmlvjMeGui6hK5Syvt4aKDrErpKKe/joYGuC3QppbyPhwa6LtCllPI+Hh3oxRroSikv4pGBrhe5UEp5I48MdD0pqpTyRh4Z6KEB2oeulPI+Hhnofr4+BOtFLpRSXsYjAx2cC3RpC10p5T08OND9tYWulPIqHhzouoSuUsq7eHCg60UulFLexaVAF5GZIpIqImki8mAjjz8jItsd/w6ISGG7V9pC2kJXSnkbv+Z2EBFf4EVgBpAFbBaRZcaYvc59jDH31dr/Z8DYDqi1RboF+unEIqWUV3GlhT4RSDPGHDbGVAKLgavOsf9cYFF7FNcW2uWilPI2rgR6byCz1v0sx7YGRKQfkAisbntpbRMWoBe5UEp5l/Y+KXoDsNQYY23sQRG5U0RSRCQlNze3nZ+6Ll1xUSnlbVwJ9GNAn1r34x3bGnMD5+huMcYsNMYkG2OSY2NjXa+yFXRNdKWUt3El0DcDA0UkUUQs2EN7Wf2dRGQIEAmsb98SW0cX6FJKeZtmA90YUw3MB1YA+4Alxpg9IvKYiMyutesNwGJjjOmYUltGryuqlPI2zQ5bBDDGLAeW19v2cL37j7RfWW139iIX2kJXSnkHj50p2k1b6EopL+Oxga596Eopb+OxgR6qga6U8jIeG+j+vj4E+ftql4tSymt4bKCDLtCllPIunh/oFdpCV0p5B48O9G5BetUipZT38OhADwv013HoSimv4eGBrheKVkp5D48O9G56UlQp5UU8OtD1IhdKKW/i2YEe4Ed5lY0qq17kQinl+Tw70HW2qFLKi3h4oOsCXUop7+Hhga4tdKWU9/DwQLe30ItdbKFbbYZ739nKlqMFHVmWUkp1CA8P9Ja10PNLKvhk13G+PpjXkWUppVSH8OhA79bCC0Xnl1YCUHhG+9yVUu7HowP9bAvdtYAucAR6UZkGulLK/Xh0oLf0IhfOQD91prLDalJKqY7i0YHe0otcFGiXi1LKjXl0oEPLLnKRr10uSik3poFeS0FpBQCF2uWilHJDXhDo/i6PQz9Vat+vqKwKm810ZFlKKdXuPD7QI4L9Xe5CyXe00G0GTlfo7FKllHvx+ECPDLbUnOxsTu39ivTEqFLKzXh8oEcE+7s8aqWgtIq4sAAACsu0H10p5V48PtCjgi2UVFRTWX3uNdFtNsOpM5X0jw0BdOiiUsr9eHygR4RYgOZHrhSXV2G1GRJjQu3769BFpZSb8fhAjwq2B/qpZlrczv7zJEcLvUiHLiql3IxLgS4iM0UkVUTSROTBJva5TkT2isgeEXm3fctsvchg+wJdzZ0YdT6eGGMP9ObeAJRSqqvxa24HEfEFXgRmAFnAZhFZZozZW2ufgcAC4DxjzCkRieuoglsqIti1LhfnLNG4sEBCA/y0D10p5XZcaaFPBNKMMYeNMZXAYuCqevvcAbxojDkFYIzJad8yWy/K0Yde0Eygn3IEelSohfAgfx3lopRyO64Eem8gs9b9LMe22gYBg0TkWxHZICIzGzuQiNwpIikikpKbm9u6ilsowtHl0lyL29lCjwq22CcjaQtdKeVm2uukqB8wELgYmAu8LCIR9Xcyxiw0xiQbY5JjY2Pb6anPLdDfl2CLb00LvCkFpZUE+fsSZPG1j13XUS5KKTfjSqAfA/rUuh/v2FZbFrDMGFNljDkCHMAe8F1CZLDFpS4XZ/dMRJBFF+hSSrkdVwJ9MzBQRBJFxALcACyrt89H2FvniEgM9i6Yw+1XZttEhjQ/WzS/tJLoUHugh7dg/RellOoqmg10Y0w1MB9YAewDlhhj9ojIYyIy27HbCiBfRPYCa4DfGGPyO6rolnJlPZeC0koig50tdPsbgDG64qJSyn00O2wRwBizHFheb9vDtW4b4FeOf11ORLCFzIIz59ynoLSSgXGhjv39qbYZSiuthAa49CNSSqlO5/EzRQGigv1daqHX9KG7OHZdKaW6Eq8I9IhgC8Xl1VRbG1+gq6zSSlmVlajQs10uoAt0KaXci1cEurPl3dSJTueFLZzrvpxtoWugK6Xch1cEunNy0akmulCcl5472+XiaKHrbFGllBvxikCPbGbFRWcLPVq7XJRSbswrAr1mPZcmTow6tzuDv5sj0HUsulLKnXhFoJ9dz+XcgR4dYr/8XKC/L0H+vjrKRSnlVrwi0M+20BtvcReUVuLrI3QLOjvmvCXXIlVKqa7AKwI9yN8Xi5/POVvokcEWRKRmm30JXQ10pZT78IpAFxGigi1NjnIpKK0k2tGKd9IldJVS7sYrAh3sAX2uLpeo+oEeZNFhi0opt+I1gR4Z3PSSuI0GuvahK6XcjNcEelRI02uiF5xpGOjhwbriolLKvXhNoDfV4q622ig8U9Ug0CODLVRabZRVWb+rEpVSqk28JtCjQuxdLjZb3Ra3c/aoc5aok84WVUq5G68J9IhgCzYDxeV1A9o58sU5S/Ts/hroSin34jWBHhViD+j60//zS5yzROv1oQc5VlzUkS5KKTfhNYEe0cQCXc6Aj6rf5eJooetYdKWUu/CaQI9s4ipEBfXWQnc6u4SuBrpSyj14TaA7A7t+l4tzslFkIxOLQPvQlVLuw2sCPSKk8ZOcBaUVdAv0w9+37o8i0N/Hvv6L9qErpdyE1wR6WIAffj7SYHJRfiOzRMG+/ktEkK7nopRyH14T6CJCRCPT/081MkvUSaf/K6XcidcEOkBksH+jwxajHBe2qC8iqOkVGpVSqqvxrkAPsTQ6bNE5Rr2+8GB/vQydUspteFegB/vX6XI5XlRGbkkF/aJDzrG/BrpSyj14VaBHhVjqrIn+8Y7jGAOzRvZsdP+IYF0TXSnlPrwq0J0nRZ1L4v53xzFGxYeTGNN4Cz08yJ/yKhvluuKiUsoNeFWgRwVbqLYZTldUczi3hN3Hipk9uleT+9dM/9d+dKWUG/CqQK+Zzl9axbId2YjAlaPOEeg6W1Qp5UZcCnQRmSkiqSKSJiIPNvL4rSKSKyLbHf9ub/9S2865nkvBmUqWbc9mcmI0PcIDm9z/7BK62o+ulOr6/JrbQUR8gReBGUAWsFlElhlj9tbb9T1jzPwOqLHdONdr+eZgLofzSrnjwv7n3D/ccZGLY4VlHV6bUkq1lSst9IlAmjHmsDGmElgMXNWxZXWMSEeL++0NGfj7CpeP6HHO/RNjQugZHsiDH+7irQ1H9fqiSqkuzZVA7w1k1rqf5dhW39UislNElopIn8YOJCJ3ikiKiKTk5ua2oty2cU7xP1FczkWDYmvWSG9KSIAf//vZ+UxNiub3H+3mrre3aPeLUqrLaq+Tov8DEowxo4BVwBuN7WSMWWiMSTbGJMfGxrbTU7uuW6A/PmK/PXtMY+9JDcWEBvDqvAn87oqhrN6fw6xnvyavpKIDq1RKqdZxJdCPAbVb3PGObTWMMfnGGGfKvQKMb5/y2pePjxAe5E+Qvy/fGxrXoq+7/YL+vHrrBLKLyvk2La8Dq1RKqdZp9qQosBkYKCKJ2IP8BuDG2juISE9jzHHH3dnAvnatsh0N7B5GUmwowRZXvvW6JiVG4+sjpOWUdEBlSinVNs2mmjGmWkTmAysAX+BVY8weEXkMSDHGLAN+LiKzgWqgALi1A2tuk0V3TG7111r8fOgXFayBrpTqklxqphpjlgPL6217uNbtBcCC9i2tY/g6O9FbKSkulIMa6EqpLsirZoq2h4FxoaTnlVJltXV2KUopVYcGegsNiAul2mY4mn+ms0tRSqk6NNBbaEBcKID2oyuluhwN9BZKinUG+ulOrkQpperSQG+hkAA/eoUHagtdKdXlaKC3woDuYaTlaqArpbqWls+uUQyIDWXRkQJsNoNPC4ZB7skuYtGmDLILy8kuLONYYRmTEqN4+ZZkRNo2nFIppbSF3goD4kIpq7KSXeT6srpllVbueCOFD7ce40RROfGRQUzpH83n+3JYuiWrA6tVSnkLbaG3gnOky8GcEuIjg136mn9+eYjsonKW/HQKExOjALDZDNcvXM+flu9j2pA4YkIDOqxmpZTn0xZ6KzgD/ZCLJ0YzC87w0peH+P7oXjVhDvZFvx6fM5IzFVYe+1/d64Wk55Vy48sbWJKSWf9wSinVKA30VogKsRAdYmkw0uXbtDyu+9d6dh8rqrP9z8v34SPCgsuHNDjWgLgw7pmWxLId2axJzQHg870n+f4L37DuUD6vfnOk474RBUBKegFWm168RLk/DfRWSooLbRDoz6w6wKb0Aub8cx3vbc4A7CH/6e4T3DstiV4RQY0e6+6LkxgQF8rv/rObv3y2n9vfTKFfdDA/Pi+B/SdOk1nQ/KzUvJIKfr1kBwdO6vj4ltiWcYpr/rWedzdldHYpSrWZBnorDXAs0uW8LN3e7GJSjp7inouTmJQYxf99sIsHlu7g0f/toU9UELdf0PT1SwP8fHl8zkiOFZbxj7WHuHZ8PEvvmsq8KQkArNp7stl6Xlidxgdbs7jplY2k55W2+ftbuiWLf3vBp4OvD9rXtne+ASvlzjTQW2lAbChFZVXkldgvSffWhqME+vvw0wuTeP3HE/n59AEsScniwMkSfn/FMAL9fc95vAkJUTw+ZyRPXzeav1wzikB/XxJiQhgYF9psoJ8sLufdTRlcNCiWaquNm17ZyPFzjMApr7LyzKoD/HLxNmyNdDVYbYYnPt3Pn5fvc+nTgTtzXqxk97Fi9mQXNbP3uaXnlXLdS+u5882U9ihNqRbTQG+l2mu6FJVV8dG2Y8we3YvwYH98fYRfXTqYN2+byEOzhjBjWHeXjjl3Yl/mjIuvMyb90uHd2ZRecM5rmf5z7SFsNsMfrhrBm7dNorisipte2djopfLWpeVx+bNf8+wXB/loezab0wsa7JOSXkBeSQVWm+Efaw+5VHt7+y76tMsqrWzLKOT65D5Y/HxYsrl1J6CNMby94SiXP/s1m44U8Pm+k5RUVLdztUo1TwO9lQZ2dwR6bgkfbMmirMrKLY4uEqcLB8Vy54VJbZo0NGNYD6w2w+r9OY0+fqLI3jq/elw8faODGRkfzqs/nkB2YRk3LNzAo//bwzOrDvDK14f51XvbufGVjVhthpduHk+Qvy8fbc9ucMzlu44T4OfDnLG9Wbols9HW/pKUzDa3aJvyxrp0Jv35czI6eEXLzekFVFptXD6yB5cN78FH27Mpr7K26Bh5JRXc+tpmfvfRbpITIvnL1aOwGdieUdgxRSt1DhrordSjWyChAX4cPHmatzccZWzfCEb0Dm/35xnVO5y4sIAmu13+uTYNm80wf/qAmm0TEqJYeHMyVpthaUoWz35xkD9+so9lO7K5d1oSK++7kMuG9+Cy4d35ZGc2FdVnQ8xmM3y6+wQXD47lV5cOwhh46cvDdZ5z0aYMHli6k4c+3NXu3y/Ah9uOkVdSyT3vbmlxwLbEukP5+PsKExOjuD65D0VlVazYc6JFx3ji0/2sP5zPY1cN540fT+TykT3wEdjUyCcfpTqaTixqJREhKTaE/27PpqisimeuH90hz+PjI8wY1p3/bDtGeZW1Tl/8iaJyFm3K5Jrx8fSJqjvB6cJBsay5/2LA3n1RUlGNCHQL9K/Z5wdje/PR9mzWpuZy2fAeAGzJOEXO6QpmjexJfGQwc8b1ZtGmDO6ZlkRcWCBbjp7i4f/uJjLYnx1ZRezNLmZYr27t9v2eKCpnR2Yh5w+I4Zu0PB79314enzOy3Y5f27pDeYztE0mwxY+pSdHERwaxJCWTq8b0dunrnZ+cZo3oUfPpLCzQn2G9upGiga46gbbQ2yApzn5iNCrEwuUjenbY88wY1p0zlVbWH8qvs/0fa9OwGcO90wY08ZV2vj5CeJB/nTAHOH9ADDGhFj7adqxm2yc7j2Px8+GSofZ+/3suHkCV1cYrXx/hZHE5d7+9hZ7hQXx073lY/HxY3M6jQ1bttbeQH5k9jLsvTmLRpgw+6IClEYrOVLHrWBFTkqIB+xvndcl9+DYt3+Wunh1ZhRSUVjJ9aN1zJMn9otiWUdjlr2r1m/d3cO87W3UMvgfRQG8D54nR6yf0aXYUS1tMSYomNMCPlY6wM8bwfkomizdlcm1yw9a5q/x8fbhyVC++2J9DUVkVNpvhs90nuGhQLKEB9g9vCTEhzB7di7c3HOXON1Moqahm4S3j6RcdwqwRPfjPtmOUVbZft8jKvSfpHxNCUmwov54xiMn9o/jtR7vYf6K43Z4DYMORfIyB8wbE1Gy7Znw8IvD+lrMnRwvPVHKwibH9q/fl4OsjXDQwts72CQlRlFVZ2ZvdvjW3p42H83l/Sxaf7DrOS191zolv1f400Nvg/AExJEQHc/Pkfh36PAF+vlw0OJbP9+WQU1zOHW9u4TdLdzKmbwS/vnRwm479w7G9qay28dnu42zLPMWJ4nJmjexRZ5/50wdQVmVlR1YRT107miE97F0sN0zsy+nyaj7ZdbxNNTgVlVWx/lA+lw7vgYjg5+vDc3PH0i3Qn3ve2dpkf/rSLVk8vTKV7ZmFjQ7DbMy6tDyC/H0Z0yeiZluviCAuHBjLkpRM/vDxXmY9+zVj/7CKGc981WD2L8AX+3MY3y+S8OC6n3ySEyIBGh1B1BUYY3jys/107xbAZcO787eVB9iaceo7rSElvYCnVx2omceh2ocGehuMio9g7W+mNTkDtD1dOqw7uacrmPbUWr46mMvvrhjK4jsmt3lBr1Hx4STGhPDRtmw+2XkCi+/Z7hanAXFh3H/pYB67ajizRp7tWpqUGEX/mBAWt9Msy7WpOVTbDJcOP/v8cWGBPH3dGA7nlvL86oMNvmbL0VM8sHQHz61O4wcvfsukx7/g/5bu5N2NGaxJzSH1xGmKy6safN23h/KZkBiFxa/un8CNk/pysriCtzYcpVuQH7+4ZCDBFl9e+za9zn7ZhWXsO17MJUPiGhy7e7dA+kYFk5L+3Yakq1btPcnWjEJ+cckg/nLNaHp0C+Tni7ZRVNbw59QRKqqt/GrJDp774iBb22k00IGTpzv0BLq70JOibuLiwXGEBfjRLyaYZ64bw8DuYe1yXBHhB2N68/cvDpB68jQXDopp0NcONNpPLyLcMLEPf16+n4MnT7tcU0FpJesP5XPZ8O74+Z4N1BV7ThAXFsCY+Ig6+58/MIZrx8fz0peHuWJkr5qTsGWVVu5/fwc9w4NYfOdkUo4W8Pm+HJbvOs579RY1mzuxL3/8wQh8fYSTxeWk5ZRw7fj4BrVdOqw7K++7kL5RwTXdaPkllby3OZMFs4bUvIE6192Z3kigg72V/mVqLsaYVg9bzTp1ht/+Zze/uWxwu42gstoMf12RSv+YEK5Ljq/5FHTdS+v57X928fzcsR2+Nv9r36aTUXAGi68Pb6xLZ3y/yDYdr6C0kiue+5p5UxL43ZXDGt0nr6SC0opqrDaDzRiCLX7fSUPsu6YtdDcRHuTP1/83jY/uOa/dwtzpB2N7YYz9D6OlJ3evHhePv6+w2IVJOUVlVTy9MpULnlzNve9u5fFP99c8Vl5lZW1qLjOGdW/0oiG/vWIoEcH+PPjhTqodJxuf/Gw/R/JK+eu1o+gTFcwPx8bz4o3j2PbwDNY9OJ0P7p7C83PH8qPJfVm0KYOfL9pGZbWt5uTy1KSYBs8jIgzqHlbnnMi8qf2otNrqfBJZvS+HPlFBNedR6puQEEV+aSVHWrkMQ2W1jfnvbuPLA7n8efm+Vh3jcG4JizZl1Jnk9OHWLA7mlHD/ZYNr3kzH94vkVzMG8fHO47zfyhPQa1JzuPnfG5s9oZx7uoIXVqfxvaFx3DS5L8t3HSenuLxVz+n01YFcqqyGpVuzGm2lb804xYQ/fc5Ff13L9L99yfee/oqpT6zm6ZWpLnfRuQttobuRiGBLhxy3X3QIY/tGsPtYEd9zcVarU3RoAJcO68EHW7P4zWWDa4LQGENxWTXZRWUcLypjZ1YRr32bTlFZFbNG9iDY4se/vznC0J7duGZ8POsO5XGm0sqlw3s0+jwRwRYemT2c+e9u4/V16Qzr2Y3X16Vz69SEBsHs5+tDr4ggekUEMb4ffH90LxKiQ/jjJ/soqagmLNCP8CB/l4dbDogL44KBMby9IYOfXpREtdXwTVoecyf2bbI1O8HRj56Sfor+sY2H/rn8dcV+tmcWMn1IHKv357D+UH7NiBxXlFdZuf3NFA7nlvLEp/uZN6Ufcyf15ZlVBxgdH87lI+r+nO++KIkvU3N54tP9zBzRo9FPaU09zxOf7uf1dekAPLf6IE9d2/QQ3r+tTKWi2spDs4YiIrz2bTrvbMzgvhmDXP7e6luTaj85XXjGPo+g/rDThV8eplugP7+/chh+PoKPj/Blai7PrU7jUG4pT107miBLxw1q+C5poCsAHp09nPT8M4QHufaHXNsNE/vwya7jXPXCt1iN4XR5FUVlVZRX1R22d8mQOO6bMYgRvcOpttrILizjoQ93kRQbwso9JwkL8GNK/6ZD64qRPflo6DGeWplKZLCFxJgQ/m9mwyWJG3P7Bf0JC/RjwYe7sBmYObwHvi24fOCtUxP4yRsprNhzgmCLLxXVtia7WwCSYkOJDPZnc3oB103o4/LzgL2P++WvjzBvSj8WzBrKhX9ZwzOfH2By/8l13kAyC87w/pYs7rggkbB6Afy3lakczi3lke8PY/3hfJ5fk8bza9IwBp66dnSDNyIfH+H3Vw7j+y98w0tfHuI3lzX/c92bXcwv39vGgZMl/Pi8BCqqbSzZnMkvvzew0Qu/7D5WxHspmfzkvMSaN7lpg2N5d1MG904b0OB8hiusNsOXB3KZPboXKUcLWLQpo06gH80vZcXeE9x9URLX1Opi+/6ongzuEcrjn+4n69QZXr4lmbhugS1+/q5GA10B9hO8o+r1XbvqvKQY5ozrTe7pCsIC/QgL8Ccs0I/u3QLpGRFIz/Ag+kQG1fmD8fP14cUbxzH7xW/46VtbqLYZpg2JO+cftYjwhx+MYMbTX3GyuJz375raopbV9RP6Ehrgz31LtnP5yMY/CTTl4sFx9I0K5o116QzqHkawxZdJ/aOa3F9ESE6IIuXouU+MVlRbMYaaTzaZBWf49ZLtjOjdjYeuGEqAny/3ThvA/1u2h/WH8pnqGGZZUFrJLa9u4kheKd+m5fH6jyfUhHpKegGvfHOEmyb15dbzErn1vETSckp45evDBPr71hyjvpHx4Vw1phevfH2EH03uR8/wpvuYV+45wfxF2wgP8ueN2yZy0aBYsgvLWLI5k5e/OsyjV42os78xhj98vJfIYAs/u2RgzfZ5UxO49bXNfLr7uMsTumrbnnmKwjNVTB8Sx4C4UP66IpUjeaUkxoQA8Oo3R/DzEeZNTajzdSLCnRcmkRgTyi8Wb+PK57/h/ssGM2ds7zrndVrjfzuyiY8MYmzftp0baA0NdNVmPj7C09eNafHXRYZYePmWZOb8Y52ju6X57p6e4UEsvHk8p85Utepk2hWjenLJ0LgWzxvw9RFumdKPP36yj33HT3P+gBgC/JpbQTOSVXtPknu6gtiwhqOR1qXlcc+7Wyk8U0V0iIWeEYEUnqnCGHjxxnE1x79+Qh/+ufYQT686wJSkaCqqbdz5ZgrHCsv4+SUDeXFNGre+tpk3bpuIj8D97++gd0QQD80aWvNcA+JCeeLqUc1+n/dfOphPd53g6ZUH+GsTXSf/2ZbF/e/vZGTvcP49L5lox4niXhFBzBnXm8WbM5k/fWCd7/n9LVlsPFLAH38wos6nwAsHxpIYE8Lr69JbFehr9ufi6yNcODCWimorT686wOLNGSy4fCiFZypZkpLF7NG96d5E63vGsO4svWsqCz7cyQNLdzo+nQzmsuE9OF1RzcGTpzlwsoTu3QKYPqT538/dx4r4+eJtBPj58Oq8CU2+eXYUPSmqOtWQHt14fu5Ypg2OZdrgprswaps6IIYrRrV+Zm5rJ4Fdm9yHYIsvJRXVXDK0+VqTE+wt+MaWAViSksktr24iNjSAX80YxKXDexATGkBMaADP3TiWftEhdeq9d/oAUo6e4ssDufx6yQ5Sjp7imevG8KsZg3hh7li2ZxYy79VN/OHjvaTnn+Gv14wmJKDl7bU+UcHMm9qPpVuzGp3M9db6dO57bweTEqN4+/ZJNWHudNdFSVRabbz67dm19Nfsz+GhD3cxuX8UN9TrfvJxvFFuyyhkp2Pm7dsbjjJ34QbmvbqJjYfrzo6ub01qDuP72ucCxHULZPqQOD7YkkVltY13NmZQVmXl9gsSz3mMYb268dG95/GvH41HRLjr7a2MeWwVox5ZydX/XM+CD3dx2+spzV49zBjDnz7ZR0SQP/2iQrjtjc2scyzP/F3RFrrqdJcM7d5g7HtXFB7kz9Xj4nln41GX3nxG9AonwM+Hj3cep39sKIkxIfj5CH9blcqLaw5xwcAYXrxpnEsnIK9Ljudfaw9xzztbOVNp5bezhta8qV0+sicvAPMXbWPL0VPcOjWhRSdQ67t32gDe25zJ48v388ZtEzHGkFFwhqVbsnjeMULlhRvHNfrG2D82lFkje/LW+qPcdVESB06e5u53tjC0ZzdeviW50e6Ma8bH89SKVO58cwt5JRVU2wxJsSEUl1dz/cINTE2K5r4Zg5iQULeL62RxOXuyi3lg5tnJdXMn9mHV3pN8uvs4b6xL54KBMQzt2fzJbxFh5ogefG9oHB9uO8bmIwUkxoYwKC6MAXGhPPnZfh77eC9+vtJgVVWn1ftzWH84n0dnD+fKUT258eWN3PbG5u+0pS6uzNQSkZnAs4Av8Iox5okm9rsaWApMMMacc5X/5ORkk5KiFwJQ7qW0opqDOSV1Zpiey09e38wXjqWP/XyEuLAAsovKmTuxD49dNQL/FvTXLtqUwYIPdzFvSj8emT28wYnNlXtO8PHO4zxx9UiCLW1rqy386hB/Xr6fCwbGsO94cc2FXK4a04unrh19zrr3Zhcz67mvmTO2N6v2nSQ2LID3fzqlQWu+tmdWHeA/245x+YgezB7Ti2E9u1FRbePtDUf515eHySup4KZJ9rkEzu97yeZMHvhgJ5/+4oKa0LbaDOc/uZrSimqKy6tr+vfbqrLaxj3vbOXzfSf58w9HcuOkvnUer7LamPn3rzDAil9eiL+vD3klFdz48gYyCs7w5NWjmD26V7uM8ReRLcaY5EYfay7QRcQXOADMALKAzcBcY8zeevuFAZ8AFmC+BrpS9j/0AydPc/BkCQdOniYtp4TzB8Zw8+R+Lf7jNsawPbOQUfERLRqh0xrlVVbm/GMdZVVWxvaNYFzfSMb1jWRozzCX6r7t9c2s3p9Dz/BAlt49ld5tmMRTVmnlyc/sQyOfmDOSGybaw/Tut7ewLaOQ9Qum16np6VUHeO6LgwzqHsqKX17YbhOlKqqt3PXWFtak2mdqz5uaUPPG9tb6dH7/3z28fEtynQva5JVU8OPXNrPrWBHJ/SL5/ZXDGO1iY6ApbQ30KcAjxpjLHPcXABhjHq+339+BVcBvgPs10JXyXnuzi/nT8r08Ons4A+LaPhHOajPc+tomNh4p4MO7pzK4RxhjH1vF90f35PE5dU/2ZheWcekzX/HnOSOZPbpXm5+7tvIqK/e8s5XV+3PoGxXM/OkDmDG0O5c8/SWDuoey6I7JDd5ArDb7YnpPrTxAXkkFPxzbmwdmDj7nKKJzaWugXwPMNMbc7rh/MzDJGDO/1j7jgN8aY64WkbU0EegicidwJ0Dfvn3HHz16tFXfkFLK++SXVHDl89/g7+vDQ7OGctfbW1h48/hGJ6NVW21tHn7YFGPs6+D//fOD7DpWRLDFlzOVVj7+2fnnXKKhpKKaf6xJ45VvjvD7K4ZycxN98c05V6C3+aSoiPgATwO3NrevMWYhsBDsLfS2PrdSyntEhwbwwo3juP6l9fzyvW34+0qd5Y9r66gwB/sJ1EuGdmf6kDjWpObwjzWHGNOn+SuWhQb48cDMIdw4qS89OmgSkyuBfgyoPdYo3rHNKQwYAax1fNToASwTkdnNdbsopVRLjO8XyUOzhvLYx3s5f0BMq4ZmthcRYfqQ7i6NT6+tsVm07cWVn8ZmYKCIJGIP8huAG50PGmOKgJq3yXN1uSilVFv9+LwEyqqsbV6l0RM1G+jGmGoRmQ+swD5s8VVjzB4ReQxIMcYs6+gilVLKSUSaveyit3Lp84oxZjmwvN62h5vY9+K2l6WUUqqldOq/Ukp5CA10pZTyEBroSinlITTQlVLKQ2igK6WUh9BAV0opD6GBrpRSHsKl9dA75IlFcoHWrs4VA3y3lwJxndbWOlpb62htrePOtfUzxjS6yHunBXpbiEhKU6uNdTatrXW0ttbR2lrHU2vTLhellPIQGuhKKeUh3DXQF3Z2AeegtbWO1tY6WlvreGRtbtmHrpRSqiF3baErpZSqRwNdKaU8hNsFuojMFJFUEUkTkQc7uZZXRSRHRHbX2hYlIqtE5KDj/065rIqI9BGRNSKyV0T2iMgvukp9IhIoIptEZIejtkcd2xNFZKPjtX1PRCzfdW21avQVkW0i8nFXqk1E0kVkl4hsF5EUx7ZOf00ddUSIyFIR2S8i+0RkSleoTUQGO35ezn/FIvLLrlCbo777HH8Hu0VkkePvo1W/b24V6CLiC7wIXA4MA+aKyLBOLOl1YGa9bQ8CXxhjBgJfOO53hmrg18aYYcBk4F7Hz6or1FcBTDfGjAbGADNFZDLwJPCMMWYAcAr4SSfU5vQLYF+t+12ptmnGmDG1xip3hdcU4FngM2PMEGA09p9fp9dmjEl1/LzGAOOBM8B/ukJtItIb+DmQbIwZgf2qcDfQ2t83Y4zb/AOmACtq3V8ALOjkmhKA3bXupwI9Hbd7Aqmd/XNz1PJfYEZXqw8IBrYCk7DPjvNr7LX+jmuKx/4HPh34GJAuVFs6EFNvW6e/pkA4cATHQIuuVFu9ei4Fvu0qtQG9gUwgCvsV5D4GLmvt75tbtdA5+807ZTm2dSXdjTHHHbdPAC27JHgHEJEEYCywkS5Sn6NLYzuQA6wCDgGFxphqxy6d+dr+HXgAsDnuR9N1ajPAShHZIiJ3OrZ1hdc0EcgFXnN0Vb0iIiFdpLbabgAWOW53em3GmGPAU0AGcBwoArbQyt83dwt0t2Lsb6+dOi5UREKBD4BfGmOKaz/WmfUZY6zG/hE4HpgIDOmMOuoTkSuBHGPMls6upQnnG2PGYe92vFdELqz9YCe+pn7AOOCfxpixQCn1ujA6++/B0Q89G3i//mOdVZuj3/4q7G+IvYAQGnbjuszdAv0Y0KfW/XjHtq7kpIj0BHD8n9NZhYiIP/Ywf8cY82FXqw/AGFMIrMH+sTJCRJwXLu+s1/Y8YLaIpAOLsXe7PNtFanO26DDG5GDvB55I13hNs4AsY8xGx/2l2AO+K9TmdDmw1Rhz0nG/K9T2PeCIMSbXGFMFfIj9d7BVv2/uFuibgYGOM8AW7B+flnVyTfUtA+Y5bs/D3nf9nRMRAf4N7DPGPF3roU6vT0RiRSTCcTsIe9/+PuzBfk1n1maMWWCMiTfGJGD//VptjLmpK9QmIiEiEua8jb0/eDdd4DU1xpwAMkVksGPTJcDerlBbLXM5290CXaO2DGCyiAQ7/madP7fW/b515gmKVp5EmAUcwN7n+ttOrmUR9n6vKuwtlJ9g72/9AjgIfA5EdVJt52P/CLkT2O74N6sr1AeMArY5atsNPOzY3h/YBKRh/1gc0Mmv78XAx12lNkcNOxz/9jh//7vCa+qoYwyQ4nhdPwIiu1BtIUA+EF5rW1ep7VFgv+Nv4S0goLW/bzr1XymlPIS7dbkopZRqgga6Ukp5CA10pZTyEBroSinlITTQlVLKQ2igK6WUh9BAV0opD/H/AZjodeDv+HLHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  7., 12., 24., 17., 17.,  8.,  9.,  1.,  2.]),\n",
       " array([-0.669009  , -0.48744178, -0.30587456, -0.12430736,  0.05725985,\n",
       "         0.23882705,  0.42039424,  0.60196143,  0.7835287 ,  0.9650959 ,\n",
       "         1.1466631 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8479, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8501, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
