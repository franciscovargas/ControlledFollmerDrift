{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a6b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchsde\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import functorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cfollmer.functional as functional\n",
    "from cfollmer.objectives import relative_entropy_control_cost\n",
    "from cfollmer.drifts import SimpleForwardNet, SimpleForwardNetBN, ResNetScoreNetwork\n",
    "from cfollmer.sampler_utils import FollmerSDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9078de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d492713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "    \n",
    "class LinModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        super(LinModel, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08da0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, gamma, dim, n_steps, data_batch_size, param_batch_size, dt=0.05, stl=False, size_list=None):\n",
    "    \n",
    "    sde = FollmerSDE(gamma, SimpleForwardNetBN(input_dim=dim, width=300)).to(device)\n",
    "    optimizer = torch.optim.Adam(sde.parameters(), lr=1e-4)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        perm = torch.randperm(N_train)\n",
    "        x = X_train[perm[:data_batch_size], :]\n",
    "        y = y_train[perm[:data_batch_size], :]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        partial_log_p = lambda params_batch: log_posterior_batch(x, y, params_batch, size_list=size_list)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        loss = relative_entropy_control_cost(sde, partial_log_p, param_batch_size=param_batch_size, dt=dt, device=device)\n",
    "        loss.backward()\n",
    "        \n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.step()\n",
    "        \n",
    "        if stl: # double check theres no references left\n",
    "            sde.drift_network_detatched.load_state_dict((sde.drift_network.state_dict()))\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    \n",
    "    return sde, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a14fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict(param_samples, x, y, size_list=None):\n",
    "    with torch.no_grad():\n",
    "        predict_func = lambda params : func_model(functional.get_params_from_array(params, size_list), x)\n",
    "        predict_func = functorch.vmap(predict_func)\n",
    "\n",
    "        preds = predict_func(param_samples)\n",
    "\n",
    "        std, mean = torch.std_mean(preds, dim=0)\n",
    "        mse = torch.mean((y_test - mean)**2)\n",
    "        logp = torch.mean(log_likelihood_batch(x, y, param_samples, size_list=size_list))\n",
    "        \n",
    "    return std, mean, logp, mse\n",
    "\n",
    "\n",
    "sigma2 = 1\n",
    "sigma_n = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCFollmerDrift:\n",
    "    \n",
    "    def __init__(self, log_posterior, X,y, dim, device, n_samp=300, gamma=torch.tensor(1), debug=False, size_list=None):\n",
    "        self.log_posterior = log_posterior\n",
    "        self.debug = debug\n",
    "        self.size_list=size_list\n",
    "        self.log_posterior = log_posterior\n",
    "        self.device = device\n",
    "        self.X = X\n",
    "        self.dim = dim\n",
    "        self.y = y\n",
    "        self.gamma = gamma\n",
    "        self.n_samp = n_samp\n",
    "        self.distrib = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc=torch.zeros(dim),\n",
    "            covariance_matrix=torch.eye(dim) * torch.sqrt(gamma)\n",
    "        )\n",
    "        \n",
    "    def g(self, thet):\n",
    "        func = lambda params: self.log_posterior(self.X, self.y, params, size_list=self.size_list)\n",
    "        func = functorch.vmap(func)\n",
    "        lp = func(thet)\n",
    "        reg = 0.5 * (thet**2).sum(dim=-1) / self.gamma\n",
    "        \n",
    "#             if torch.any(torch.isinf(torch.exp(lp + reg))):\n",
    "\n",
    "        out = torch.exp(lp + reg)\n",
    "        isnan = torch.isinf(torch.abs(out)) | torch.isnan(out)\n",
    "        if self.debug and torch.any(isnan):\n",
    "            import pdb; pdb.set_trace()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return out # nans exp(reg)\n",
    "\n",
    "    def ln_g(self, thet):\n",
    "        func = lambda params: self.log_posterior(self.X, self.y, params, size_list=self.size_list)\n",
    "        func = functorch.vmap(func)\n",
    "        lp = func(thet)\n",
    "        reg = 0.5 * (thet**2).sum(dim=-1) / self.gamma\n",
    "        \n",
    "        out = lp + reg\n",
    "        isnan = torch.isinf(torch.abs(out)) | torch.isnan(out)\n",
    "        if self.debug and torch.any(isnan):\n",
    "            import pdb; pdb.set_trace()\n",
    "            \n",
    "        return out # nans exp(reg)\n",
    "        \n",
    "    def mc_follmer_drift_(self, t, params, Z):\n",
    "        # Using Stein Estimator for SFS drift\n",
    "\n",
    "        g_YZt = self.g(params[None, ...] + torch.sqrt(1-t) * Z)\n",
    "        num = (Z * g_YZt[..., None]).mean(dim=0)\n",
    "        denom = torch.sqrt(1-t) * (g_YZt).mean(dim=0)\n",
    "        \n",
    "        out = num / denom[...,None]\n",
    "        \n",
    "        isnan = torch.isinf(torch.abs(out)) | torch.isnan(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def mc_follmer_drift_stable(self, t, params, Z):\n",
    "        # Using Stein Estimator for SFS drift\n",
    "        N, d = Z.shape\n",
    "        lnN = torch.log(torch.tensor(N)).to(self.device)\n",
    "        \n",
    "        ln_g_YZt = self.ln_g(params[None, ...] + torch.sqrt(1-t) * Z)\n",
    "        \n",
    "        Z_plus = torch.nn.functional.relu(Z)\n",
    "        Z_minus = torch.nn.functional.relu(-Z)        \n",
    "        \n",
    "        ln_num_plus = torch.logsumexp(\n",
    "            (torch.log(Z_plus) + ln_g_YZt[..., None]) - lnN,\n",
    "            dim=0,\n",
    "        )\n",
    "        ln_num_minus = torch.logsumexp(\n",
    "            (torch.log(Z_minus) + ln_g_YZt[..., None]) - lnN,\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        ln_denom = torch.logsumexp(\n",
    "            torch.log(torch.sqrt(1-t))  + (ln_g_YZt) - lnN,\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        out =  torch.exp(ln_num_plus-ln_denom) - torch.exp(ln_num_minus-ln_denom)\n",
    "        \n",
    "        \n",
    "        isnan = torch.isinf(torch.abs(out)) | torch.isnan(out)\n",
    "        \n",
    "        return out \n",
    "    \n",
    "    def mc_follmer_drift_debug(self, t, params):\n",
    "        # Using Stein Estimator for SFS drift\n",
    "        \n",
    "        \n",
    "        Z = self.distrib.rsample((self.n_samp,)).to(self.device)\n",
    "        params = params[0]\n",
    "\n",
    "        g_YZt = self.g(params[None, ...] + torch.sqrt(1-t) * Z)\n",
    "        num = (Z * g_YZt[..., None]).mean(dim=0)\n",
    "        denom = torch.sqrt(1-t) * (g_YZt).mean(dim=0)\n",
    "        \n",
    "        out = num / denom[...,None]\n",
    "        \n",
    "        isnan = torch.isinf(torch.abs(out)) | torch.isnan(out)\n",
    "        \n",
    "        if self.debug and torch.any(isnan):\n",
    "            import pdb; pdb.set_trace()\n",
    "        \n",
    "        return out.reshape(1,-1)\n",
    "    \n",
    "    def mc_follmer_drift(self, t , params_batch):\n",
    "        Z = self.distrib.rsample((params_batch.shape[0], self.n_samp)).to(self.device)\n",
    "        \n",
    "        func = lambda params, z: self.mc_follmer_drift_stable(t, params, z)\n",
    "        func = functorch.vmap(func, in_dims=(0,0) )\n",
    "        out = func(params_batch, Z)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class MCFollmerSDE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma, dim, log_posterior, X_train, y_train, device, debug=False, size_list=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.noise_type = 'diagonal'\n",
    "        self.sde_type = 'ito'\n",
    "        self.gamma = gamma\n",
    "        self.size_list = size_list\n",
    "        if debug:\n",
    "            self.drift =  MCFollmerDrift(log_posterior, X_train, y_train, dim, device, gamma=gamma, debug=debug, size_list=size_list).mc_follmer_drift_debug\n",
    "        else:\n",
    "            self.drift =  MCFollmerDrift(log_posterior, X_train, y_train, dim, device, gamma=gamma, size_list=size_list).mc_follmer_drift\n",
    "        self.dim = dim\n",
    "        \n",
    "    def f(self, t, y, detach=False):\n",
    "        return self.drift(t, y)\n",
    "        \n",
    "    def g(self, t, y):\n",
    "        return torch.sqrt(self.gamma )* torch.ones_like(y)\n",
    "\n",
    "    def sample_trajectory(self, batch_size, dt=0.05, device=None):\n",
    "        param_init = torch.zeros((batch_size, self.dim), device=device)\n",
    "\n",
    "        n_steps = int(1.0 / dt)\n",
    "\n",
    "        ts = torch.linspace(0, 1, n_steps, device=device)\n",
    "\n",
    "        param_trajectory = torchsde.sdeint(self, param_init, ts, method=\"euler\", dt=dt)\n",
    "\n",
    "        return param_trajectory, ts\n",
    "\n",
    "    def sample(self, batch_size, dt=0.05, device=None):\n",
    "        return self.sample_trajectory(batch_size, dt=dt, device=device)[0] [-1]#[-1]\n",
    "    \n",
    "\n",
    "# mcfol = MCFollmerDrift(log_posterior, X_train, y_train, dim, device)\n",
    "# sde_sfs = MCFollmerSDE(torch.tensor(gamma), dim, log_posterior, X_train, y_train, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2729461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_true_std(X_train, X_test, sigma_n, sigma2, dim):\n",
    "    # https://github.com/probml/pml-book/releases/latest/download/book1.pdf\n",
    "    # See Eq 11.124 in the above link page 430 on pdf viewer (page 400 on page number in pdf)\n",
    "\n",
    "    X_trainnp = X_train.cpu().detach().numpy()\n",
    "    n_, d = X_trainnp.shape\n",
    "\n",
    "    X_trainnp = np.concatenate((X_trainnp, np.ones((n_, 1))), axis=1)\n",
    "\n",
    "    X_testnp = X_test.cpu().detach().numpy()\n",
    "    n_, d = X_testnp.shape\n",
    "\n",
    "    X_testnp = np.concatenate((X_testnp, np.ones((n_, 1))), axis=1)\n",
    "\n",
    "\n",
    "    Sigma_post = sigma_n**2 * np.linalg.inv(sigma_n**2 *  np.eye(dim) / sigma2 + np.dot(X_trainnp.T,X_trainnp))\n",
    "    \n",
    "    sigma_pred = []\n",
    "    for i in range(n_):\n",
    "        sigma_pred += [np.dot(X_testnp[i,:].dot(Sigma_post), X_testnp[i,:]) + sigma_n**2 ]\n",
    "\n",
    "    std_true = np.sqrt(sigma_pred)\n",
    "    return std_true\n",
    "\n",
    "\n",
    "def pred_true_mean(y_train, X_train, X_test, sigma_n, sigma2, dim):\n",
    "    # https://github.com/probml/pml-book/releases/latest/download/book1.pdf\n",
    "    # See Eq 11.124 in the above link page 430 on pdf viewer (page 400 on page number in pdf)\n",
    "\n",
    "    X_trainnp = X_train.cpu().detach().numpy()\n",
    "    n_, d = X_trainnp.shape\n",
    "    \n",
    "    lambda_ = sigma_n**2 / sigma2 \n",
    "\n",
    "    X_trainnp = np.concatenate((X_trainnp, np.ones((n_, 1))), axis=1)\n",
    "\n",
    "    X_testnp = X_test.cpu().detach().numpy()\n",
    "    n_, d = X_testnp.shape\n",
    "\n",
    "    X_testnp = np.concatenate((X_testnp, np.ones((n_, 1))), axis=1)\n",
    "\n",
    "    Xty = np.dot(X_trainnp.T, y_train)\n",
    "\n",
    "    Sigma_post = np.linalg.inv(sigma_n**2 *  np.eye(dim) / sigma2 + np.dot(X_trainnp.T,X_trainnp))\n",
    "    \n",
    "    w = np.dot(Sigma_post, Xty)\n",
    "    \n",
    "    return np.dot(X_testnp,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19372f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/fav25/.conda/envs/functorch/lib/python3.9/site-packages/torch/nn/functional.py:2282: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::batch_norm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-jk216yxu/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch.batch_norm(\n",
      "100%|██████████| 300/300 [00:16<00:00, 18.68it/s]"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "def lin_reg_data_gen(dim, sigma_n, device, num_samps=30):\n",
    "\n",
    "    w = np.ones((dim,1))\n",
    "    b = 1\n",
    "\n",
    "    func = lambda x: np.dot(x, w)  + 1.0\n",
    "\n",
    "    # Test inputs\n",
    "    num_test_samples = 30\n",
    "\n",
    "    if dim == 1:\n",
    "        X_test = np.linspace(-16, 16, num_samps).reshape(num_samps,1)\n",
    "        X_train = np.linspace(-3.5, 3.5, num_samps).reshape(-1,1)\n",
    "    else:\n",
    "        X_test  = np.random.randn(num_samps, dim)\n",
    "        X_train  = np.random.randn(num_samps, dim)\n",
    "\n",
    "    # Noise free training inputs\n",
    "\n",
    "    #f_train = np.cos(X_train) \n",
    "    f_train = func(X_train)\n",
    "\n",
    "    # Noise-free training outputs\n",
    "    #f = np.cos(X_test)\n",
    "    f = func(X_test)\n",
    "    y_test  = f\n",
    "\n",
    "    # Noisy training Inputs with additive Gaussian noise (zero-mean, variance sigma_n)\n",
    "\n",
    "    mu = np.zeros(X_train.shape[0])\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    epsilon = np.random.multivariate_normal(mu, sigma_n**2 * np.eye(X_train.shape[0]))\n",
    "\n",
    "    # Noisy targets\n",
    "    try:\n",
    "        y_train = f_train + epsilon.reshape(X_train.shape[0],1)\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, f\n",
    "\n",
    "\n",
    "gamma = 0.1**2\n",
    "\n",
    "n_steps = 300\n",
    "data_batch_size = 50\n",
    "param_batch_size = 32\n",
    "samps_lost = []\n",
    "\n",
    "errors = []\n",
    "\n",
    "samp_no = 100\n",
    "\n",
    "\n",
    "def log_prior(params):\n",
    "    return -torch.sum(params**2) / (2 * sigma2)\n",
    "\n",
    "def log_likelihood(x, y, params, size_list=None):\n",
    "#     import pdb; pdb.set_trace()\n",
    "\n",
    "    preds = func_model(functional.get_params_from_array(params, size_list), x)\n",
    "    \n",
    "    diff = preds - y\n",
    "    \n",
    "    return - torch.sum(diff**2) / (2 * sigma_n**2)\n",
    "\n",
    "def log_likelihood_batch(x, y, params_batch, size_list=None):\n",
    "    func = lambda params: log_likelihood(x, y, params, size_list=size_list)\n",
    "    func = functorch.vmap(func)\n",
    "    return func(params_batch)\n",
    "\n",
    "def log_posterior(x, y, params, size_list=None):\n",
    "    return log_prior(params) + (N_train / x.shape[0]) * log_likelihood(x, y, params, size_list=size_list)\n",
    "\n",
    "def log_posterior_batch(x, y, params_batch, size_list=None):\n",
    "    func = lambda params: log_posterior(x, y, params, size_list=size_list)\n",
    "    func = functorch.vmap(func)\n",
    "    return func(params_batch)\n",
    "\n",
    "for i in range(1,11):\n",
    "    dim = dim_data =  2**i\n",
    "\n",
    "\n",
    "    X_train, y_train, X_test, y_test, f = lin_reg_data_gen(dim,  sigma_n, device)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    N_train , _ = X_train.shape\n",
    "    N_test , _ = X_test.shape\n",
    "    \n",
    "    X_train = torch.tensor(X_train, device=device, dtype=torch.float)\n",
    "    X_test = torch.tensor(X_test, device=device, dtype=torch.float)\n",
    "\n",
    "    y_train = torch.tensor(y_train, device=device, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test, device=device, dtype=torch.float)\n",
    "    \n",
    "    ################################  N-SFS #########################################################\n",
    "    model = LinModel(input_dim=dim).to(device)\n",
    "    func_model, params = functorch.make_functional(model)\n",
    "    size_list = functional.params_to_size_tuples(params)\n",
    "    dim_mod = functional.get_number_of_params(size_list)\n",
    "#     print(size_list)\n",
    "#     \n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    sde, losses = train(X_train, y_train, gamma, dim_mod, n_steps, data_batch_size, param_batch_size, size_list=size_list)\n",
    "    param_samples = sde.sample(samp_no, dt=0.01, device=device)\n",
    "    std, mean, logp, mse = predict(param_samples, X_test, y_test, size_list=size_list)\n",
    "    std = torch.sqrt(std**2 + sigma_n**2)\n",
    "    \n",
    "    ################################  SFS ############################################################\n",
    "    sde_sfs = MCFollmerSDE(torch.tensor(gamma), dim_mod, log_posterior, X_train, y_train, device, size_list=size_list)\n",
    "    sfs_samps = sde_sfs.sample(samp_no, dt=0.01, device=device)\n",
    "    sfs_samps = sfs_samps[~torch.isnan(sfs_samps).sum(dim=1).bool()]\n",
    "    std_sfs, mean_sfs, logp_sfs, mse_sfs = predict(sfs_samps, X_test, y_test, size_list=size_list)\n",
    "    std_sfs = torch.sqrt(std_sfs**2 + sigma_n**2)\n",
    "    \n",
    "    samps_lost.append(((~torch.isnan(sfs_samps).sum(dim=1).bool()).sum(), samp_no) )\n",
    "    \n",
    "    \n",
    "    ################################  True ############################################################\n",
    "    std_true = pred_true_std(X_train, X_test, sigma_n,  sigma2, dim_mod)\n",
    "    mean_true = pred_true_mean(y_train.detach().cpu(), X_train, X_test, sigma_n,  sigma2, dim_mod)\n",
    "    \n",
    "    \n",
    "    ################################  Errors ############################################################\n",
    "    \n",
    "    mae =  lambda x,y: torch.abs(torch.tensor(x).to(device)-torch.tensor(y).to(device)).sum(dim=-1).mean()\n",
    "    mse =  lambda x,y: torch.sqrt(((torch.tensor(x).to(device)-torch.tensor(y).to(device) )**2).sum(dim=-1).mean())\n",
    "    \n",
    "    errors.append(\n",
    "        {\n",
    "            \"mae_sfs_mean\": mae(mean_sfs, mean_true),\n",
    "            \"mae_nsfs_mean\": mae(mean, mean_true),\n",
    "            \"mse_sfs_mean\": mse(mean_sfs, mean_true),\n",
    "            \"mse_nsfs_mean\": mse(mean, mean_true),\n",
    "            \"mae_sfs_std\": mae(std_sfs, std_true),\n",
    "            \"mae_nsfs_std\": mae(std, std_true),\n",
    "            \"mse_sfs_std\": mse(std_sfs, std_true),\n",
    "            \"mse_nsfs_std\": mse(std, std_true),\n",
    "        }\n",
    "    )\n",
    "#     del func_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
