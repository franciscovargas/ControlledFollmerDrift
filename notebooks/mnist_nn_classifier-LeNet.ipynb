{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from torch import datasets\n",
    "\n",
    "from torch import _vmap_internals\n",
    "from torchvision import datasets, transforms\n",
    "from functorch import vmap\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functorch succesfully imported\n"
     ]
    }
   ],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.drifts import *\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Cat}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = datasets.MNIST(\"../data/mnist/\", download=True, train=True)\n",
    "images_test = datasets.MNIST(\"../data/mnist/\", download=True, train=False)\n",
    "\n",
    "transform = torch.nn.Sequential(transforms.Normalize((0.1307,), (0.3081)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = images_train.data, images_train.targets\n",
    "X_test, y_test = images_test.data, images_test.targets\n",
    "\n",
    "X_train = torch.flatten(transform(X_train.float()), 1)\n",
    "X_test = torch.flatten(transform(X_test.float()), 1)\n",
    "\n",
    "y_train = F.one_hot(y_train)\n",
    "y_test = F.one_hot(y_test)\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27637/1347329722.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_27637/1347329722.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_27637/1347329722.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_27637/1347329722.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_test, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = vmap(preds_func)\n",
    "        vmapped = batched_preds(preds_func, Θ)\n",
    "        preds = torch.hstack(vmapped)\n",
    "#         preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim, out_dim, device=device, depth=1, width=50, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=3.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        cel = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ll_cel = -1.0 * cel(preds, y.argmax(dim=1))\n",
    "        return ll_cel\n",
    "    \n",
    "    batched_loss =  vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39760"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c5115f3e5147998a76e73f52c0b074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2378: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::batch_norm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch.batch_norm(\n",
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:1891: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2942: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::cross_entropy_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.411092519760132\n",
      "2.225189447402954\n",
      "1.966545820236206\n",
      "1.6961148977279663\n",
      "1.3891446590423584\n",
      "1.2065753936767578\n",
      "1.0401029586791992\n",
      "0.8705145716667175\n",
      "0.7646529674530029\n",
      "0.6692946553230286\n",
      "0.615804135799408\n",
      "0.5522686839103699\n",
      "0.4932783246040344\n",
      "0.4630196988582611\n",
      "0.4415580928325653\n",
      "0.4189043641090393\n",
      "0.3944348096847534\n",
      "0.3844749629497528\n",
      "0.38448360562324524\n",
      "0.3780539929866791\n",
      "0.3753856122493744\n",
      "0.3657078444957733\n",
      "0.3354945182800293\n",
      "0.3230648934841156\n",
      "0.33025792241096497\n",
      "0.3336712718009949\n",
      "0.3149867355823517\n",
      "0.30683112144470215\n",
      "0.3031422197818756\n",
      "0.3259376585483551\n",
      "0.30745261907577515\n",
      "0.3041743338108063\n",
      "0.30191516876220703\n",
      "0.30404382944107056\n",
      "0.28975051641464233\n",
      "0.2922874689102173\n",
      "0.29828792810440063\n",
      "0.3024911880493164\n",
      "0.3197266459465027\n",
      "0.2959238886833191\n",
      "0.28482943773269653\n",
      "0.28801682591438293\n",
      "0.3185204565525055\n",
      "0.2715175151824951\n",
      "0.2924477458000183\n",
      "0.2989695370197296\n",
      "0.281375527381897\n",
      "0.27676498889923096\n",
      "0.29948675632476807\n",
      "0.27484557032585144\n",
      "0.265476793050766\n",
      "0.2891625165939331\n",
      "0.29610297083854675\n",
      "0.2666754424571991\n",
      "0.27740803360939026\n",
      "0.25266218185424805\n",
      "0.29121434688568115\n",
      "0.2648676633834839\n",
      "0.25579530000686646\n",
      "0.27021971344947815\n",
      "0.28020337224006653\n",
      "0.2659341096878052\n",
      "0.2573183476924896\n",
      "0.28301966190338135\n",
      "0.2741205096244812\n",
      "0.2624055743217468\n",
      "0.25729671120643616\n",
      "0.24822662770748138\n",
      "0.24393025040626526\n",
      "0.2591632008552551\n",
      "0.2544196546077728\n",
      "0.25150272250175476\n",
      "0.2561357319355011\n",
      "0.2810530960559845\n",
      "0.2516511380672455\n",
      "0.25286900997161865\n",
      "0.26666897535324097\n",
      "0.23195591568946838\n",
      "0.2414042055606842\n",
      "0.22856754064559937\n",
      "0.2568228542804718\n",
      "0.24913525581359863\n",
      "0.24980293214321136\n",
      "0.24161602556705475\n",
      "0.24182209372520447\n",
      "0.22993414103984833\n",
      "0.24370145797729492\n",
      "0.23888525366783142\n",
      "0.2504957914352417\n",
      "0.26098063588142395\n",
      "0.2634103298187256\n",
      "0.26961493492126465\n",
      "0.24900813400745392\n",
      "0.24664200842380524\n",
      "0.25346651673316956\n",
      "0.2327694445848465\n",
      "0.23967188596725464\n",
      "0.24005769193172455\n",
      "0.24623197317123413\n",
      "0.24366359412670135\n",
      "0.24701867997646332\n",
      "0.24394889175891876\n",
      "0.2331274151802063\n",
      "0.23563264310359955\n",
      "0.24350935220718384\n",
      "0.24433784186840057\n",
      "0.22651220858097076\n",
      "0.22838205099105835\n",
      "0.25182607769966125\n",
      "0.23924194276332855\n",
      "0.23454022407531738\n",
      "0.2388886660337448\n",
      "0.25737515091896057\n",
      "0.270159512758255\n",
      "0.24337618052959442\n",
      "0.23429134488105774\n",
      "0.22167502343654633\n",
      "0.22764605283737183\n",
      "0.23595835268497467\n",
      "0.22792765498161316\n",
      "0.22530364990234375\n",
      "0.24214375019073486\n",
      "0.23308615386486053\n",
      "0.23376338183879852\n",
      "0.25632917881011963\n",
      "0.23266524076461792\n",
      "0.224086731672287\n",
      "0.22453175485134125\n",
      "0.2229185402393341\n",
      "0.2430843710899353\n",
      "0.23886728286743164\n",
      "0.22645659744739532\n",
      "0.2367626279592514\n",
      "0.23843756318092346\n",
      "0.21609985828399658\n",
      "0.23477940261363983\n",
      "0.22139917314052582\n",
      "0.2406538873910904\n",
      "0.23926080763339996\n",
      "0.23749004304409027\n",
      "0.23647285997867584\n",
      "0.24199427664279938\n",
      "0.24125531315803528\n",
      "0.22673042118549347\n",
      "0.21920569241046906\n",
      "0.2164575606584549\n",
      "0.22911836206912994\n",
      "0.22103828191757202\n",
      "0.2219073474407196\n",
      "0.22451938688755035\n",
      "0.2436436265707016\n",
      "0.22699040174484253\n",
      "0.21989871561527252\n",
      "0.2122403383255005\n",
      "0.22843492031097412\n",
      "0.22173623740673065\n",
      "0.22908678650856018\n",
      "0.23497092723846436\n",
      "0.22081735730171204\n",
      "0.22736112773418427\n",
      "0.2219129353761673\n",
      "0.21109631657600403\n",
      "0.2367890626192093\n",
      "0.22967135906219482\n",
      "0.22599287331104279\n",
      "0.21710151433944702\n",
      "0.22279317677021027\n",
      "0.2323983609676361\n",
      "0.22169382870197296\n",
      "0.23270340263843536\n",
      "0.23537933826446533\n",
      "0.20709553360939026\n",
      "0.2339949905872345\n",
      "0.2268034815788269\n",
      "0.21467818319797516\n",
      "0.22605930268764496\n",
      "0.23651178181171417\n",
      "0.2116209864616394\n",
      "0.21258868277072906\n",
      "0.24159592390060425\n",
      "0.21394836902618408\n",
      "0.2188434600830078\n",
      "0.217795729637146\n",
      "0.245324969291687\n",
      "0.24410033226013184\n",
      "0.2100633680820465\n",
      "0.22888736426830292\n",
      "0.22639314830303192\n",
      "0.2279140204191208\n",
      "0.22822877764701843\n",
      "0.22549483180046082\n",
      "0.23516100645065308\n",
      "0.2304651290178299\n",
      "0.24508164823055267\n",
      "0.2322581708431244\n",
      "0.22656241059303284\n",
      "0.23217786848545074\n",
      "0.22542895376682281\n",
      "0.2251855880022049\n",
      "0.22964875400066376\n",
      "0.22216512262821198\n",
      "0.23354798555374146\n",
      "0.2186456322669983\n",
      "0.22352835536003113\n",
      "0.2084781676530838\n",
      "0.19913525879383087\n",
      "0.20841063559055328\n",
      "0.21822145581245422\n",
      "0.21165627241134644\n",
      "0.23981121182441711\n",
      "0.22345133125782013\n",
      "0.23212039470672607\n",
      "0.2243914157152176\n",
      "0.22122174501419067\n",
      "0.21640028059482574\n",
      "0.1958993524312973\n",
      "0.20899540185928345\n",
      "0.23074284195899963\n",
      "0.2602076232433319\n",
      "0.23823048174381256\n",
      "0.22300943732261658\n",
      "0.22312456369400024\n",
      "0.21814367175102234\n",
      "0.21840716898441315\n",
      "0.19901444017887115\n",
      "0.21835365891456604\n",
      "0.20017372071743011\n",
      "0.2251121550798416\n",
      "0.24042591452598572\n",
      "0.24330534040927887\n",
      "0.23581953346729279\n",
      "0.23220223188400269\n",
      "0.20127466320991516\n",
      "0.21643495559692383\n",
      "0.20227743685245514\n",
      "0.2247290164232254\n",
      "0.2088925987482071\n",
      "0.22461675107479095\n",
      "0.22562271356582642\n",
      "0.21132513880729675\n",
      "0.21176859736442566\n",
      "0.21981485188007355\n",
      "0.2071934938430786\n",
      "0.2246568351984024\n",
      "0.20606055855751038\n",
      "0.20427396893501282\n",
      "0.21510902047157288\n",
      "0.21288590133190155\n",
      "0.22552335262298584\n",
      "0.1974770873785019\n",
      "0.19906310737133026\n",
      "0.21694211661815643\n",
      "0.2149611860513687\n",
      "0.21227753162384033\n",
      "0.217220276594162\n",
      "0.22837072610855103\n",
      "0.22265659272670746\n",
      "0.19606447219848633\n",
      "0.21329271793365479\n",
      "0.19986145198345184\n",
      "0.20932340621948242\n",
      "0.2289903312921524\n",
      "0.23119601607322693\n",
      "0.2189488559961319\n",
      "0.24015451967716217\n",
      "0.2401444911956787\n",
      "0.24019856750965118\n",
      "0.21511249244213104\n",
      "0.24341417849063873\n",
      "0.23112739622592926\n",
      "0.23840466141700745\n",
      "0.2432490587234497\n",
      "0.23816896975040436\n",
      "0.2257632613182068\n",
      "0.21503356099128723\n",
      "0.24338661134243011\n",
      "0.21878491342067719\n",
      "0.2237052172422409\n",
      "0.24244751036167145\n",
      "0.2190941721200943\n",
      "0.2174348086118698\n",
      "0.21892152726650238\n",
      "0.22649340331554413\n",
      "0.21560300886631012\n",
      "0.2063656747341156\n",
      "0.21472853422164917\n",
      "0.20567040145397186\n",
      "0.2136738896369934\n",
      "0.20280133187770844\n",
      "0.23703737556934357\n",
      "0.23376132547855377\n",
      "0.2076788693666458\n",
      "0.224248006939888\n",
      "0.21281953155994415\n",
      "0.2155160903930664\n",
      "0.22246186435222626\n",
      "0.20913277566432953\n",
      "0.21674470603466034\n",
      "0.2070128619670868\n",
      "0.2151825875043869\n",
      "0.20498129725456238\n",
      "0.22109848260879517\n",
      "0.2398039996623993\n",
      "0.2100857049226761\n",
      "0.2035069614648819\n",
      "0.22513139247894287\n",
      "0.22154995799064636\n",
      "0.209263876080513\n",
      "0.22029736638069153\n",
      "0.19369736313819885\n",
      "0.21328924596309662\n",
      "0.21854948997497559\n",
      "0.21194913983345032\n",
      "0.22093439102172852\n",
      "0.20247192680835724\n",
      "0.202945739030838\n",
      "0.1991906464099884\n",
      "0.23010531067848206\n",
      "0.221462681889534\n",
      "0.220295712351799\n",
      "0.22410763800144196\n",
      "0.23664331436157227\n",
      "0.24494220316410065\n",
      "0.22480280697345734\n",
      "0.2224220335483551\n",
      "0.2251732349395752\n",
      "0.20870035886764526\n",
      "0.201063334941864\n",
      "0.229119673371315\n",
      "0.19869622588157654\n",
      "0.20915472507476807\n",
      "0.20841947197914124\n",
      "0.2092267870903015\n",
      "0.22275321185588837\n",
      "0.2231731116771698\n",
      "0.20920322835445404\n",
      "0.20147588849067688\n",
      "0.20200204849243164\n",
      "0.21153901517391205\n",
      "0.2074144333600998\n",
      "0.2179669588804245\n",
      "0.21178855001926422\n",
      "0.19130772352218628\n",
      "0.2156432718038559\n",
      "0.2282647341489792\n",
      "0.21997536718845367\n",
      "0.22877705097198486\n",
      "0.21842123568058014\n",
      "0.22302721440792084\n",
      "0.21928106248378754\n",
      "0.2383836805820465\n",
      "0.21849705278873444\n",
      "0.206686869263649\n",
      "0.2195112556219101\n",
      "0.23648294806480408\n",
      "0.21717879176139832\n",
      "0.23293578624725342\n",
      "0.2309766560792923\n",
      "0.234932079911232\n",
      "0.2181718945503235\n",
      "0.21789586544036865\n",
      "0.19813236594200134\n"
     ]
    }
   ],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=300, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "\n",
    "\n",
    "γ =  0.1**2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = gaussian_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0] // 5), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"uniform\",\n",
    "    γ_min= 0.1**2, γ_max= 0.4**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([(net.forward(X, θ)[None,...]).softmax(dim=-1) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "((pred.argmax(dim=-1)).float().flatten()== y_train.argmax(dim=-1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred_test.argmax(dim=-1)).float().flatten()== y_test.argmax(dim=-1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
