{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from torch import datasets\n",
    "\n",
    "from torch import _vmap_internals\n",
    "from torchvision import datasets, transforms\n",
    "from functorch import vmap\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functorch succesfully imported\n"
     ]
    }
   ],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.drifts import *\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Cat}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = datasets.MNIST(\"../data/mnist/\", download=True, transform=ToTensor(), train=True)\n",
    "images_test = datasets.MNIST(\"../data/mnist/\", download=True, transform=ToTensor(), train=False)\n",
    "\n",
    "transform = torch.nn.Sequential(transforms.Normalize((0.1307,), (0.3081)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = images_train.data, images_train.targets\n",
    "X_test, y_test = images_test.data, images_test.targets\n",
    "\n",
    "X_train = transform(X_train.float())\n",
    "X_test = transform(X_test.float())\n",
    "\n",
    "y_train = F.one_hot(y_train)\n",
    "y_test = F.one_hot(y_test)\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31737/1347329722.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_31737/1347329722.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_31737/1347329722.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_31737/1347329722.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_test, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = torch.nn.Sequential(            \n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=256, out_features=120),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=84, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "from functorch import make_functional\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LeNet5Fun(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.model = LeNet5(n_classes=10)\n",
    "        self.func_model, self.params = make_functional(self.model)\n",
    "        \n",
    "        \n",
    "        self.dim = sum([math.prod(x.shape) for x in self.params])\n",
    "        \n",
    "        self.size_tuples = [p.shape for p in self.params]\n",
    "\n",
    "    def get_params_from_array(self, array):\n",
    "        cur_index = 0\n",
    "        param_list = []\n",
    "        for s in self.size_tuples:\n",
    "            step_number = math.prod(s)\n",
    "            param_list.append(array[cur_index:cur_index+step_number].reshape(s))\n",
    "            cur_index += step_number\n",
    "        return param_list\n",
    "    \n",
    "    def forward(self, x, Θ):\n",
    "        Θ = self.get_params_from_array(Θ)\n",
    "        return self.func_model(Θ, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "\n",
    "# net = ClassificationNetwork(\n",
    "#     dim, out_dim, device=device, depth=1, width=50, activation=F.tanh\n",
    "# )\n",
    "net = LeNet5Fun()\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=3.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        cel = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ll_cel = -1.0 * cel(preds, y.argmax(dim=1))\n",
    "        return ll_cel\n",
    "    \n",
    "    batched_loss =  vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44426"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef7dd4e0ae14e1b9d7222461fa9f5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2378: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::batch_norm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch.batch_norm(\n",
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2942: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::cross_entropy_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6960827112197876\n",
      "1.7166500091552734\n",
      "1.7330795526504517\n",
      "1.7196152210235596\n",
      "1.651671051979065\n",
      "1.5884760618209839\n",
      "1.4036871194839478\n",
      "1.2798196077346802\n",
      "1.1249005794525146\n",
      "0.9033806920051575\n",
      "0.7390279769897461\n",
      "0.6206254363059998\n",
      "0.476980060338974\n",
      "0.39369624853134155\n",
      "0.2911602556705475\n",
      "0.26078811287879944\n",
      "0.13060130178928375\n",
      "0.1005076989531517\n",
      "0.04203454405069351\n",
      "-0.020976228639483452\n",
      "-0.07784675806760788\n",
      "-0.04950717091560364\n",
      "-0.11115044355392456\n",
      "-0.13782377541065216\n",
      "-0.16209064424037933\n",
      "-0.20069517195224762\n",
      "-0.2191692292690277\n",
      "-0.2233324497938156\n",
      "-0.2747950255870819\n",
      "-0.23233062028884888\n",
      "-0.29939115047454834\n",
      "-0.30652865767478943\n",
      "-0.29169318079948425\n",
      "-0.33285239338874817\n",
      "-0.344258189201355\n",
      "-0.37027525901794434\n",
      "-0.34104594588279724\n",
      "-0.35482335090637207\n",
      "-0.3695380389690399\n",
      "-0.39507564902305603\n",
      "-0.3938941955566406\n",
      "-0.37779316306114197\n",
      "-0.401852011680603\n",
      "-0.3810427188873291\n",
      "-0.4184652864933014\n",
      "-0.42003941535949707\n",
      "-0.4380367398262024\n",
      "-0.42583727836608887\n",
      "-0.445064514875412\n",
      "-0.42867353558540344\n",
      "-0.43868863582611084\n",
      "-0.42914557456970215\n",
      "-0.4263988137245178\n",
      "-0.4204554855823517\n",
      "-0.46036773920059204\n",
      "-0.44842344522476196\n",
      "-0.45177891850471497\n",
      "-0.45436376333236694\n",
      "-0.4536888003349304\n",
      "-0.4117008447647095\n",
      "-0.45488592982292175\n",
      "-0.46083545684814453\n",
      "-0.476873517036438\n",
      "-0.4398171007633209\n",
      "-0.46136000752449036\n",
      "-0.45987504720687866\n",
      "-0.47443142533302307\n",
      "-0.455160528421402\n",
      "-0.46053755283355713\n",
      "-0.4786985516548157\n",
      "-0.4771938920021057\n",
      "-0.4717819094657898\n",
      "-0.4929226040840149\n",
      "-0.4602954387664795\n",
      "-0.4780106842517853\n",
      "-0.4986957013607025\n",
      "-0.4945176839828491\n",
      "-0.4769751727581024\n",
      "-0.5260175466537476\n",
      "-0.509573221206665\n",
      "-0.4926450550556183\n",
      "-0.511059582233429\n",
      "-0.48533156514167786\n",
      "-0.4942111372947693\n",
      "-0.4655316472053528\n",
      "-0.5128034949302673\n",
      "-0.49283480644226074\n",
      "-0.508822500705719\n",
      "-0.5032649636268616\n",
      "-0.49872374534606934\n",
      "-0.49768224358558655\n",
      "-0.49089741706848145\n",
      "-0.4770662784576416\n",
      "-0.4747694432735443\n",
      "-0.49608540534973145\n",
      "-0.45747268199920654\n",
      "-0.4841548502445221\n",
      "-0.49324139952659607\n",
      "-0.4949716627597809\n",
      "-0.4774657189846039\n",
      "-0.530635416507721\n",
      "-0.49065983295440674\n",
      "-0.46110257506370544\n",
      "-0.4830248951911926\n",
      "-0.5170725584030151\n",
      "-0.49038317799568176\n",
      "-0.5026376843452454\n",
      "-0.5170837640762329\n",
      "-0.47787001729011536\n",
      "-0.48425114154815674\n",
      "-0.4885149598121643\n",
      "-0.5275128483772278\n",
      "-0.5084891319274902\n",
      "-0.51419997215271\n",
      "-0.5033226609230042\n",
      "-0.5229183435440063\n",
      "-0.5028796195983887\n",
      "-0.517938494682312\n",
      "-0.5221621990203857\n",
      "-0.5187883973121643\n",
      "-0.5158902406692505\n",
      "-0.5252419114112854\n",
      "-0.5095890164375305\n",
      "-0.49792975187301636\n",
      "-0.5090476274490356\n",
      "-0.5190024375915527\n",
      "-0.5005588531494141\n",
      "-0.5229521989822388\n",
      "-0.47974637150764465\n",
      "-0.5037135481834412\n",
      "-0.5005316734313965\n",
      "-0.5033673644065857\n",
      "-0.5196331739425659\n",
      "-0.507446825504303\n",
      "-0.5049558877944946\n",
      "-0.5037089586257935\n",
      "-0.5256892442703247\n",
      "-0.4873392581939697\n",
      "-0.4930516183376312\n",
      "-0.490827351808548\n",
      "-0.5143246650695801\n",
      "-0.5024287700653076\n",
      "-0.5109819173812866\n",
      "-0.4919935166835785\n",
      "-0.5011640191078186\n",
      "-0.5116221308708191\n",
      "-0.5113791227340698\n",
      "-0.5321709513664246\n",
      "-0.5050852298736572\n",
      "-0.5197652578353882\n",
      "-0.4740613102912903\n",
      "-0.5055238604545593\n",
      "-0.5055774450302124\n",
      "-0.525627076625824\n",
      "-0.5073676109313965\n",
      "-0.5076203942298889\n",
      "-0.5125717520713806\n",
      "-0.5042874217033386\n",
      "-0.49587810039520264\n",
      "-0.5107426643371582\n",
      "-0.485134482383728\n",
      "-0.5227347612380981\n",
      "-0.5058167576789856\n",
      "-0.5197023749351501\n",
      "-0.525597870349884\n",
      "-0.5020503401756287\n",
      "-0.5150536894798279\n",
      "-0.5269097685813904\n",
      "-0.5226050615310669\n",
      "-0.5163426995277405\n",
      "-0.5472989082336426\n",
      "-0.5280411243438721\n",
      "-0.5190308094024658\n",
      "-0.5115115642547607\n",
      "-0.48179540038108826\n",
      "-0.49633198976516724\n",
      "-0.5312783122062683\n",
      "-0.5301541686058044\n",
      "-0.5231479406356812\n",
      "-0.5251386761665344\n",
      "-0.5274720191955566\n",
      "-0.5129051208496094\n",
      "-0.5035908818244934\n",
      "-0.5260639786720276\n",
      "-0.5315199494361877\n",
      "-0.5256798267364502\n",
      "-0.5101644992828369\n",
      "-0.5200690031051636\n",
      "-0.4803287982940674\n",
      "-0.5107381343841553\n",
      "-0.49756765365600586\n",
      "-0.5076843500137329\n",
      "-0.5084999799728394\n",
      "-0.522576630115509\n",
      "-0.5214704871177673\n",
      "-0.4941898584365845\n",
      "-0.5153779983520508\n",
      "-0.5208099484443665\n",
      "-0.5248791575431824\n",
      "-0.5138190984725952\n",
      "-0.5204488635063171\n",
      "-0.5168865919113159\n",
      "-0.5026434063911438\n",
      "-0.5023890733718872\n",
      "-0.5432892441749573\n",
      "-0.5127319693565369\n",
      "-0.5335564017295837\n",
      "-0.5064039826393127\n",
      "-0.5269980430603027\n",
      "-0.5294380784034729\n",
      "-0.5337527990341187\n",
      "-0.5272324681282043\n",
      "-0.5093153715133667\n",
      "-0.5276226997375488\n",
      "-0.5450551509857178\n",
      "-0.5346261262893677\n",
      "-0.4917309880256653\n",
      "-0.5145447254180908\n",
      "-0.5210652351379395\n",
      "-0.5310094952583313\n",
      "-0.5287996530532837\n",
      "-0.5115926265716553\n",
      "-0.5312790274620056\n",
      "-0.4895777106285095\n",
      "-0.5161577463150024\n",
      "-0.5121098160743713\n",
      "-0.5102803111076355\n",
      "-0.5187076926231384\n",
      "-0.535537600517273\n",
      "-0.4893087148666382\n",
      "-0.5067564845085144\n",
      "-0.4981229305267334\n",
      "-0.5025862455368042\n",
      "-0.4926561713218689\n",
      "-0.506252110004425\n",
      "-0.505587100982666\n",
      "-0.5105947852134705\n",
      "-0.5048193335533142\n",
      "-0.49916771054267883\n",
      "-0.5184202790260315\n",
      "-0.5006303191184998\n",
      "-0.49065086245536804\n",
      "-0.5179738998413086\n",
      "-0.5016788840293884\n",
      "-0.5074616074562073\n",
      "-0.5173199772834778\n",
      "-0.47475332021713257\n",
      "-0.5174304246902466\n",
      "-0.49453532695770264\n",
      "-0.5187175869941711\n",
      "-0.5174108147621155\n",
      "-0.4972434341907501\n",
      "-0.5430720448493958\n",
      "-0.526648759841919\n",
      "-0.5197714567184448\n",
      "-0.5226826667785645\n",
      "-0.5190981030464172\n",
      "-0.5031192898750305\n",
      "-0.5441915988922119\n",
      "-0.5263563394546509\n",
      "-0.5249104499816895\n",
      "-0.516106903553009\n",
      "-0.5373910069465637\n",
      "-0.5348792672157288\n",
      "-0.49428457021713257\n",
      "-0.5301871299743652\n",
      "-0.5245123505592346\n",
      "-0.5222085118293762\n",
      "-0.5076448321342468\n",
      "-0.5254760980606079\n",
      "-0.5284824967384338\n",
      "-0.5298190712928772\n",
      "-0.5193624496459961\n",
      "-0.5348712205886841\n",
      "-0.5006559491157532\n",
      "-0.522049605846405\n",
      "-0.5283326506614685\n",
      "-0.5137103796005249\n",
      "-0.5156349539756775\n",
      "-0.4981081187725067\n",
      "-0.525824248790741\n",
      "-0.5153257250785828\n",
      "-0.509722888469696\n",
      "-0.5481387376785278\n",
      "-0.5407306551933289\n",
      "-0.5173006653785706\n",
      "-0.5106801986694336\n",
      "-0.5353546738624573\n",
      "-0.5020296573638916\n",
      "-0.5269679427146912\n",
      "-0.5220880508422852\n",
      "-0.5253015160560608\n",
      "-0.5191112160682678\n",
      "-0.5152762532234192\n",
      "-0.5435175895690918\n",
      "-0.4860698878765106\n",
      "-0.49963247776031494\n",
      "-0.5027018785476685\n",
      "-0.5217717289924622\n",
      "-0.518099308013916\n",
      "-0.49387162923812866\n",
      "-0.5261607766151428\n",
      "-0.5088080167770386\n",
      "-0.5360859036445618\n",
      "-0.5141753554344177\n",
      "-0.5173780918121338\n",
      "-0.5361142754554749\n",
      "-0.523198127746582\n",
      "-0.5181431770324707\n",
      "-0.5241609811782837\n",
      "-0.49715879559516907\n",
      "-0.5326115489006042\n",
      "-0.527538001537323\n",
      "-0.5212686657905579\n",
      "-0.5262973308563232\n",
      "-0.5047352910041809\n",
      "-0.5157143473625183\n",
      "-0.5217058658599854\n",
      "-0.5073831081390381\n",
      "-0.5287373661994934\n",
      "-0.5381760001182556\n",
      "-0.4898522198200226\n",
      "-0.4917484521865845\n",
      "-0.5183172225952148\n",
      "-0.5205800533294678\n",
      "-0.4862701892852783\n",
      "-0.53998202085495\n",
      "-0.5318285822868347\n",
      "-0.5292724370956421\n",
      "-0.5202144980430603\n",
      "-0.5434707999229431\n",
      "-0.5237658619880676\n",
      "-0.5014790296554565\n",
      "-0.5090741515159607\n",
      "-0.5466774106025696\n",
      "-0.5345717668533325\n",
      "-0.5239650011062622\n",
      "-0.53661048412323\n",
      "-0.5169364213943481\n",
      "-0.5298159122467041\n",
      "-0.5352747440338135\n",
      "-0.5189912915229797\n",
      "-0.5371544361114502\n",
      "-0.537495493888855\n",
      "-0.501995325088501\n",
      "-0.5432150959968567\n",
      "-0.5204342603683472\n",
      "-0.49983254075050354\n",
      "-0.5368843674659729\n",
      "-0.5241446495056152\n",
      "-0.506458580493927\n",
      "-0.5270947813987732\n",
      "-0.5133833289146423\n",
      "-0.5265661478042603\n",
      "-0.5253531336784363\n",
      "-0.506218433380127\n",
      "-0.528980016708374\n",
      "-0.5358487963676453\n",
      "-0.5145221948623657\n",
      "-0.543962299823761\n",
      "-0.5090565085411072\n",
      "-0.5238998532295227\n",
      "-0.5152206420898438\n",
      "-0.5312052965164185\n",
      "-0.5227692723274231\n",
      "-0.5107840299606323\n",
      "-0.505531907081604\n",
      "-0.5269381999969482\n",
      "-0.5153510570526123\n",
      "-0.5314980149269104\n",
      "-0.5182143449783325\n",
      "-0.5353204607963562\n",
      "-0.5383440256118774\n",
      "-0.48773393034935\n",
      "-0.5231358408927917\n",
      "-0.5121311545372009\n",
      "-0.5281663537025452\n",
      "-0.5086257457733154\n",
      "-0.5285332798957825\n",
      "-0.5370185375213623\n",
      "-0.5383557677268982\n",
      "-0.5281785726547241\n",
      "-0.5247331261634827\n",
      "-0.5130392909049988\n",
      "-0.5264624357223511\n",
      "-0.5375228524208069\n",
      "-0.5138684511184692\n",
      "-0.5273340344429016\n",
      "-0.5290549397468567\n",
      "-0.5166422128677368\n",
      "-0.4943579137325287\n",
      "-0.528219997882843\n",
      "-0.5286886096000671\n",
      "-0.5289623737335205\n",
      "-0.48070815205574036\n",
      "-0.5459582805633545\n",
      "-0.4844684600830078\n",
      "-0.5055813789367676\n",
      "-0.4935638904571533\n",
      "-0.5103614330291748\n",
      "-0.49864742159843445\n",
      "-0.5028448700904846\n",
      "-0.519929051399231\n",
      "-0.5255516171455383\n",
      "-0.5032959580421448\n",
      "-0.5152996182441711\n",
      "-0.5428684949874878\n",
      "-0.5125737190246582\n",
      "-0.5101602077484131\n",
      "-0.5344886779785156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5177499651908875\n",
      "-0.5178843140602112\n",
      "-0.5372817516326904\n",
      "-0.5203011631965637\n",
      "-0.5435367226600647\n",
      "-0.524146318435669\n",
      "-0.49615100026130676\n",
      "-0.5239524841308594\n",
      "-0.5110782384872437\n",
      "-0.49684369564056396\n",
      "-0.5207894444465637\n",
      "-0.4934364855289459\n",
      "-0.5245131254196167\n",
      "-0.5206199288368225\n",
      "-0.5199262499809265\n",
      "-0.5010809898376465\n",
      "-0.5093575119972229\n",
      "-0.5206947922706604\n",
      "-0.533033549785614\n",
      "-0.5145379900932312\n",
      "-0.5177889466285706\n",
      "-0.5178874731063843\n",
      "-0.5073843598365784\n",
      "-0.5368494987487793\n",
      "-0.5304684042930603\n",
      "-0.5131508111953735\n",
      "-0.5310683846473694\n",
      "-0.48769620060920715\n",
      "-0.5164811015129089\n",
      "-0.5093755125999451\n",
      "-0.5095905065536499\n",
      "-0.5289328098297119\n",
      "-0.5354899168014526\n",
      "-0.522897481918335\n",
      "-0.5246502757072449\n",
      "-0.5220800638198853\n",
      "-0.5366208553314209\n",
      "-0.5203928351402283\n",
      "-0.5428689122200012\n",
      "-0.5216640830039978\n",
      "-0.4934418499469757\n",
      "-0.522594153881073\n",
      "-0.5301214456558228\n",
      "-0.54118412733078\n",
      "-0.5020854473114014\n",
      "-0.5388450026512146\n",
      "-0.5050864219665527\n",
      "-0.5172852277755737\n",
      "-0.5106538534164429\n",
      "-0.5174733996391296\n",
      "-0.5140073299407959\n",
      "-0.5175748467445374\n",
      "-0.5258094668388367\n",
      "-0.5322570204734802\n",
      "-0.5297413468360901\n",
      "-0.511953592300415\n",
      "-0.5389207005500793\n",
      "-0.5154681205749512\n",
      "-0.5361394882202148\n",
      "-0.5151103734970093\n",
      "-0.5167252421379089\n",
      "-0.5037469267845154\n",
      "-0.53030925989151\n",
      "-0.512761116027832\n",
      "-0.518479585647583\n",
      "-0.5116452574729919\n",
      "-0.5080702900886536\n",
      "-0.5269149541854858\n",
      "-0.5062195062637329\n",
      "-0.5124866962432861\n",
      "-0.527163565158844\n",
      "-0.5185542702674866\n",
      "-0.5340263843536377\n",
      "-0.5302305817604065\n",
      "-0.514807939529419\n",
      "-0.5139480233192444\n",
      "-0.5208834409713745\n",
      "-0.5451784729957581\n",
      "-0.502419650554657\n",
      "-0.5298178791999817\n",
      "-0.5483053922653198\n",
      "-0.5040953159332275\n",
      "-0.4844418466091156\n",
      "-0.47071877121925354\n",
      "-0.5155850648880005\n",
      "-0.49151280522346497\n",
      "-0.5381061434745789\n",
      "-0.5206462144851685\n",
      "-0.4983803927898407\n",
      "-0.4940028786659241\n",
      "-0.5201811194419861\n",
      "-0.5088517069816589\n",
      "-0.5228447318077087\n",
      "-0.486358106136322\n",
      "-0.513217568397522\n",
      "-0.5331680178642273\n",
      "-0.5371242165565491\n",
      "-0.5207632184028625\n",
      "-0.5323519110679626\n",
      "-0.5168699622154236\n",
      "-0.5218204855918884\n",
      "-0.5365180373191833\n",
      "-0.5135974287986755\n",
      "-0.49597591161727905\n",
      "-0.5018859505653381\n",
      "-0.5056741833686829\n",
      "-0.5191041231155396\n",
      "-0.5048889517784119\n",
      "-0.5311806797981262\n",
      "-0.5178651213645935\n",
      "-0.5485153794288635\n",
      "-0.5319638848304749\n",
      "-0.5132835507392883\n",
      "-0.5116406083106995\n",
      "-0.5282657146453857\n",
      "-0.5008223652839661\n",
      "-0.5540591478347778\n",
      "-0.5097572207450867\n",
      "-0.5437668561935425\n",
      "-0.5223193764686584\n",
      "-0.5281621217727661\n",
      "-0.5212383270263672\n",
      "-0.5307772755622864\n",
      "-0.5364360809326172\n",
      "-0.5237181782722473\n",
      "-0.5348914861679077\n",
      "-0.4916413426399231\n",
      "-0.5272057056427002\n",
      "-0.5255138874053955\n",
      "-0.5315520167350769\n",
      "-0.5252892374992371\n",
      "-0.5170753002166748\n",
      "-0.5148869752883911\n",
      "-0.5238764882087708\n",
      "-0.5206572413444519\n",
      "-0.5453494191169739\n",
      "-0.5003448724746704\n",
      "-0.5356284379959106\n",
      "-0.5012245774269104\n",
      "-0.5433724522590637\n",
      "-0.49938419461250305\n",
      "-0.5279786586761475\n",
      "-0.5101510882377625\n",
      "-0.5126127004623413\n",
      "-0.5236620903015137\n",
      "-0.5371381044387817\n",
      "-0.5337511301040649\n",
      "-0.5118281841278076\n",
      "-0.5329387784004211\n",
      "-0.5408856868743896\n",
      "-0.527845025062561\n",
      "-0.5111241936683655\n",
      "-0.5387625098228455\n",
      "-0.5304572582244873\n",
      "-0.5350130796432495\n",
      "-0.527928352355957\n",
      "-0.5418198704719543\n",
      "-0.5109444260597229\n",
      "-0.5347106456756592\n",
      "-0.5397453308105469\n",
      "-0.5106333494186401\n",
      "-0.518057107925415\n",
      "-0.5386489629745483\n",
      "-0.5201618075370789\n",
      "-0.5247848629951477\n",
      "-0.5358558297157288\n",
      "-0.5188471674919128\n",
      "-0.50711590051651\n",
      "-0.5033650994300842\n",
      "-0.5258846282958984\n",
      "-0.5419442653656006\n",
      "-0.5412260890007019\n",
      "-0.5331623554229736\n",
      "-0.5248016715049744\n",
      "-0.5173653364181519\n",
      "-0.538909912109375\n",
      "-0.5358133316040039\n",
      "-0.5201948881149292\n",
      "-0.5394155383110046\n",
      "-0.5407963991165161\n",
      "-0.5515053272247314\n",
      "-0.5299350619316101\n",
      "-0.5267622470855713\n",
      "-0.5305059552192688\n",
      "-0.5109828114509583\n",
      "-0.5146158933639526\n",
      "-0.5440548658370972\n",
      "-0.5220214128494263\n",
      "-0.5523742437362671\n",
      "-0.5012243390083313\n",
      "-0.5242348313331604\n",
      "-0.5057694911956787\n",
      "-0.5365650653839111\n",
      "-0.5303745269775391\n",
      "-0.48844313621520996\n",
      "-0.5469647645950317\n",
      "-0.49740713834762573\n",
      "-0.5333338379859924\n",
      "-0.529405951499939\n",
      "-0.48790839314460754\n",
      "-0.5064197778701782\n",
      "-0.5188513994216919\n",
      "-0.5005674362182617\n",
      "-0.5431098937988281\n",
      "-0.4888564348220825\n",
      "-0.5114461183547974\n",
      "-0.5245518088340759\n",
      "-0.5107483863830566\n",
      "-0.5436476469039917\n",
      "-0.5189555883407593\n",
      "-0.5544928312301636\n",
      "-0.5289908647537231\n",
      "-0.5302090644836426\n",
      "-0.5214346051216125\n",
      "-0.5327288508415222\n",
      "-0.5143963694572449\n",
      "-0.517241895198822\n",
      "-0.5345509648323059\n",
      "-0.5192800760269165\n",
      "-0.5272055864334106\n",
      "-0.5318012833595276\n",
      "-0.5197898149490356\n",
      "-0.5163087248802185\n",
      "-0.5087954998016357\n",
      "-0.5265867114067078\n",
      "-0.5332913398742676\n",
      "-0.4856846034526825\n",
      "-0.5039142370223999\n",
      "-0.4866267442703247\n",
      "-0.5252180695533752\n",
      "-0.4796055853366852\n",
      "-0.5294436812400818\n",
      "-0.49948978424072266\n",
      "-0.5106590390205383\n",
      "-0.4951975345611572\n",
      "-0.49714383482933044\n",
      "-0.53924959897995\n",
      "-0.46461889147758484\n",
      "-0.5125676989555359\n",
      "-0.5274503827095032\n",
      "-0.46207159757614136\n",
      "-0.5228388905525208\n",
      "-0.5082805752754211\n",
      "-0.5327348113059998\n",
      "-0.5269034504890442\n",
      "-0.5013537406921387\n",
      "-0.5004029870033264\n",
      "-0.512173593044281\n",
      "-0.5168583393096924\n",
      "-0.5295484662055969\n",
      "-0.5396152138710022\n",
      "-0.5190036296844482\n",
      "-0.5462859869003296\n",
      "-0.5020739436149597\n",
      "-0.5343105792999268\n",
      "-0.5394805669784546\n",
      "-0.5258307456970215\n",
      "-0.5328420996665955\n",
      "-0.5144855380058289\n",
      "-0.5203590989112854\n",
      "-0.543091893196106\n",
      "-0.533957302570343\n",
      "-0.5065628290176392\n",
      "-0.5241258144378662\n",
      "-0.5554516315460205\n",
      "-0.537212073802948\n",
      "-0.541547417640686\n",
      "-0.5140411853790283\n",
      "-0.5365557074546814\n",
      "-0.5222930312156677\n",
      "-0.5363956093788147\n",
      "-0.5457683205604553\n",
      "-0.5317568182945251\n",
      "-0.5198695659637451\n",
      "-0.5147103667259216\n",
      "-0.5342657566070557\n",
      "-0.5261424779891968\n",
      "-0.5138716101646423\n",
      "-0.5212904214859009\n",
      "-0.5317530632019043\n",
      "-0.5482367873191833\n",
      "-0.5310010313987732\n",
      "-0.5392897129058838\n",
      "-0.5337245464324951\n",
      "-0.5366421937942505\n",
      "-0.5198440551757812\n",
      "-0.5147563219070435\n",
      "-0.5362439155578613\n",
      "-0.5305091738700867\n",
      "-0.5221937894821167\n",
      "-0.5378808379173279\n",
      "-0.5281257033348083\n",
      "-0.5360371470451355\n",
      "-0.5079965591430664\n",
      "-0.5466656684875488\n",
      "-0.5225796103477478\n",
      "-0.5084407329559326\n",
      "-0.5331194400787354\n",
      "-0.5082842707633972\n",
      "-0.5245596766471863\n",
      "-0.5234495997428894\n",
      "-0.5058648586273193\n",
      "-0.5275353789329529\n",
      "-0.5177718997001648\n",
      "-0.5036766529083252\n",
      "-0.5184589624404907\n",
      "-0.5209476947784424\n",
      "-0.5496435761451721\n",
      "-0.5416528582572937\n",
      "-0.5260392427444458\n",
      "-0.5102946758270264\n",
      "-0.5224985480308533\n",
      "-0.5135537981987\n",
      "-0.5290043950080872\n",
      "-0.5244109034538269\n",
      "-0.5332865118980408\n",
      "-0.5438339710235596\n",
      "-0.5353232622146606\n",
      "-0.5162671208381653\n",
      "-0.5235955715179443\n",
      "-0.5095367431640625\n",
      "-0.521130383014679\n",
      "-0.5256868600845337\n",
      "-0.506287693977356\n",
      "-0.5262461304664612\n",
      "-0.5198781490325928\n",
      "-0.5081884860992432\n",
      "-0.5214464664459229\n",
      "-0.5121439099311829\n",
      "-0.5080499053001404\n",
      "-0.5341535806655884\n",
      "-0.5287631154060364\n",
      "-0.5264503359794617\n",
      "-0.5523403286933899\n",
      "-0.5159094929695129\n",
      "-0.5432584285736084\n",
      "-0.5245873928070068\n",
      "-0.5394041538238525\n",
      "-0.5241494178771973\n",
      "-0.529805064201355\n",
      "-0.5123069882392883\n",
      "-0.526982843875885\n",
      "-0.5182487964630127\n",
      "-0.5177713632583618\n",
      "-0.5257800817489624\n",
      "-0.537625789642334\n",
      "-0.5264902114868164\n",
      "-0.5083596706390381\n",
      "-0.5117446184158325\n",
      "-0.5408147573471069\n",
      "-0.49077391624450684\n",
      "-0.5289395451545715\n",
      "-0.5277888774871826\n",
      "-0.5366790890693665\n",
      "-0.5283440947532654\n",
      "-0.5348333120346069\n",
      "-0.524381697177887\n",
      "-0.5420147776603699\n",
      "-0.5087358951568604\n",
      "-0.54339998960495\n",
      "-0.5356297492980957\n",
      "-0.5344972014427185\n",
      "-0.5336043834686279\n",
      "-0.5274875164031982\n",
      "-0.5328178405761719\n",
      "-0.526642918586731\n",
      "-0.5415405035018921\n",
      "-0.5346441864967346\n",
      "-0.5365528464317322\n",
      "-0.5326988697052002\n",
      "-0.5214187502861023\n",
      "-0.5232658982276917\n",
      "-0.5091255903244019\n",
      "-0.52362459897995\n",
      "-0.5346755981445312\n",
      "-0.5276246666908264\n",
      "-0.5310399532318115\n",
      "-0.5534769892692566\n",
      "-0.5171660780906677\n",
      "-0.5312540531158447\n",
      "-0.5385110378265381\n",
      "-0.49970778822898865\n",
      "-0.5244998335838318\n",
      "-0.534481942653656\n",
      "-0.533620297908783\n",
      "-0.5345547795295715\n",
      "-0.48655980825424194\n",
      "-0.5419482588768005\n",
      "-0.5183572173118591\n",
      "-0.5438750982284546\n",
      "-0.5330485105514526\n",
      "-0.5170577764511108\n",
      "-0.4944041669368744\n",
      "-0.5309734344482422\n",
      "-0.5139845013618469\n",
      "-0.499935507774353\n",
      "-0.5517234206199646\n",
      "-0.5234531760215759\n",
      "-0.48393315076828003\n",
      "-0.5184749960899353\n",
      "-0.542797863483429\n",
      "-0.5438400506973267\n",
      "-0.5211430788040161\n",
      "-0.5385243892669678\n",
      "-0.5627111196517944\n",
      "-0.5435050129890442\n",
      "-0.5247955322265625\n",
      "-0.5353678464889526\n",
      "-0.5264366865158081\n",
      "-0.5396717190742493\n",
      "-0.5190136432647705\n",
      "-0.5401453971862793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5508491396903992\n",
      "-0.530553936958313\n",
      "-0.5134468674659729\n",
      "-0.5318052172660828\n",
      "-0.5098716020584106\n",
      "-0.5238289833068848\n",
      "-0.5335902571678162\n",
      "-0.5311751365661621\n",
      "-0.5206242799758911\n",
      "-0.5360412001609802\n",
      "-0.5256909728050232\n",
      "-0.5374013185501099\n",
      "-0.5216191411018372\n",
      "-0.529878556728363\n",
      "-0.5300170183181763\n",
      "-0.5313544273376465\n",
      "-0.5414072275161743\n",
      "-0.5231736898422241\n",
      "-0.5340124368667603\n",
      "-0.49592655897140503\n",
      "-0.5441533923149109\n",
      "-0.48223480582237244\n",
      "-0.5139981508255005\n",
      "-0.526178240776062\n",
      "-0.5068190693855286\n",
      "-0.533254861831665\n",
      "-0.5182589292526245\n",
      "-0.49367305636405945\n",
      "-0.49515119194984436\n",
      "-0.48623156547546387\n",
      "-0.5033935904502869\n",
      "-0.503464937210083\n",
      "-0.5502143502235413\n",
      "-0.5163991451263428\n",
      "-0.5251315832138062\n",
      "-0.5199782848358154\n",
      "-0.5260187387466431\n",
      "-0.5190722346305847\n",
      "-0.5550847053527832\n",
      "-0.5063503980636597\n",
      "-0.520051121711731\n",
      "-0.51236891746521\n",
      "-0.5214918255805969\n",
      "-0.5156753659248352\n",
      "-0.49985411763191223\n",
      "-0.5317153334617615\n",
      "-0.5206975936889648\n",
      "-0.5179317593574524\n",
      "-0.525216281414032\n",
      "-0.5220448970794678\n",
      "-0.5127182006835938\n",
      "-0.5307905673980713\n",
      "-0.527599036693573\n",
      "-0.5154536366462708\n",
      "-0.5078173279762268\n",
      "-0.5196583271026611\n",
      "-0.5180038809776306\n",
      "-0.5153553485870361\n",
      "-0.5360896587371826\n",
      "-0.5198965668678284\n",
      "-0.5241526365280151\n",
      "-0.5350338816642761\n",
      "-0.5471000075340271\n",
      "-0.5358838438987732\n",
      "-0.5408827662467957\n",
      "-0.5430265069007874\n",
      "-0.5069058537483215\n",
      "-0.5196431875228882\n",
      "-0.5280835032463074\n",
      "-0.5381121635437012\n",
      "-0.5384214520454407\n",
      "-0.50594162940979\n",
      "-0.5255531072616577\n",
      "-0.528815507888794\n",
      "-0.5233866572380066\n",
      "-0.5444486141204834\n",
      "-0.537401020526886\n",
      "-0.5292797684669495\n",
      "-0.5310059189796448\n",
      "-0.47907984256744385\n",
      "-0.5281097888946533\n",
      "-0.4987451434135437\n",
      "-0.5361812710762024\n",
      "-0.5238009095191956\n",
      "-0.4916408956050873\n",
      "-0.49744880199432373\n",
      "-0.5254693031311035\n",
      "-0.525580108165741\n",
      "-0.5269426703453064\n",
      "-0.5162554979324341\n",
      "-0.5172237753868103\n",
      "-0.5246480107307434\n",
      "-0.5271934866905212\n",
      "-0.549035906791687\n",
      "-0.5172573328018188\n",
      "-0.5275929570198059\n",
      "-0.534782886505127\n",
      "-0.5327509045600891\n",
      "-0.5250580906867981\n",
      "-0.515978991985321\n",
      "-0.5239278078079224\n",
      "-0.5363279581069946\n",
      "-0.5256381630897522\n",
      "-0.5176860094070435\n",
      "-0.541052520275116\n",
      "-0.5233217477798462\n",
      "-0.5420850515365601\n",
      "-0.5323953628540039\n",
      "-0.5348544716835022\n",
      "-0.5465352535247803\n",
      "-0.5325998663902283\n",
      "-0.5351105332374573\n",
      "-0.5272653102874756\n",
      "-0.5422027111053467\n",
      "-0.5229833722114563\n",
      "-0.4887922406196594\n",
      "-0.5426267385482788\n",
      "-0.5134076476097107\n",
      "-0.524857759475708\n",
      "-0.5154001712799072\n",
      "-0.5564873218536377\n",
      "-0.5223143100738525\n",
      "-0.534720778465271\n",
      "-0.5298190712928772\n",
      "-0.5189875960350037\n",
      "-0.5272138118743896\n",
      "-0.5353482961654663\n",
      "-0.5238709449768066\n",
      "-0.5485771894454956\n",
      "-0.5202363133430481\n",
      "-0.5421735048294067\n",
      "-0.5376526117324829\n",
      "-0.5654726028442383\n",
      "-0.5362392663955688\n",
      "-0.5474432706832886\n",
      "-0.5412431359291077\n",
      "-0.5305811762809753\n",
      "-0.5393996834754944\n",
      "-0.5206943154335022\n",
      "-0.5216649770736694\n",
      "-0.5221924781799316\n",
      "-0.5388296842575073\n",
      "-0.536950945854187\n",
      "-0.5191792249679565\n",
      "-0.5405945777893066\n",
      "-0.5578754544258118\n",
      "-0.5265330076217651\n",
      "-0.5226873159408569\n",
      "-0.5567393898963928\n",
      "-0.5267951488494873\n",
      "-0.5426671504974365\n",
      "-0.5164518356323242\n",
      "-0.5295325517654419\n",
      "-0.5304170846939087\n",
      "-0.5394349098205566\n",
      "-0.5471904873847961\n",
      "-0.5191967487335205\n",
      "-0.5196478366851807\n",
      "-0.5245420932769775\n",
      "-0.530274510383606\n",
      "-0.5352201461791992\n",
      "-0.5093855857849121\n",
      "-0.5450020432472229\n",
      "-0.49628946185112\n",
      "-0.5187751650810242\n",
      "-0.5063601732254028\n",
      "-0.5163112878799438\n",
      "-0.5090341567993164\n",
      "-0.5333502292633057\n",
      "-0.508241593837738\n",
      "-0.49824556708335876\n",
      "-0.5217830538749695\n",
      "-0.47226235270500183\n",
      "-0.5282406806945801\n",
      "-0.511448860168457\n",
      "-0.5411348938941956\n",
      "-0.5114507079124451\n",
      "-0.5179120898246765\n",
      "-0.5203949213027954\n",
      "-0.5253305435180664\n",
      "-0.5367801189422607\n",
      "-0.5299441814422607\n",
      "-0.49424099922180176\n",
      "-0.5442627668380737\n",
      "-0.533348560333252\n",
      "-0.5413694977760315\n",
      "-0.5216419696807861\n",
      "-0.5413280725479126\n",
      "-0.5477780699729919\n",
      "-0.5075861215591431\n",
      "-0.5200598239898682\n",
      "-0.5308375358581543\n",
      "-0.512039840221405\n",
      "-0.5137810707092285\n",
      "-0.5272504091262817\n",
      "-0.5349390506744385\n",
      "-0.5210084319114685\n",
      "-0.51413893699646\n",
      "-0.5303916931152344\n",
      "-0.5336359143257141\n",
      "-0.5176078677177429\n",
      "-0.5461829900741577\n",
      "-0.5367170572280884\n",
      "-0.5295886397361755\n",
      "-0.5503808856010437\n",
      "-0.5080942511558533\n",
      "-0.5382039546966553\n",
      "-0.51578688621521\n",
      "-0.5249858498573303\n",
      "-0.5212079882621765\n",
      "-0.5284032225608826\n",
      "-0.5204919576644897\n",
      "-0.5489109754562378\n",
      "-0.5042460560798645\n",
      "-0.5136681795120239\n",
      "-0.5312813520431519\n",
      "-0.5148833394050598\n",
      "-0.5318388342857361\n",
      "-0.5194292664527893\n",
      "-0.5263667106628418\n",
      "-0.5297852158546448\n",
      "-0.5361775755882263\n",
      "-0.5028405785560608\n",
      "-0.5309253931045532\n",
      "-0.5114377737045288\n",
      "-0.5365121364593506\n",
      "-0.5255023837089539\n",
      "-0.5440829396247864\n",
      "-0.5167680978775024\n",
      "-0.5398057103157043\n",
      "-0.5126295685768127\n",
      "-0.52452552318573\n",
      "-0.5510052442550659\n",
      "-0.5230153203010559\n",
      "-0.5374511480331421\n",
      "-0.5287874341011047\n",
      "-0.5081909894943237\n",
      "-0.541416347026825\n",
      "-0.5491302609443665\n",
      "-0.5250238180160522\n",
      "-0.5114510655403137\n",
      "-0.5339866280555725\n",
      "-0.5242002010345459\n",
      "-0.5254864692687988\n",
      "-0.5349580645561218\n",
      "-0.5241592526435852\n",
      "-0.5255563259124756\n",
      "-0.5218407511711121\n",
      "-0.5281834006309509\n",
      "-0.5424907803535461\n",
      "-0.4986039102077484\n",
      "-0.522513747215271\n",
      "-0.5028983950614929\n",
      "-0.5203316807746887\n",
      "-0.5030502676963806\n",
      "-0.5420010685920715\n",
      "-0.542209804058075\n",
      "-0.5160583853721619\n",
      "-0.549175500869751\n",
      "-0.5307968854904175\n",
      "-0.5250225067138672\n",
      "-0.5125175714492798\n",
      "-0.5391137003898621\n",
      "-0.5407633781433105\n",
      "-0.5188634991645813\n",
      "-0.5263848900794983\n",
      "-0.5281538367271423\n",
      "-0.5011239051818848\n",
      "-0.5353004336357117\n",
      "-0.5341993570327759\n",
      "-0.5160385370254517\n",
      "-0.5325303673744202\n",
      "-0.49734142422676086\n",
      "-0.5240258574485779\n",
      "-0.4970709979534149\n",
      "-0.5171405076980591\n",
      "-0.5198907852172852\n",
      "-0.5399197936058044\n",
      "-0.5647816061973572\n",
      "-0.5419847369194031\n",
      "-0.534638524055481\n",
      "-0.5371289253234863\n",
      "-0.5444124937057495\n",
      "-0.5345758199691772\n",
      "-0.4979132115840912\n",
      "-0.5293392539024353\n",
      "-0.5127077698707581\n",
      "-0.5379700660705566\n",
      "-0.507101833820343\n",
      "-0.5103036761283875\n",
      "-0.5412925481796265\n",
      "-0.5116868019104004\n",
      "-0.502983808517456\n",
      "-0.5075254440307617\n",
      "-0.5219582915306091\n",
      "-0.48737362027168274\n",
      "-0.541458010673523\n",
      "-0.4717058837413788\n",
      "-0.517273485660553\n",
      "-0.5334845781326294\n",
      "-0.5111784338951111\n",
      "-0.5339892506599426\n",
      "-0.5350738167762756\n",
      "-0.5314356088638306\n",
      "-0.5273159742355347\n",
      "-0.5456907153129578\n",
      "-0.553331732749939\n",
      "-0.5318061113357544\n",
      "-0.5251551270484924\n",
      "-0.537079393863678\n",
      "-0.5219014286994934\n",
      "-0.5431227684020996\n",
      "-0.5213658809661865\n",
      "-0.5285207629203796\n",
      "-0.5074976682662964\n",
      "-0.514117956161499\n",
      "-0.4955129623413086\n",
      "-0.5200673937797546\n",
      "-0.5379105806350708\n",
      "-0.5491924285888672\n",
      "-0.5600745677947998\n",
      "-0.5101311802864075\n",
      "-0.5434569716453552\n",
      "-0.5025006532669067\n",
      "-0.5198346376419067\n",
      "-0.5319401621818542\n",
      "-0.5371861457824707\n",
      "-0.5341548323631287\n",
      "-0.5051193833351135\n",
      "-0.521099328994751\n",
      "-0.5232198238372803\n",
      "-0.522407054901123\n",
      "-0.5172690153121948\n",
      "-0.5033689141273499\n",
      "-0.5423826575279236\n",
      "-0.5161606073379517\n",
      "-0.5169116258621216\n",
      "-0.5311576128005981\n",
      "-0.5084890723228455\n",
      "-0.5215265154838562\n",
      "-0.46796372532844543\n",
      "-0.5234346985816956\n",
      "-0.5170098543167114\n",
      "-0.5352409482002258\n",
      "-0.5082939267158508\n",
      "-0.5135943293571472\n",
      "-0.5069430470466614\n",
      "-0.528950035572052\n",
      "-0.5048428773880005\n",
      "-0.5286158919334412\n",
      "-0.5189474821090698\n",
      "-0.5138227343559265\n",
      "-0.5465794205665588\n",
      "-0.5284371972084045\n",
      "-0.5263984799385071\n",
      "-0.5404876470565796\n",
      "-0.5431588888168335\n",
      "-0.5405704975128174\n",
      "-0.5385951399803162\n",
      "-0.5381894707679749\n",
      "-0.5340768694877625\n",
      "-0.541671872138977\n",
      "-0.524992823600769\n",
      "-0.5456467270851135\n",
      "-0.492878258228302\n",
      "-0.5155377388000488\n",
      "-0.5145730376243591\n",
      "-0.5207728743553162\n",
      "-0.48816755414009094\n",
      "-0.5235829949378967\n",
      "-0.5374334454536438\n",
      "-0.5109968781471252\n",
      "-0.5184471607208252\n",
      "-0.5303834676742554\n",
      "-0.5304922461509705\n",
      "-0.5225822925567627\n",
      "-0.5334301590919495\n",
      "-0.5334604978561401\n",
      "-0.5230265855789185\n",
      "-0.5420849323272705\n",
      "-0.517360270023346\n",
      "-0.5274807810783386\n",
      "-0.5119191408157349\n",
      "-0.49600470066070557\n",
      "-0.5325674414634705\n",
      "-0.5323287844657898\n",
      "-0.5402641296386719\n",
      "-0.5137923955917358\n",
      "-0.5272735357284546\n",
      "-0.5429527759552002\n",
      "-0.5323677062988281\n",
      "-0.5448976755142212\n",
      "-0.5548447966575623\n",
      "-0.498029887676239\n",
      "-0.5363146066665649\n",
      "-0.5164525508880615\n",
      "-0.5259272456169128\n",
      "-0.5314381718635559\n",
      "-0.5219449996948242\n",
      "-0.512370765209198\n",
      "-0.5313534736633301\n",
      "-0.5073497891426086\n",
      "-0.5214938521385193\n",
      "-0.5135993957519531\n",
      "-0.5047691464424133\n",
      "-0.4685177505016327\n",
      "-0.5261818766593933\n",
      "-0.513809859752655\n",
      "-0.5112792253494263\n",
      "-0.5173707008361816\n",
      "-0.5315462946891785\n",
      "-0.533043384552002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5285442471504211\n",
      "-0.5191594958305359\n",
      "-0.5225018262863159\n",
      "-0.5336624383926392\n",
      "-0.5377469062805176\n",
      "-0.530182957649231\n",
      "-0.5228663086891174\n",
      "-0.5189470648765564\n",
      "-0.5246420502662659\n",
      "-0.5297033786773682\n",
      "-0.5471230149269104\n",
      "-0.5450146198272705\n",
      "-0.544823169708252\n",
      "-0.5442025065422058\n",
      "-0.5255005955696106\n",
      "-0.5183727741241455\n",
      "-0.5396707057952881\n",
      "-0.5489407777786255\n",
      "-0.5428522825241089\n",
      "-0.5359956622123718\n",
      "-0.5146759152412415\n",
      "-0.5209023952484131\n",
      "-0.5297985076904297\n",
      "-0.5418591499328613\n",
      "-0.5306465029716492\n",
      "-0.5227605104446411\n",
      "-0.5174424648284912\n",
      "-0.5385802984237671\n",
      "-0.5210449695587158\n",
      "-0.5351582169532776\n",
      "-0.5112058520317078\n",
      "-0.5296859741210938\n",
      "-0.5392109155654907\n",
      "-0.5246307253837585\n",
      "-0.5397690534591675\n",
      "-0.5296059846878052\n",
      "-0.5311675667762756\n",
      "-0.5263268351554871\n",
      "-0.5276607871055603\n",
      "-0.49710220098495483\n",
      "-0.5201163291931152\n",
      "-0.5127891898155212\n",
      "-0.5245069861412048\n",
      "-0.5288599133491516\n",
      "-0.5123634934425354\n",
      "-0.5295038819313049\n",
      "-0.5091747641563416\n",
      "-0.5337113738059998\n",
      "-0.4988567531108856\n",
      "-0.5424922108650208\n",
      "-0.5150132179260254\n",
      "-0.5354169011116028\n",
      "-0.5265205502510071\n",
      "-0.5096330046653748\n",
      "-0.5051147937774658\n",
      "-0.5220755338668823\n",
      "-0.522261917591095\n",
      "-0.5385921001434326\n",
      "-0.5299962162971497\n",
      "-0.5418981313705444\n",
      "-0.5143653750419617\n",
      "-0.5369685292243958\n",
      "-0.5209841728210449\n",
      "-0.5156142711639404\n",
      "-0.5316854119300842\n",
      "-0.5003266334533691\n",
      "-0.5282978415489197\n",
      "-0.5305862426757812\n",
      "-0.5219428539276123\n",
      "-0.5279187560081482\n",
      "-0.5492680668830872\n",
      "-0.524228572845459\n",
      "-0.5356974005699158\n",
      "-0.5188593864440918\n",
      "-0.523350715637207\n",
      "-0.5341125130653381\n",
      "-0.547505795955658\n",
      "-0.5226502418518066\n",
      "-0.5104967355728149\n",
      "-0.5463191270828247\n",
      "-0.5308307409286499\n",
      "-0.5315748453140259\n",
      "-0.5365687012672424\n",
      "-0.5029534697532654\n",
      "-0.5174915194511414\n",
      "-0.5205749273300171\n",
      "-0.5408377051353455\n",
      "-0.49868717789649963\n",
      "-0.5340358018875122\n",
      "-0.5354597568511963\n",
      "-0.5179651975631714\n",
      "-0.5176126956939697\n",
      "-0.5134438276290894\n",
      "-0.5426580905914307\n",
      "-0.49406009912490845\n",
      "-0.5192416310310364\n",
      "-0.5198506116867065\n",
      "-0.5279208421707153\n",
      "-0.5330533981323242\n",
      "-0.53727126121521\n",
      "-0.5415951013565063\n",
      "-0.5238243937492371\n",
      "-0.5280269384384155\n",
      "-0.5237120985984802\n",
      "-0.516642153263092\n",
      "-0.5135831832885742\n",
      "-0.49896204471588135\n",
      "-0.5467970967292786\n",
      "-0.527294933795929\n",
      "-0.556387186050415\n",
      "-0.4975815713405609\n",
      "-0.5617019534111023\n",
      "-0.5328431725502014\n",
      "-0.519648015499115\n",
      "-0.5286702513694763\n",
      "-0.512457013130188\n",
      "-0.544247567653656\n",
      "-0.534409761428833\n",
      "-0.5112534761428833\n",
      "-0.5409421324729919\n",
      "-0.528140664100647\n",
      "-0.4986859858036041\n",
      "-0.4954744875431061\n",
      "-0.4805324375629425\n",
      "-0.5131977200508118\n",
      "-0.4918135106563568\n",
      "-0.49158528447151184\n",
      "-0.5090897679328918\n",
      "-0.4622076749801636\n",
      "-0.5058450698852539\n",
      "-0.4731411635875702\n",
      "-0.5265721082687378\n",
      "-0.4855232834815979\n",
      "-0.5232862830162048\n",
      "-0.4646265506744385\n",
      "-0.5294846296310425\n",
      "-0.5123088955879211\n",
      "-0.4954393208026886\n",
      "-0.5441592931747437\n",
      "-0.5235424041748047\n",
      "-0.5082718133926392\n",
      "-0.5491864085197449\n",
      "-0.5284460186958313\n",
      "-0.5418877005577087\n",
      "-0.5360273122787476\n",
      "-0.5376682877540588\n",
      "-0.5265805721282959\n",
      "-0.5133132338523865\n",
      "-0.5348606109619141\n",
      "-0.5228900909423828\n",
      "-0.5297979712486267\n",
      "-0.5441399216651917\n",
      "-0.5114225745201111\n",
      "-0.536386251449585\n",
      "-0.5211777091026306\n",
      "-0.5103398561477661\n",
      "-0.5299987196922302\n",
      "-0.5263568162918091\n",
      "-0.5288858413696289\n",
      "-0.5443298816680908\n",
      "-0.5297253727912903\n",
      "-0.5211692452430725\n",
      "-0.539555549621582\n",
      "-0.5270098447799683\n",
      "-0.5092105865478516\n",
      "-0.5309999585151672\n",
      "-0.5395072102546692\n",
      "-0.49732908606529236\n",
      "-0.5289768576622009\n",
      "-0.5047106742858887\n",
      "-0.5466984510421753\n",
      "-0.5310396552085876\n",
      "-0.5389593839645386\n",
      "-0.5427130460739136\n",
      "-0.509321391582489\n",
      "-0.5328788757324219\n",
      "-0.5235629677772522\n",
      "-0.5200576782226562\n",
      "-0.5090671181678772\n",
      "-0.511335015296936\n",
      "-0.5308084487915039\n",
      "-0.5211028456687927\n",
      "-0.5067592859268188\n"
     ]
    }
   ],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=300, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "\n",
    "\n",
    "γ =  0.1**2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = gaussian_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train.reshape(-1,1,28,28), y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0] // 20), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"uniform\",\n",
    "    γ_min= 0.1**2, γ_max= 0.4**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8926),\n",
       " tensor(-0.2547),\n",
       " tensor(-0.4263),\n",
       " tensor(-0.4754),\n",
       " tensor(-0.4905),\n",
       " tensor(-0.5041),\n",
       " tensor(-0.5060),\n",
       " tensor(-0.5075),\n",
       " tensor(-0.5168),\n",
       " tensor(-0.5138),\n",
       " tensor(-0.5215),\n",
       " tensor(-0.5094),\n",
       " tensor(-0.5130),\n",
       " tensor(-0.5204),\n",
       " tensor(-0.5193),\n",
       " tensor(-0.5191),\n",
       " tensor(-0.5216),\n",
       " tensor(-0.5242),\n",
       " tensor(-0.5205),\n",
       " tensor(-0.5178),\n",
       " tensor(-0.5177),\n",
       " tensor(-0.5163),\n",
       " tensor(-0.5222),\n",
       " tensor(-0.5192),\n",
       " tensor(-0.5151),\n",
       " tensor(-0.5170),\n",
       " tensor(-0.5258),\n",
       " tensor(-0.5229),\n",
       " tensor(-0.5251),\n",
       " tensor(-0.5309),\n",
       " tensor(-0.5174),\n",
       " tensor(-0.5209),\n",
       " tensor(-0.5084),\n",
       " tensor(-0.5295),\n",
       " tensor(-0.5294),\n",
       " tensor(-0.5244),\n",
       " tensor(-0.5205),\n",
       " tensor(-0.5272),\n",
       " tensor(-0.5301),\n",
       " tensor(-0.5264),\n",
       " tensor(-0.5273),\n",
       " tensor(-0.5291),\n",
       " tensor(-0.5147),\n",
       " tensor(-0.5198),\n",
       " tensor(-0.5304),\n",
       " tensor(-0.5205),\n",
       " tensor(-0.5285),\n",
       " tensor(-0.5349),\n",
       " tensor(-0.5318),\n",
       " tensor(-0.5168),\n",
       " tensor(-0.5259),\n",
       " tensor(-0.5273),\n",
       " tensor(-0.5273),\n",
       " tensor(-0.5247),\n",
       " tensor(-0.5243),\n",
       " tensor(-0.5219),\n",
       " tensor(-0.5267),\n",
       " tensor(-0.5263),\n",
       " tensor(-0.5212),\n",
       " tensor(-0.5248),\n",
       " tensor(-0.5274),\n",
       " tensor(-0.5193),\n",
       " tensor(-0.5325),\n",
       " tensor(-0.5248),\n",
       " tensor(-0.5225),\n",
       " tensor(-0.5281),\n",
       " tensor(-0.5240),\n",
       " tensor(-0.5325),\n",
       " tensor(-0.5303),\n",
       " tensor(-0.5298),\n",
       " tensor(-0.5221),\n",
       " tensor(-0.5323),\n",
       " tensor(-0.5275),\n",
       " tensor(-0.5231),\n",
       " tensor(-0.5299),\n",
       " tensor(-0.5287),\n",
       " tensor(-0.5059),\n",
       " tensor(-0.5293),\n",
       " tensor(-0.5231)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4cdc428130>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 50\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  3.,  6.,  9.,  7., 11.,  8.,  3.,  1.]),\n",
       " array([-0.64350617, -0.59688014, -0.55025417, -0.50362813, -0.45700213,\n",
       "        -0.41037613, -0.3637501 , -0.3171241 , -0.2704981 , -0.22387208,\n",
       "        -0.17724606], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([(net.forward(X, θ)[None,...]).softmax(dim=-1) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4f1243730a4831a7f8f1fb46b29a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 47.46 GiB total capacity; 43.30 GiB already allocated; 3.31 MiB free; 46.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29527/682142721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΘ_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29527/3562858173.py\u001b[0m in \u001b[0;36mpredc\u001b[0;34m(X, Θ)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mθ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mθ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mΘ\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_29527/3562858173.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mθ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mθ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mΘ\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_29527/1554499846.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, Θ)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m#         import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mΘ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params_from_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/functorch/_src/make_functional.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mold_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swap_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# Remove the loaded state on self.stateless_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29527/1554499846.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#         print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 47.46 GiB total capacity; 43.30 GiB already allocated; 3.31 MiB free; 46.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pred = []\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "subsamp = 30\n",
    "\n",
    "stride = 10\n",
    "\n",
    "for i in tqdm(range(0,len(X_train), stride)):\n",
    "    \n",
    "    pred.append(predc(X_train[i:i+stride,...].reshape(-1,1,28,28), Θ_1[:subsamp,:]).cpu())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.vstack(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = torch.vstack(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamp = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9716)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred.argmax(dim=-1)).float().flatten().cpu() == y_train[:len(pred)].argmax(dim=-1).cpu() ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48860, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef36a0235174be7836e0a13ec237881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31737/243730503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "subsamp = 30\n",
    "\n",
    "stride = 10\n",
    "\n",
    "\n",
    "pred_test = []\n",
    "for i in tqdm(range(0,len(X_test), stride)):\n",
    "    \n",
    "    pred_test.append(predc(X_test[i:i+stride,...].float().reshape(-1,1,28,28), Θ_1[:subsamp,:]).cpu())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pred_test = (torch.vstack(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (torch.vstack(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9756)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test.argmax(dim=-1)).float().flatten().cpu() == y_test.argmax(dim=-1).cpu()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
