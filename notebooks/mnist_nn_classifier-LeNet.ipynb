{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from torch import datasets\n",
    "\n",
    "from torch import _vmap_internals\n",
    "from torchvision import datasets, transforms\n",
    "from functorch import vmap\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functorch succesfully imported\n"
     ]
    }
   ],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.drifts import *\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Cat}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = datasets.MNIST(\"../data/mnist/\", download=True, transform=ToTensor(), train=True)\n",
    "images_test = datasets.MNIST(\"../data/mnist/\", download=True, transform=ToTensor(), train=False)\n",
    "\n",
    "transform = torch.nn.Sequential(transforms.Normalize((0.1307,), (0.3081)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = images_train.data, images_train.targets\n",
    "X_test, y_test = images_test.data, images_test.targets\n",
    "\n",
    "X_train = transform(X_train.float())\n",
    "X_test = transform(X_test.float())\n",
    "\n",
    "y_train = F.one_hot(y_train)\n",
    "y_test = F.one_hot(y_test)\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29527/1347329722.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_29527/1347329722.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_29527/1347329722.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
      "/tmp/ipykernel_29527/1347329722.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_test, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = torch.nn.Sequential(            \n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=256, out_features=120),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=84, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "from functorch import make_functional\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LeNet5Fun(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.model = LeNet5(n_classes=10)\n",
    "        self.func_model, self.params = make_functional(self.model)\n",
    "        \n",
    "        \n",
    "        self.dim = sum([math.prod(x.shape) for x in self.params])\n",
    "        \n",
    "        self.size_tuples = [p.shape for p in self.params]\n",
    "\n",
    "    def get_params_from_array(self, array):\n",
    "        cur_index = 0\n",
    "        param_list = []\n",
    "        for s in self.size_tuples:\n",
    "            step_number = math.prod(s)\n",
    "            param_list.append(array[cur_index:cur_index+step_number].reshape(s))\n",
    "            cur_index += step_number\n",
    "        return param_list\n",
    "    \n",
    "    def forward(self, x, Θ):\n",
    "        Θ = self.get_params_from_array(Θ)\n",
    "        return self.func_model(Θ, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "\n",
    "# net = ClassificationNetwork(\n",
    "#     dim, out_dim, device=device, depth=1, width=50, activation=F.tanh\n",
    "# )\n",
    "net = LeNet5Fun()\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=3.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        cel = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ll_cel = -1.0 * cel(preds, y.argmax(dim=1))\n",
    "        return ll_cel\n",
    "    \n",
    "    batched_loss =  vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44426"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052a27ff00d844ab8243af1d612baf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2378: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::batch_norm. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch.batch_norm(\n",
      "/local/scratch/home/fav25/hjb2/lib/python3.8/site-packages/torch/nn/functional.py:2942: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::cross_entropy_loss. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at  /tmp/pip-req-build-t07f5mpb/functorch/csrc/BatchedFallback.cpp:106.)\n",
      "  return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6934889554977417\n",
      "1.682041049003601\n",
      "1.7083704471588135\n",
      "1.6547383069992065\n",
      "1.4843652248382568\n",
      "1.2474050521850586\n",
      "1.0920374393463135\n",
      "0.9228026270866394\n",
      "0.8101496696472168\n",
      "0.575107216835022\n",
      "0.4128902554512024\n",
      "0.37032076716423035\n",
      "0.21082764863967896\n",
      "0.11910095810890198\n",
      "0.09894826263189316\n",
      "0.05193038284778595\n",
      "-0.01872221939265728\n",
      "-0.03882938623428345\n",
      "-0.06529614329338074\n",
      "-0.10613273084163666\n",
      "-0.0931284949183464\n",
      "-0.2079002857208252\n",
      "-0.2010226994752884\n",
      "-0.23627139627933502\n",
      "-0.25669246912002563\n",
      "-0.25908640027046204\n",
      "-0.2945830523967743\n",
      "-0.27499616146087646\n",
      "-0.3318319618701935\n",
      "-0.2823198437690735\n",
      "-0.35332196950912476\n",
      "-0.3623194992542267\n",
      "-0.31903567910194397\n",
      "-0.3487713932991028\n",
      "-0.36587467789649963\n",
      "-0.37138012051582336\n",
      "-0.40518203377723694\n",
      "-0.40245339274406433\n",
      "-0.3899686336517334\n",
      "-0.3916984498500824\n",
      "-0.38440078496932983\n",
      "-0.41223815083503723\n",
      "-0.3923581838607788\n",
      "-0.37056660652160645\n",
      "-0.41244783997535706\n",
      "-0.4361793100833893\n",
      "-0.4233507812023163\n",
      "-0.41886937618255615\n",
      "-0.4071148633956909\n",
      "-0.41920357942581177\n",
      "-0.42952612042427063\n",
      "-0.40312060713768005\n",
      "-0.4097372591495514\n",
      "-0.44033193588256836\n",
      "-0.44063785672187805\n",
      "-0.4272131323814392\n",
      "-0.44650527834892273\n",
      "-0.452764630317688\n",
      "-0.4338740110397339\n",
      "-0.4649398624897003\n",
      "-0.4348231852054596\n",
      "-0.4687654972076416\n",
      "-0.4438105523586273\n",
      "-0.4328552186489105\n",
      "-0.46247920393943787\n",
      "-0.4655560255050659\n",
      "-0.46146509051322937\n",
      "-0.4507710337638855\n",
      "-0.44841301441192627\n",
      "-0.48061609268188477\n",
      "-0.4818309247493744\n",
      "-0.4652236998081207\n",
      "-0.46146535873413086\n",
      "-0.48662734031677246\n",
      "-0.4577377736568451\n",
      "-0.46601182222366333\n",
      "-0.4980701804161072\n",
      "-0.5012125372886658\n",
      "-0.48112502694129944\n",
      "-0.4806256592273712\n",
      "-0.4408715069293976\n",
      "-0.48980844020843506\n",
      "-0.46548572182655334\n",
      "-0.5150530338287354\n",
      "-0.49684569239616394\n",
      "-0.5003025531768799\n",
      "-0.4765899181365967\n",
      "-0.485470175743103\n",
      "-0.48322683572769165\n",
      "-0.4872329533100128\n",
      "-0.4784623980522156\n",
      "-0.4978649318218231\n",
      "-0.47717031836509705\n",
      "-0.502819836139679\n",
      "-0.49404793977737427\n",
      "-0.46656334400177\n",
      "-0.4852758049964905\n",
      "-0.4932944178581238\n",
      "-0.5095049738883972\n",
      "-0.48879697918891907\n",
      "-0.512103259563446\n",
      "-0.5082547068595886\n",
      "-0.4820338785648346\n",
      "-0.5145165324211121\n",
      "-0.491314172744751\n",
      "-0.48539403080940247\n",
      "-0.490562379360199\n",
      "-0.4978366792201996\n",
      "-0.5010176301002502\n",
      "-0.4870714545249939\n",
      "-0.5057081580162048\n",
      "-0.42834845185279846\n",
      "-0.5174046158790588\n",
      "-0.5217590928077698\n",
      "-0.492688924074173\n",
      "-0.4802370071411133\n",
      "-0.5186147093772888\n",
      "-0.4881775677204132\n",
      "-0.4887705147266388\n",
      "-0.5232353210449219\n",
      "-0.5415924191474915\n",
      "-0.49375852942466736\n",
      "-0.5136218667030334\n",
      "-0.5049759149551392\n",
      "-0.48907190561294556\n",
      "-0.5009666085243225\n",
      "-0.5289817452430725\n",
      "-0.4890153706073761\n",
      "-0.47873184084892273\n",
      "-0.4918206036090851\n",
      "-0.4897877871990204\n",
      "-0.5085724592208862\n",
      "-0.49503737688064575\n",
      "-0.4875582158565521\n",
      "-0.5044031739234924\n",
      "-0.4851092994213104\n",
      "-0.5142796039581299\n",
      "-0.5074143409729004\n",
      "-0.49186041951179504\n",
      "-0.4785761535167694\n",
      "-0.5038827061653137\n",
      "-0.5055338740348816\n",
      "-0.5047367215156555\n",
      "-0.526526689529419\n",
      "-0.513554036617279\n",
      "-0.5081877112388611\n",
      "-0.48714205622673035\n",
      "-0.5122361779212952\n",
      "-0.5133811831474304\n",
      "-0.5185609459877014\n",
      "-0.5168294906616211\n",
      "-0.5213474631309509\n",
      "-0.5056133270263672\n",
      "-0.5269855260848999\n",
      "-0.5261297225952148\n",
      "-0.47595784068107605\n",
      "-0.5233185887336731\n",
      "-0.5107748508453369\n",
      "-0.49112242460250854\n",
      "-0.511894702911377\n",
      "-0.48653465509414673\n",
      "-0.522188663482666\n",
      "-0.4955420196056366\n",
      "-0.5148388743400574\n",
      "-0.5267783999443054\n",
      "-0.5133532881736755\n",
      "-0.5221409797668457\n",
      "-0.5085253715515137\n",
      "-0.5183761119842529\n",
      "-0.48435142636299133\n",
      "-0.5157262682914734\n",
      "-0.49143144488334656\n",
      "-0.5216277837753296\n",
      "-0.5316786170005798\n",
      "-0.5195078253746033\n",
      "-0.5156772136688232\n",
      "-0.5058831572532654\n",
      "-0.5068519115447998\n",
      "-0.4911051094532013\n",
      "-0.5126515030860901\n",
      "-0.5216019749641418\n",
      "-0.5130404233932495\n",
      "-0.5030207633972168\n",
      "-0.5208832025527954\n",
      "-0.511223316192627\n",
      "-0.4875548183917999\n",
      "-0.5045499205589294\n",
      "-0.5182263255119324\n",
      "-0.5246610641479492\n",
      "-0.5136908888816833\n",
      "-0.5049635171890259\n",
      "-0.5067607760429382\n",
      "-0.5048706531524658\n",
      "-0.5069754719734192\n",
      "-0.514538049697876\n",
      "-0.526712954044342\n",
      "-0.505231499671936\n",
      "-0.5166849493980408\n",
      "-0.5274066925048828\n",
      "-0.5147231221199036\n",
      "-0.5242254137992859\n",
      "-0.5105875730514526\n",
      "-0.5401768088340759\n",
      "-0.508928120136261\n",
      "-0.5249013900756836\n",
      "-0.5197744965553284\n",
      "-0.5168312191963196\n",
      "-0.5315661430358887\n",
      "-0.5224637389183044\n",
      "-0.520251989364624\n",
      "-0.4776240289211273\n",
      "-0.5305222868919373\n",
      "-0.513923168182373\n",
      "-0.5139234662055969\n",
      "-0.5338748097419739\n",
      "-0.5291978120803833\n",
      "-0.4746004641056061\n",
      "-0.5365238785743713\n",
      "-0.5007302761077881\n",
      "-0.4932869076728821\n",
      "-0.5270597338676453\n",
      "-0.5254052877426147\n",
      "-0.5050013661384583\n",
      "-0.5248879790306091\n",
      "-0.513135552406311\n",
      "-0.5056853294372559\n",
      "-0.5269147753715515\n",
      "-0.520642101764679\n",
      "-0.5382758975028992\n",
      "-0.5009269118309021\n",
      "-0.5087040662765503\n",
      "-0.5128048062324524\n",
      "-0.516098141670227\n",
      "-0.5198937654495239\n",
      "-0.5066098570823669\n",
      "-0.5248006582260132\n",
      "-0.5147742629051208\n",
      "-0.516018807888031\n",
      "-0.5118157863616943\n",
      "-0.5490761399269104\n",
      "-0.4930937886238098\n",
      "-0.5121284127235413\n",
      "-0.5324828028678894\n",
      "-0.5181150436401367\n",
      "-0.5130248069763184\n",
      "-0.5074568390846252\n",
      "-0.5162414312362671\n",
      "-0.5491659045219421\n",
      "-0.49758046865463257\n",
      "-0.528649091720581\n",
      "-0.5147271752357483\n",
      "-0.5426372289657593\n",
      "-0.5207476019859314\n",
      "-0.5093384981155396\n",
      "-0.5248026847839355\n",
      "-0.536543071269989\n",
      "-0.5415071249008179\n",
      "-0.4914293587207794\n",
      "-0.508121907711029\n",
      "-0.5175032019615173\n",
      "-0.5507115721702576\n",
      "-0.5147219300270081\n",
      "-0.5335537195205688\n",
      "-0.5366439819335938\n",
      "-0.5415108799934387\n",
      "-0.502720832824707\n",
      "-0.5197400450706482\n",
      "-0.5260680913925171\n",
      "-0.5165985226631165\n",
      "-0.504772424697876\n",
      "-0.5180190205574036\n",
      "-0.5180593132972717\n",
      "-0.5196592807769775\n",
      "-0.5093269348144531\n",
      "-0.5161988735198975\n",
      "-0.5112151503562927\n",
      "-0.5251474380493164\n",
      "-0.5106760859489441\n",
      "-0.5084130167961121\n",
      "-0.5301694869995117\n",
      "-0.5466012358665466\n",
      "-0.5268806219100952\n",
      "-0.5347351431846619\n",
      "-0.523260235786438\n",
      "-0.519320547580719\n",
      "-0.5448474287986755\n",
      "-0.5270878672599792\n",
      "-0.5548310279846191\n",
      "-0.5421635508537292\n",
      "-0.5156234502792358\n",
      "-0.5007267594337463\n",
      "-0.5225217342376709\n",
      "-0.4914790093898773\n",
      "-0.5233960747718811\n",
      "-0.5046826004981995\n",
      "-0.5158747434616089\n",
      "-0.5260166525840759\n",
      "-0.5301575064659119\n",
      "-0.5140426158905029\n",
      "-0.5200836658477783\n",
      "-0.49974751472473145\n",
      "-0.5273801684379578\n",
      "-0.541357159614563\n",
      "-0.5120379328727722\n",
      "-0.5052222609519958\n",
      "-0.5285389423370361\n",
      "-0.5170736312866211\n",
      "-0.5253419280052185\n",
      "-0.5218834280967712\n",
      "-0.5485325455665588\n",
      "-0.5341241359710693\n",
      "-0.5249930620193481\n",
      "-0.5054692625999451\n",
      "-0.521113395690918\n",
      "-0.4986993372440338\n",
      "-0.5310012698173523\n",
      "-0.5272313356399536\n",
      "-0.4930489957332611\n",
      "-0.5240049958229065\n",
      "-0.5031346678733826\n",
      "-0.50436931848526\n",
      "-0.5071839690208435\n",
      "-0.5339267253875732\n",
      "-0.5327003598213196\n",
      "-0.5210469365119934\n",
      "-0.5429138541221619\n",
      "-0.533896803855896\n",
      "-0.5256869792938232\n",
      "-0.5298727750778198\n",
      "-0.5264253616333008\n",
      "-0.5022573471069336\n",
      "-0.5298040509223938\n",
      "-0.5194617509841919\n",
      "-0.4914880692958832\n",
      "-0.5190885663032532\n",
      "-0.5270168781280518\n",
      "-0.5147996544837952\n",
      "-0.5260920524597168\n",
      "-0.505196213722229\n",
      "-0.5184862017631531\n",
      "-0.49214640259742737\n",
      "-0.5184616446495056\n",
      "-0.5096557140350342\n",
      "-0.5437527894973755\n",
      "-0.4737797677516937\n",
      "-0.5199507474899292\n",
      "-0.5487151145935059\n",
      "-0.5192855000495911\n",
      "-0.5237324237823486\n",
      "-0.5150946974754333\n",
      "-0.5036080479621887\n",
      "-0.5304371118545532\n",
      "-0.4983011782169342\n",
      "-0.5314514636993408\n",
      "-0.4657486081123352\n",
      "-0.535878598690033\n",
      "-0.49116504192352295\n",
      "-0.5165636539459229\n",
      "-0.506273090839386\n",
      "-0.533886730670929\n",
      "-0.510672926902771\n",
      "-0.5066230297088623\n",
      "-0.5359046459197998\n",
      "-0.50755774974823\n",
      "-0.5166628956794739\n",
      "-0.5235603451728821\n",
      "-0.5050042867660522\n",
      "-0.5401285886764526\n",
      "-0.5230140686035156\n",
      "-0.5451337099075317\n",
      "-0.5244185924530029\n",
      "-0.489951491355896\n",
      "-0.5109094381332397\n",
      "-0.52736496925354\n",
      "-0.5486452579498291\n",
      "-0.5013393759727478\n",
      "-0.5299620628356934\n",
      "-0.5091288685798645\n",
      "-0.5148475170135498\n",
      "-0.5243145823478699\n",
      "-0.5230269432067871\n",
      "-0.5263941287994385\n",
      "-0.5128316879272461\n",
      "-0.5208839178085327\n",
      "-0.5324506163597107\n",
      "-0.5357397198677063\n",
      "-0.5221928358078003\n",
      "-0.4995247721672058\n",
      "-0.5271403193473816\n",
      "-0.5355080366134644\n",
      "-0.5061041116714478\n",
      "-0.5420087575912476\n",
      "-0.5278716087341309\n",
      "-0.5257086753845215\n",
      "-0.5149439573287964\n",
      "-0.5135781168937683\n",
      "-0.5371178984642029\n",
      "-0.5239559412002563\n",
      "-0.526886522769928\n",
      "-0.5154263973236084\n",
      "-0.5033684968948364\n",
      "-0.5337370038032532\n",
      "-0.5254356265068054\n",
      "-0.5101723074913025\n",
      "-0.5379282832145691\n",
      "-0.5128158926963806\n",
      "-0.5186111330986023\n",
      "-0.5393492579460144\n",
      "-0.49589309096336365\n",
      "-0.5212164521217346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5155816674232483\n",
      "-0.5135477781295776\n",
      "-0.49482157826423645\n",
      "-0.5380510091781616\n",
      "-0.5188863277435303\n",
      "-0.5013899803161621\n",
      "-0.5141793489456177\n",
      "-0.5280399322509766\n",
      "-0.5257059931755066\n",
      "-0.5129442811012268\n",
      "-0.5114151835441589\n",
      "-0.5313513278961182\n",
      "-0.5478516221046448\n",
      "-0.5320116877555847\n",
      "-0.5235782861709595\n",
      "-0.5168309211730957\n",
      "-0.5253767371177673\n",
      "-0.5193021893501282\n",
      "-0.5083229541778564\n",
      "-0.5198453068733215\n",
      "-0.5490954518318176\n",
      "-0.5139727592468262\n",
      "-0.5380529761314392\n",
      "-0.5293859243392944\n",
      "-0.49353182315826416\n",
      "-0.5372241735458374\n",
      "-0.5226840972900391\n",
      "-0.515090823173523\n",
      "-0.5264928936958313\n",
      "-0.5203284621238708\n",
      "-0.5428516268730164\n",
      "-0.5296195149421692\n",
      "-0.5482606291770935\n",
      "-0.5098691582679749\n",
      "-0.5239064693450928\n",
      "-0.5343331098556519\n",
      "-0.5079092383384705\n",
      "-0.533955454826355\n",
      "-0.5291103720664978\n",
      "-0.5417516827583313\n",
      "-0.5236768126487732\n",
      "-0.5199479460716248\n",
      "-0.5445462465286255\n",
      "-0.5259272456169128\n",
      "-0.5272689461708069\n",
      "-0.5370115041732788\n",
      "-0.5350571870803833\n",
      "-0.532121479511261\n",
      "-0.524308443069458\n",
      "-0.5151787996292114\n",
      "-0.5180321931838989\n",
      "-0.538692057132721\n",
      "-0.5364261865615845\n",
      "-0.5332688093185425\n",
      "-0.5316601991653442\n",
      "-0.5171149373054504\n",
      "-0.5374459624290466\n",
      "-0.5219705104827881\n",
      "-0.5395894050598145\n",
      "-0.5491815805435181\n",
      "-0.5288386344909668\n",
      "-0.5239485502243042\n",
      "-0.5109716057777405\n",
      "-0.527870774269104\n",
      "-0.5361741781234741\n",
      "-0.5306265354156494\n",
      "-0.5226929187774658\n",
      "-0.5212434530258179\n",
      "-0.5215508341789246\n",
      "-0.5290623307228088\n",
      "-0.5399106740951538\n",
      "-0.5112059116363525\n",
      "-0.5165223479270935\n",
      "-0.5486372709274292\n",
      "-0.49075081944465637\n",
      "-0.4964277148246765\n",
      "-0.4976903796195984\n",
      "-0.5311450362205505\n",
      "-0.5375243425369263\n",
      "-0.5040827989578247\n",
      "-0.5342646241188049\n",
      "-0.5007413625717163\n",
      "-0.5097559094429016\n",
      "-0.5282317996025085\n",
      "-0.509827196598053\n",
      "-0.5053710341453552\n",
      "-0.5354402661323547\n",
      "-0.5187680721282959\n",
      "-0.5252515077590942\n",
      "-0.5323750376701355\n",
      "-0.544033408164978\n",
      "-0.5343616604804993\n",
      "-0.5490319728851318\n",
      "-0.5367146730422974\n",
      "-0.537678599357605\n",
      "-0.5182456970214844\n",
      "-0.5473060011863708\n",
      "-0.5094693899154663\n",
      "-0.5429991483688354\n",
      "-0.5301627516746521\n",
      "-0.5154483318328857\n",
      "-0.5262420773506165\n",
      "-0.5195583701133728\n",
      "-0.514407217502594\n",
      "-0.5155651569366455\n",
      "-0.535599410533905\n",
      "-0.5241194367408752\n",
      "-0.547243058681488\n",
      "-0.5361980199813843\n",
      "-0.5284556746482849\n",
      "-0.5268300771713257\n",
      "-0.5454927086830139\n",
      "-0.5390421152114868\n",
      "-0.5048027634620667\n",
      "-0.5342251062393188\n",
      "-0.5139877796173096\n",
      "-0.5274540781974792\n",
      "-0.5330462455749512\n",
      "-0.5065595507621765\n",
      "-0.531766414642334\n",
      "-0.5166160464286804\n",
      "-0.5325217843055725\n",
      "-0.5175104737281799\n",
      "-0.5098958611488342\n",
      "-0.48204442858695984\n",
      "-0.5191752910614014\n",
      "-0.5447064638137817\n",
      "-0.5382774472236633\n",
      "-0.5156633853912354\n",
      "-0.5417452454566956\n",
      "-0.5126084089279175\n",
      "-0.5206509828567505\n",
      "-0.5140175819396973\n",
      "-0.5446560382843018\n",
      "-0.5445445775985718\n",
      "-0.5311189889907837\n",
      "-0.5391104221343994\n",
      "-0.5184481739997864\n",
      "-0.5325726866722107\n",
      "-0.5533474087715149\n",
      "-0.5187445282936096\n",
      "-0.5204449892044067\n",
      "-0.5189838409423828\n",
      "-0.5331745147705078\n",
      "-0.530385434627533\n",
      "-0.5091809630393982\n",
      "-0.5216140747070312\n",
      "-0.5303614139556885\n",
      "-0.5269739031791687\n",
      "-0.5412521958351135\n",
      "-0.5244013071060181\n",
      "-0.5218952298164368\n",
      "-0.5291203856468201\n",
      "-0.5409060716629028\n",
      "-0.49236929416656494\n",
      "-0.5207465887069702\n",
      "-0.4663129448890686\n",
      "-0.5448134541511536\n",
      "-0.4936245083808899\n",
      "-0.5331477522850037\n",
      "-0.5253098607063293\n",
      "-0.5217406153678894\n",
      "-0.5423343181610107\n",
      "-0.4906612038612366\n",
      "-0.5407532453536987\n",
      "-0.5143263339996338\n",
      "-0.49258825182914734\n",
      "-0.5165114402770996\n",
      "-0.530487596988678\n",
      "-0.511620044708252\n",
      "-0.5524201393127441\n",
      "-0.5014249086380005\n",
      "-0.5344118475914001\n",
      "-0.5125595927238464\n",
      "-0.5204906463623047\n",
      "-0.5290437936782837\n",
      "-0.5234352350234985\n",
      "-0.5503815412521362\n",
      "-0.5490173697471619\n",
      "-0.5162016749382019\n",
      "-0.538589596748352\n",
      "-0.5415066480636597\n",
      "-0.5273223519325256\n",
      "-0.5123357176780701\n",
      "-0.5121499300003052\n",
      "-0.4952087700366974\n",
      "-0.525441586971283\n",
      "-0.5282687544822693\n",
      "-0.5196663737297058\n",
      "-0.5580601096153259\n",
      "-0.5337597727775574\n",
      "-0.538490891456604\n",
      "-0.5294092893600464\n",
      "-0.5318095088005066\n",
      "-0.5071215629577637\n",
      "-0.5346823930740356\n",
      "-0.532488226890564\n",
      "-0.5357458591461182\n",
      "-0.5101653933525085\n",
      "-0.5392797589302063\n",
      "-0.5215014815330505\n",
      "-0.5477985739707947\n",
      "-0.5127085447311401\n",
      "-0.5059245824813843\n",
      "-0.4892425835132599\n",
      "-0.5396071672439575\n",
      "-0.5406304001808167\n",
      "-0.5433765649795532\n",
      "-0.5213214755058289\n",
      "-0.5310773253440857\n",
      "-0.5458798408508301\n",
      "-0.5325425267219543\n",
      "-0.5281486511230469\n",
      "-0.534996747970581\n",
      "-0.5394409894943237\n",
      "-0.541928768157959\n",
      "-0.5163678526878357\n",
      "-0.5253077149391174\n",
      "-0.5324682593345642\n",
      "-0.5410351753234863\n",
      "-0.5214789509773254\n",
      "-0.537011444568634\n",
      "-0.5178129076957703\n",
      "-0.5245863199234009\n",
      "-0.5346141457557678\n",
      "-0.5292498469352722\n",
      "-0.5154502987861633\n",
      "-0.5449489951133728\n",
      "-0.5164750218391418\n",
      "-0.5264695882797241\n",
      "-0.5254358053207397\n",
      "-0.5288353562355042\n",
      "-0.5183276534080505\n",
      "-0.5578163862228394\n",
      "-0.5408499836921692\n",
      "-0.5321603417396545\n",
      "-0.5285521745681763\n",
      "-0.5286428928375244\n",
      "-0.5534912347793579\n",
      "-0.5414168834686279\n",
      "-0.541408360004425\n",
      "-0.5134935975074768\n",
      "-0.5330391526222229\n",
      "-0.4858573377132416\n",
      "-0.5110471844673157\n",
      "-0.5215272903442383\n",
      "-0.5141884088516235\n",
      "-0.512414276599884\n",
      "-0.5260614156723022\n",
      "-0.5050898194313049\n",
      "-0.5284629464149475\n",
      "-0.5151939392089844\n",
      "-0.5267894864082336\n",
      "-0.5046207904815674\n",
      "-0.5161573886871338\n",
      "-0.5495697259902954\n",
      "-0.5055144429206848\n",
      "-0.5502228736877441\n",
      "-0.5023863315582275\n",
      "-0.5142443180084229\n",
      "-0.5259538292884827\n",
      "-0.5049827694892883\n",
      "-0.5448409914970398\n",
      "-0.5401913523674011\n",
      "-0.5130844116210938\n",
      "-0.5293078422546387\n",
      "-0.4982258677482605\n",
      "-0.5358176827430725\n",
      "-0.5291479229927063\n",
      "-0.5269045233726501\n",
      "-0.5328603982925415\n",
      "-0.5166745781898499\n",
      "-0.5269860625267029\n",
      "-0.5451506972312927\n",
      "-0.5278470516204834\n",
      "-0.5293299555778503\n",
      "-0.5309715270996094\n",
      "-0.5089284181594849\n",
      "-0.5538628697395325\n",
      "-0.5415031313896179\n",
      "-0.5285602807998657\n",
      "-0.5328969955444336\n",
      "-0.5116377472877502\n",
      "-0.4984929859638214\n",
      "-0.5159944891929626\n",
      "-0.5164031982421875\n",
      "-0.5269433259963989\n",
      "-0.4725906550884247\n",
      "-0.5353360176086426\n",
      "-0.5449257493019104\n",
      "-0.5114573836326599\n",
      "-0.5311192274093628\n",
      "-0.5187801718711853\n",
      "-0.517784059047699\n",
      "-0.48975494503974915\n",
      "-0.5249373316764832\n",
      "-0.5312341451644897\n",
      "-0.5538276433944702\n",
      "-0.5504374504089355\n",
      "-0.5202773213386536\n",
      "-0.5246111154556274\n",
      "-0.5321299433708191\n",
      "-0.5181819796562195\n",
      "-0.5202078819274902\n",
      "-0.5053163766860962\n",
      "-0.5066480040550232\n",
      "-0.5391335487365723\n",
      "-0.5012958645820618\n",
      "-0.5154805183410645\n",
      "-0.5235009789466858\n",
      "-0.5240073800086975\n",
      "-0.5230328440666199\n",
      "-0.5172457695007324\n",
      "-0.5353310704231262\n",
      "-0.5377619862556458\n",
      "-0.5276471972465515\n",
      "-0.5289773344993591\n",
      "-0.5318321585655212\n",
      "-0.5131683945655823\n",
      "-0.5521349310874939\n",
      "-0.5014439821243286\n",
      "-0.5241084098815918\n",
      "-0.5305171012878418\n",
      "-0.5361166000366211\n",
      "-0.5421764254570007\n",
      "-0.5229364037513733\n",
      "-0.4992155432701111\n",
      "-0.5221318006515503\n",
      "-0.5229542851448059\n",
      "-0.521338701248169\n",
      "-0.5076320171356201\n",
      "-0.5524351596832275\n",
      "-0.496259480714798\n",
      "-0.5561328530311584\n",
      "-0.5311902761459351\n",
      "-0.5147331953048706\n",
      "-0.5397170186042786\n",
      "-0.5173812508583069\n",
      "-0.541917085647583\n",
      "-0.5495054125785828\n",
      "-0.5266610980033875\n",
      "-0.5168498754501343\n",
      "-0.501564085483551\n",
      "-0.5200697183609009\n",
      "-0.5431767702102661\n",
      "-0.5154435634613037\n",
      "-0.5227860808372498\n",
      "-0.5207046866416931\n",
      "-0.5358533263206482\n",
      "-0.5375400185585022\n",
      "-0.5068411231040955\n",
      "-0.5348066091537476\n",
      "-0.5291644930839539\n",
      "-0.5511273741722107\n",
      "-0.5391327738761902\n",
      "-0.4939398467540741\n",
      "-0.5391300320625305\n",
      "-0.5261772871017456\n",
      "-0.5227065086364746\n",
      "-0.541895866394043\n",
      "-0.5017738342285156\n",
      "-0.5238174796104431\n",
      "-0.4847983717918396\n",
      "-0.5320841670036316\n",
      "-0.5115839242935181\n",
      "-0.5144695043563843\n",
      "-0.5304350256919861\n",
      "-0.5302339196205139\n",
      "-0.5729946494102478\n",
      "-0.5329433679580688\n",
      "-0.5281879305839539\n",
      "-0.5283364653587341\n",
      "-0.5273564457893372\n",
      "-0.5268775820732117\n",
      "-0.5199861526489258\n",
      "-0.5303879976272583\n",
      "-0.5342541337013245\n",
      "-0.5148153305053711\n",
      "-0.5084033608436584\n",
      "-0.5155712962150574\n",
      "-0.5299122333526611\n",
      "-0.5410858392715454\n",
      "-0.5401312112808228\n",
      "-0.4932939410209656\n",
      "-0.5263945460319519\n",
      "-0.513221800327301\n",
      "-0.5497738718986511\n",
      "-0.535492479801178\n",
      "-0.5275530815124512\n",
      "-0.518365740776062\n",
      "-0.5335715413093567\n",
      "-0.5177968144416809\n",
      "-0.524445116519928\n",
      "-0.5408426523208618\n",
      "-0.5262332558631897\n",
      "-0.5128520727157593\n",
      "-0.5453494787216187\n",
      "-0.5142213106155396\n",
      "-0.524628221988678\n",
      "-0.529789388179779\n",
      "-0.5128574371337891\n",
      "-0.5218777060508728\n",
      "-0.5288976430892944\n",
      "-0.5254442691802979\n",
      "-0.5392588376998901\n",
      "-0.5279062986373901\n",
      "-0.5353569388389587\n",
      "-0.5295302271842957\n",
      "-0.5416973233222961\n",
      "-0.5206486582756042\n",
      "-0.5181405544281006\n",
      "-0.5218014717102051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5101883411407471\n"
     ]
    }
   ],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=300, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "\n",
    "\n",
    "γ =  0.1**2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = gaussian_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train.reshape(-1,1,28,28), y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0] // 20), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"uniform\",\n",
    "    γ_min= 0.1**2, γ_max= 0.4**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 50\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([(net.forward(X, θ)[None,...]).softmax(dim=-1) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pred = []\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "subsamp = 30\n",
    "\n",
    "stride = 10\n",
    "\n",
    "for i in tqdm(range(0,len(X_train), stride)):\n",
    "    \n",
    "    pred.append(predc(X_train[i:i+stride,...].reshape(-1,1,28,28), Θ_1[:subsamp,:]).cpu())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.vstack(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = torch.vstack(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamp = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "((pred.argmax(dim=-1)).float().flatten().cpu() == y_train[:len(pred)].argmax(dim=-1).cpu() ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "pred_test = []\n",
    "for i in tqdm(range(0,len(X_test), stride)):\n",
    "    \n",
    "    pred_test.append(predc(X_test[i:i+stride,...].float().reshape(-1,1,28,28), Θ_1[:subsamp,:]).cpu())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pred_test = pred_test(torch.vstack(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred_test.argmax(dim=-1)).float().flatten().cpu() == y_test.argmax(dim=-1).cpu()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
