{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from torch import datasets\n",
    "\n",
    "from torch import _vmap_internals\n",
    "from torchvision import datasets, transforms\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.drifts import *\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = datasets.MNIST(\"../data/mnist/\", download=True, train=True)\n",
    "images_test = datasets.MNIST(\"../data/mnist/\", download=True, train=False)\n",
    "\n",
    "transform = torch.nn.Sequential(transforms.Normalize((0.1307,), (0.3081)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = images_train.data, images_train.targets\n",
    "X_test, y_test = images_test.data, images_test.targets\n",
    "\n",
    "X_train = torch.flatten(transform(X_train.float()), 1)\n",
    "X_test = torch.flatten(transform(X_test.float()), 1)\n",
    "\n",
    "y_train = F.one_hot(y_train)\n",
    "y_test = F.one_hot(y_test)\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c969fd29504f>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
      "<ipython-input-5-c969fd29504f>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
      "<ipython-input-5-c969fd29504f>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
      "<ipython-input-5-c969fd29504f>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_test, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim, out_dim, device=device, depth=1, width=50, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=3.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        cel = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ll_cel = -1.0 * cel(preds, y.argmax(dim=1))\n",
    "        return ll_cel\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d42c29a71f4519a37c7d108b0ad6f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:143: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f = _vmap_internals.vmap(f_)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:144: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f_detached = _vmap_internals.vmap(sde.f_detached)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:152: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  g = _vmap_internals.vmap(sde.g)\n",
      "<ipython-input-8-be3279387231>:30: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n",
      "/home/fav25/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4022300243377686\n",
      "2.2128050327301025\n",
      "1.9833970069885254\n",
      "1.6329466104507446\n",
      "1.3610379695892334\n",
      "1.1391723155975342\n",
      "0.980584979057312\n",
      "0.8404780030250549\n",
      "0.7329635620117188\n",
      "0.6571563482284546\n",
      "0.5830343961715698\n",
      "0.5209563970565796\n",
      "0.5016452074050903\n",
      "0.4762250781059265\n",
      "0.45124584436416626\n",
      "0.4096267521381378\n",
      "0.40328866243362427\n",
      "0.3922756016254425\n",
      "0.36632657051086426\n",
      "0.35734695196151733\n",
      "0.3713151812553406\n",
      "0.34074217081069946\n",
      "0.3391034007072449\n",
      "0.31357333064079285\n",
      "0.3057865500450134\n",
      "0.3094225525856018\n",
      "0.3050806522369385\n",
      "0.29477980732917786\n",
      "0.3075468838214874\n",
      "0.30766722559928894\n",
      "0.2836025357246399\n",
      "0.3036523461341858\n",
      "0.29038694500923157\n",
      "0.2780734896659851\n",
      "0.27936920523643494\n",
      "0.28309139609336853\n",
      "0.2751404047012329\n",
      "0.27318280935287476\n",
      "0.2971305251121521\n",
      "0.2868804931640625\n",
      "0.26342684030532837\n",
      "0.2614476978778839\n",
      "0.29014554619789124\n",
      "0.28242701292037964\n",
      "0.2679479420185089\n",
      "0.27000442147254944\n",
      "0.273060142993927\n",
      "0.2707056999206543\n",
      "0.27386710047721863\n",
      "0.28714215755462646\n",
      "0.29627227783203125\n",
      "0.2620818316936493\n",
      "0.2727094292640686\n",
      "0.26597341895103455\n",
      "0.27039510011672974\n",
      "0.25985610485076904\n",
      "0.25714704394340515\n",
      "0.27420347929000854\n",
      "0.27719810605049133\n",
      "0.2771316468715668\n",
      "0.2520351707935333\n",
      "0.255969375371933\n",
      "0.2568662166595459\n",
      "0.25450924038887024\n",
      "0.2419787347316742\n",
      "0.2440088838338852\n",
      "0.24439935386180878\n",
      "0.2739901542663574\n",
      "0.2730252742767334\n",
      "0.24737171828746796\n",
      "0.23676718771457672\n",
      "0.2557540237903595\n",
      "0.24747061729431152\n",
      "0.25630590319633484\n",
      "0.2637363374233246\n",
      "0.2730104327201843\n",
      "0.2478126436471939\n",
      "0.2450987547636032\n",
      "0.24413429200649261\n",
      "0.2362247109413147\n",
      "0.24907878041267395\n",
      "0.24951767921447754\n",
      "0.2567193806171417\n",
      "0.2558431625366211\n",
      "0.25738394260406494\n",
      "0.2520381510257721\n",
      "0.25122636556625366\n",
      "0.2308199256658554\n",
      "0.2670072615146637\n",
      "0.2646790146827698\n",
      "0.2526818513870239\n",
      "0.264375776052475\n",
      "0.2606791853904724\n",
      "0.27796438336372375\n",
      "0.2550942897796631\n",
      "0.26152828335762024\n",
      "0.24605700373649597\n",
      "0.265048623085022\n",
      "0.24524779617786407\n",
      "0.25839826464653015\n",
      "0.2502044439315796\n",
      "0.2332455813884735\n",
      "0.2569742202758789\n",
      "0.2343425303697586\n",
      "0.2453547716140747\n",
      "0.23823119699954987\n",
      "0.24446119368076324\n",
      "0.24383412301540375\n",
      "0.23213647305965424\n",
      "0.2561676502227783\n",
      "0.23479007184505463\n",
      "0.23697739839553833\n",
      "0.22044672071933746\n",
      "0.21882642805576324\n",
      "0.21646754443645477\n",
      "0.2185574322938919\n",
      "0.2343207746744156\n",
      "0.22971601784229279\n",
      "0.23344911634922028\n",
      "0.24195009469985962\n",
      "0.26391464471817017\n",
      "0.2575426399707794\n",
      "0.24115872383117676\n",
      "0.22466011345386505\n",
      "0.2086578607559204\n",
      "0.21793542802333832\n",
      "0.23333078622817993\n",
      "0.23083347082138062\n",
      "0.24696362018585205\n",
      "0.23381078243255615\n",
      "0.21347306668758392\n",
      "0.22873277962207794\n",
      "0.2432132065296173\n",
      "0.223581463098526\n",
      "0.22357596457004547\n",
      "0.2234269231557846\n",
      "0.22588816285133362\n",
      "0.22489334642887115\n",
      "0.22509388625621796\n",
      "0.23427744209766388\n",
      "0.2329539805650711\n",
      "0.21926553547382355\n",
      "0.22746099531650543\n",
      "0.2310078740119934\n",
      "0.2252095490694046\n",
      "0.224725142121315\n",
      "0.2020506113767624\n",
      "0.23215213418006897\n",
      "0.22713232040405273\n",
      "0.23201408982276917\n",
      "0.20898158848285675\n",
      "0.21800807118415833\n",
      "0.22081533074378967\n",
      "0.22979432344436646\n",
      "0.22595687210559845\n",
      "0.2179093062877655\n",
      "0.23773394525051117\n",
      "0.23388026654720306\n",
      "0.2268477976322174\n",
      "0.21446864306926727\n",
      "0.215478777885437\n",
      "0.20858263969421387\n",
      "0.22626893222332\n",
      "0.2117544710636139\n",
      "0.2195238471031189\n",
      "0.22705145180225372\n",
      "0.22121870517730713\n",
      "0.23007157444953918\n",
      "0.23443515598773956\n",
      "0.24031029641628265\n",
      "0.22848883271217346\n",
      "0.23290714621543884\n",
      "0.23443959653377533\n",
      "0.2125241905450821\n",
      "0.23976393043994904\n",
      "0.22149017453193665\n",
      "0.2242598831653595\n",
      "0.21981757879257202\n",
      "0.22258083522319794\n",
      "0.2331501990556717\n",
      "0.2121819406747818\n",
      "0.216569721698761\n",
      "0.2207183986902237\n",
      "0.208783358335495\n",
      "0.23264479637145996\n",
      "0.2074991911649704\n",
      "0.2047683745622635\n",
      "0.2166537493467331\n",
      "0.210536390542984\n",
      "0.21978165209293365\n",
      "0.1873263120651245\n",
      "0.23358343541622162\n",
      "0.2329523116350174\n",
      "0.22613269090652466\n",
      "0.22865964472293854\n",
      "0.2066916674375534\n",
      "0.22403211891651154\n",
      "0.2349870800971985\n",
      "0.22426167130470276\n",
      "0.21912340819835663\n",
      "0.24268969893455505\n",
      "0.23263941705226898\n",
      "0.2188972532749176\n",
      "0.21787196397781372\n",
      "0.22693224251270294\n",
      "0.22633568942546844\n",
      "0.20517760515213013\n",
      "0.2261725664138794\n",
      "0.2145136147737503\n",
      "0.21864593029022217\n",
      "0.21447886526584625\n",
      "0.21573016047477722\n",
      "0.2325509488582611\n",
      "0.22573222219944\n",
      "0.2371230572462082\n",
      "0.22517982125282288\n",
      "0.2194717973470688\n",
      "0.2080077826976776\n",
      "0.2175925076007843\n",
      "0.22139179706573486\n",
      "0.22464314103126526\n",
      "0.22230836749076843\n",
      "0.21020624041557312\n",
      "0.23080456256866455\n",
      "0.21461492776870728\n",
      "0.21082578599452972\n",
      "0.2126179039478302\n",
      "0.21992549300193787\n",
      "0.22192630171775818\n",
      "0.21695157885551453\n",
      "0.21795977652072906\n",
      "0.20031149685382843\n",
      "0.19951242208480835\n",
      "0.19498762488365173\n",
      "0.23237818479537964\n",
      "0.20217636227607727\n",
      "0.20592248439788818\n",
      "0.2355981022119522\n",
      "0.2110755741596222\n",
      "0.1975383311510086\n",
      "0.2014407068490982\n",
      "0.21844537556171417\n",
      "0.21301297843456268\n",
      "0.22075068950653076\n",
      "0.19529570639133453\n",
      "0.22111216187477112\n",
      "0.21167021989822388\n",
      "0.22305020689964294\n",
      "0.23289908468723297\n",
      "0.2549867331981659\n",
      "0.24683664739131927\n",
      "0.2197626382112503\n",
      "0.20254579186439514\n",
      "0.22451959550380707\n",
      "0.2275533229112625\n",
      "0.2142951488494873\n",
      "0.2433004528284073\n",
      "0.23374846577644348\n",
      "0.23077014088630676\n",
      "0.24259920418262482\n",
      "0.21974794566631317\n",
      "0.2050780951976776\n",
      "0.22202585637569427\n",
      "0.24792449176311493\n",
      "0.23430034518241882\n",
      "0.23420372605323792\n",
      "0.23983603715896606\n",
      "0.2018813043832779\n",
      "0.19515873491764069\n",
      "0.21042944490909576\n",
      "0.19903497397899628\n",
      "0.18408800661563873\n",
      "0.2034575194120407\n",
      "0.21052691340446472\n",
      "0.2195822149515152\n",
      "0.22149387001991272\n",
      "0.21044737100601196\n",
      "0.21940702199935913\n",
      "0.21420393884181976\n",
      "0.20484285056591034\n",
      "0.20209527015686035\n",
      "0.2381938397884369\n",
      "0.2213190346956253\n",
      "0.21368937194347382\n",
      "0.22722159326076508\n",
      "0.2320864051580429\n",
      "0.2186707705259323\n",
      "0.21868659555912018\n",
      "0.19010666012763977\n",
      "0.21390946209430695\n",
      "0.21299515664577484\n",
      "0.22031454741954803\n",
      "0.21629071235656738\n",
      "0.21317802369594574\n",
      "0.2072102278470993\n",
      "0.21440446376800537\n",
      "0.23125888407230377\n",
      "0.21833917498588562\n",
      "0.2156202793121338\n",
      "0.25536197423934937\n",
      "0.2062642127275467\n",
      "0.2104365974664688\n",
      "0.1964864283800125\n",
      "0.2026720941066742\n",
      "0.22414879500865936\n",
      "0.20967048406600952\n",
      "0.21990300714969635\n",
      "0.20178522169589996\n",
      "0.22154878079891205\n",
      "0.2098424881696701\n",
      "0.2154460996389389\n",
      "0.21430815756320953\n",
      "0.22508442401885986\n",
      "0.24008320271968842\n",
      "0.21235758066177368\n",
      "0.20925764739513397\n",
      "0.23392435908317566\n",
      "0.22160658240318298\n",
      "0.22430558502674103\n",
      "0.2330082207918167\n",
      "0.20699599385261536\n",
      "0.22405283153057098\n",
      "0.2302137166261673\n",
      "0.19895599782466888\n",
      "0.22369100153446198\n",
      "0.22234627604484558\n",
      "0.22866441309452057\n",
      "0.21329466998577118\n",
      "0.186810702085495\n",
      "0.21189658343791962\n",
      "0.2142641544342041\n",
      "0.21378128230571747\n",
      "0.21370798349380493\n",
      "0.2108553647994995\n",
      "0.21053281426429749\n",
      "0.24565128982067108\n",
      "0.21748203039169312\n",
      "0.20527563989162445\n",
      "0.21648834645748138\n",
      "0.21410298347473145\n",
      "0.2416069358587265\n",
      "0.22034454345703125\n",
      "0.21392638981342316\n",
      "0.19722488522529602\n",
      "0.20496827363967896\n",
      "0.2321723848581314\n",
      "0.21017539501190186\n",
      "0.21790286898612976\n",
      "0.2342812716960907\n",
      "0.24386994540691376\n",
      "0.21669776737689972\n",
      "0.24519632756710052\n",
      "0.23098433017730713\n",
      "0.22530843317508698\n",
      "0.2280183732509613\n",
      "0.23019783198833466\n",
      "0.23579920828342438\n",
      "0.2423798143863678\n",
      "0.22627338767051697\n",
      "0.23231923580169678\n",
      "0.22970828413963318\n",
      "0.21775148808956146\n",
      "0.23875461518764496\n",
      "0.23491531610488892\n",
      "0.23964641988277435\n",
      "0.20595568418502808\n",
      "0.23281912505626678\n",
      "0.2201651632785797\n",
      "0.2054140865802765\n",
      "0.21763208508491516\n",
      "0.2203165888786316\n",
      "0.22382479906082153\n",
      "0.2091839760541916\n",
      "0.2118677943944931\n",
      "0.20539414882659912\n",
      "0.21903885900974274\n",
      "0.21910548210144043\n",
      "0.2010694146156311\n",
      "0.19767720997333527\n",
      "0.1821507215499878\n",
      "0.21017543971538544\n",
      "0.21387599408626556\n",
      "0.20197367668151855\n",
      "0.20836366713047028\n",
      "0.21015450358390808\n",
      "0.1796065866947174\n",
      "0.20017960667610168\n",
      "0.2079107165336609\n",
      "0.20276975631713867\n",
      "0.20126019418239594\n",
      "0.1986338496208191\n",
      "0.21368886530399323\n",
      "0.20443421602249146\n",
      "0.2045922875404358\n",
      "0.20885924994945526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=300, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "\n",
    "\n",
    "γ =  0.1**2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = gaussian_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0] // 5), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"uniform\",\n",
    "    γ_min= 0.1**2, γ_max= 0.4**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.9185),\n",
       " tensor(0.8701),\n",
       " tensor(0.5066),\n",
       " tensor(0.3858),\n",
       " tensor(0.3341),\n",
       " tensor(0.3049),\n",
       " tensor(0.2870),\n",
       " tensor(0.2831),\n",
       " tensor(0.2731),\n",
       " tensor(0.2750),\n",
       " tensor(0.2735),\n",
       " tensor(0.2691),\n",
       " tensor(0.2523),\n",
       " tensor(0.2566),\n",
       " tensor(0.2520),\n",
       " tensor(0.2493),\n",
       " tensor(0.2537),\n",
       " tensor(0.2532),\n",
       " tensor(0.2622),\n",
       " tensor(0.2553),\n",
       " tensor(0.2440),\n",
       " tensor(0.2430),\n",
       " tensor(0.2255),\n",
       " tensor(0.2316),\n",
       " tensor(0.2392),\n",
       " tensor(0.2326),\n",
       " tensor(0.2265),\n",
       " tensor(0.2267),\n",
       " tensor(0.2272),\n",
       " tensor(0.2236),\n",
       " tensor(0.2207),\n",
       " tensor(0.2262),\n",
       " tensor(0.2163),\n",
       " tensor(0.2306),\n",
       " tensor(0.2296),\n",
       " tensor(0.2243),\n",
       " tensor(0.2182),\n",
       " tensor(0.2118),\n",
       " tensor(0.2217),\n",
       " tensor(0.2218),\n",
       " tensor(0.2278),\n",
       " tensor(0.2182),\n",
       " tensor(0.2251),\n",
       " tensor(0.2183),\n",
       " tensor(0.2205),\n",
       " tensor(0.2164),\n",
       " tensor(0.2090),\n",
       " tensor(0.2105),\n",
       " tensor(0.2098),\n",
       " tensor(0.2287),\n",
       " tensor(0.2242),\n",
       " tensor(0.2329),\n",
       " tensor(0.2258),\n",
       " tensor(0.2163),\n",
       " tensor(0.2033),\n",
       " tensor(0.2141),\n",
       " tensor(0.2205),\n",
       " tensor(0.2147),\n",
       " tensor(0.2140),\n",
       " tensor(0.2270),\n",
       " tensor(0.2080),\n",
       " tensor(0.2125),\n",
       " tensor(0.2215),\n",
       " tensor(0.2244),\n",
       " tensor(0.2168),\n",
       " tensor(0.2126),\n",
       " tensor(0.2126),\n",
       " tensor(0.2198),\n",
       " tensor(0.2156),\n",
       " tensor(0.2277),\n",
       " tensor(0.2292),\n",
       " tensor(0.2334),\n",
       " tensor(0.2322),\n",
       " tensor(0.2164),\n",
       " tensor(0.2141),\n",
       " tensor(0.2038),\n",
       " tensor(0.2089),\n",
       " tensor(0.1983),\n",
       " tensor(0.2060)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff3de26d1f0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  4.,  8.,  8., 10., 19., 17., 13., 13.,  5.]),\n",
       " array([-0.26206753, -0.2176878 , -0.17330807, -0.12892835, -0.08454862,\n",
       "        -0.0401689 ,  0.00421083,  0.04859056,  0.09297028,  0.13735001,\n",
       "         0.18172973], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([(net.forward(X, θ)[None,...]).softmax(dim=-1) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9394, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred.argmax(dim=-1)).float().flatten()== y_train.argmax(dim=-1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9406, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test.argmax(dim=-1)).float().flatten()== y_test.argmax(dim=-1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
