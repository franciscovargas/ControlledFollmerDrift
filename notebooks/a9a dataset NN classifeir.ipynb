{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 109.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /local/scratch/home/fav25/.local/lib/python3.8/site-packages (from scikit-learn) (1.7.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4681, 100.0)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14b0c8a3f644a4a96fc6b75295075de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-223-356f6d07a40e>:29: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685671865940094\n",
      "0.6337451338768005\n",
      "0.594122588634491\n",
      "0.5701420903205872\n",
      "0.5517074465751648\n",
      "0.5362404584884644\n",
      "0.5272958278656006\n",
      "0.5123623013496399\n",
      "0.5098778009414673\n",
      "0.5017162561416626\n",
      "0.4982032775878906\n",
      "0.4926101863384247\n",
      "0.48154959082603455\n",
      "0.47604429721832275\n",
      "0.47294533252716064\n",
      "0.471507728099823\n",
      "0.47002291679382324\n",
      "0.4715939164161682\n",
      "0.4732230603694916\n",
      "0.47234293818473816\n",
      "0.4675259590148926\n",
      "0.4682024419307709\n",
      "0.4695989787578583\n",
      "0.4662298262119293\n",
      "0.4664607644081116\n",
      "0.4636399745941162\n",
      "0.46165522933006287\n",
      "0.4556082487106323\n",
      "0.4605257511138916\n",
      "0.46034419536590576\n",
      "0.46026134490966797\n",
      "0.45912039279937744\n",
      "0.4510858654975891\n",
      "0.4550943970680237\n",
      "0.4481995403766632\n",
      "0.4504145681858063\n",
      "0.4517788887023926\n",
      "0.4467530846595764\n",
      "0.4460289180278778\n",
      "0.447070837020874\n",
      "0.4468604028224945\n",
      "0.44725027680397034\n",
      "0.4378793239593506\n",
      "0.4449732005596161\n",
      "0.4417146146297455\n",
      "0.44826605916023254\n",
      "0.44377315044403076\n",
      "0.43960335850715637\n",
      "0.44425952434539795\n",
      "0.43787726759910583\n",
      "0.4418650269508362\n",
      "0.4387127757072449\n",
      "0.44035303592681885\n",
      "0.4436842203140259\n",
      "0.44226956367492676\n",
      "0.44408532977104187\n",
      "0.43697604537010193\n",
      "0.44284388422966003\n",
      "0.4387485980987549\n",
      "0.4424862861633301\n",
      "0.44428750872612\n",
      "0.4450240731239319\n",
      "0.4412555396556854\n",
      "0.4401146471500397\n",
      "0.44067442417144775\n",
      "0.4371713101863861\n",
      "0.4431016147136688\n",
      "0.43943771719932556\n",
      "0.4396204650402069\n",
      "0.44177553057670593\n",
      "0.4369919002056122\n",
      "0.444019079208374\n",
      "0.43895792961120605\n",
      "0.44438156485557556\n",
      "0.4380274713039398\n",
      "0.44840070605278015\n",
      "0.43777400255203247\n",
      "0.4410903751850128\n",
      "0.439179927110672\n",
      "0.43550431728363037\n",
      "0.4386143386363983\n",
      "0.43938148021698\n",
      "0.43988117575645447\n",
      "0.4410838782787323\n",
      "0.44264546036720276\n",
      "0.4382338225841522\n",
      "0.44189128279685974\n",
      "0.4384814202785492\n",
      "0.4436911344528198\n",
      "0.44112062454223633\n",
      "0.441684365272522\n",
      "0.44212886691093445\n",
      "0.43863624334335327\n",
      "0.4418517053127289\n",
      "0.43985432386398315\n",
      "0.43969348073005676\n",
      "0.4407061040401459\n",
      "0.4433285892009735\n",
      "0.44078710675239563\n",
      "0.44159454107284546\n",
      "0.44036105275154114\n",
      "0.44082924723625183\n",
      "0.4388594627380371\n",
      "0.4375341534614563\n",
      "0.44156768918037415\n",
      "0.43858474493026733\n",
      "0.44200411438941956\n",
      "0.43999144434928894\n",
      "0.4401114583015442\n",
      "0.43920162320137024\n",
      "0.4383598268032074\n",
      "0.4403180480003357\n",
      "0.4396060109138489\n",
      "0.44232386350631714\n",
      "0.442935049533844\n",
      "0.4388217628002167\n",
      "0.44053345918655396\n",
      "0.4433066248893738\n",
      "0.4428059160709381\n",
      "0.44330230355262756\n",
      "0.4382442831993103\n",
      "0.43926548957824707\n",
      "0.4354221820831299\n",
      "0.4382253885269165\n",
      "0.443069189786911\n",
      "0.4406892657279968\n",
      "0.43848106265068054\n",
      "0.43962356448173523\n",
      "0.444681316614151\n",
      "0.44177210330963135\n",
      "0.4398276209831238\n",
      "0.4365665018558502\n",
      "0.44031545519828796\n",
      "0.44357535243034363\n",
      "0.44397270679473877\n",
      "0.4412018060684204\n",
      "0.4406578540802002\n",
      "0.4436039328575134\n",
      "0.4425506293773651\n",
      "0.4438580870628357\n",
      "0.4430612623691559\n",
      "0.44297924637794495\n",
      "0.44489556550979614\n",
      "0.44079557061195374\n",
      "0.44257643818855286\n",
      "0.44499796628952026\n",
      "0.4421328902244568\n",
      "0.4410843849182129\n",
      "0.44149115681648254\n",
      "0.44501590728759766\n",
      "0.44379723072052\n",
      "0.4452943205833435\n",
      "0.44177722930908203\n",
      "0.44312047958374023\n",
      "0.4439578056335449\n",
      "0.4435022473335266\n",
      "0.4431631565093994\n",
      "0.43844014406204224\n",
      "0.44535064697265625\n",
      "0.44284477829933167\n",
      "0.44178688526153564\n",
      "0.4420778155326843\n",
      "0.4403238594532013\n",
      "0.4423553943634033\n",
      "0.44760802388191223\n",
      "0.44316110014915466\n",
      "0.4460141658782959\n",
      "0.4464724063873291\n",
      "0.44323042035102844\n",
      "0.4441545903682709\n",
      "0.44243094325065613\n",
      "0.4456246495246887\n",
      "0.44849473237991333\n",
      "0.4451286196708679\n",
      "0.4418899118900299\n",
      "0.44363152980804443\n",
      "0.445599228143692\n",
      "0.44440916180610657\n",
      "0.44426247477531433\n",
      "0.440788209438324\n",
      "0.44485974311828613\n",
      "0.4461732506752014\n",
      "0.4434029459953308\n",
      "0.44885438680648804\n",
      "0.4409862458705902\n",
      "0.44644707441329956\n",
      "0.4451923668384552\n",
      "0.44443055987358093\n",
      "0.439150333404541\n",
      "0.4411328136920929\n",
      "0.44434523582458496\n",
      "0.44117113947868347\n",
      "0.44304248690605164\n",
      "0.44350796937942505\n",
      "0.4394493103027344\n",
      "0.4457472264766693\n",
      "0.44730937480926514\n",
      "0.44054603576660156\n",
      "0.44545766711235046\n",
      "0.4465886950492859\n",
      "0.4453370273113251\n",
      "0.444944828748703\n",
      "0.44517406821250916\n",
      "0.44773364067077637\n",
      "0.4486694037914276\n",
      "0.44541695713996887\n",
      "0.4446246922016144\n",
      "0.45058152079582214\n",
      "0.4453774392604828\n",
      "0.44418880343437195\n",
      "0.4436974823474884\n",
      "0.447892963886261\n",
      "0.4481372535228729\n",
      "0.4474707841873169\n",
      "0.44389504194259644\n",
      "0.43975308537483215\n",
      "0.44342759251594543\n",
      "0.4423831105232239\n",
      "0.44339925050735474\n",
      "0.4453308880329132\n",
      "0.4551515579223633\n",
      "0.4455967843532562\n",
      "0.4495985507965088\n",
      "0.4470908045768738\n",
      "0.4465150535106659\n",
      "0.4476172924041748\n",
      "0.4441344738006592\n",
      "0.44439011812210083\n",
      "0.44923025369644165\n",
      "0.45181694626808167\n",
      "0.45139384269714355\n",
      "0.44896620512008667\n",
      "0.45532751083374023\n",
      "0.45348137617111206\n",
      "0.45088279247283936\n",
      "0.4544822871685028\n",
      "0.44581764936447144\n",
      "0.4501941204071045\n",
      "0.4488019645214081\n",
      "0.4447810649871826\n",
      "0.4463943541049957\n",
      "0.4461608827114105\n",
      "0.4490302801132202\n",
      "0.4467030167579651\n",
      "0.4488960802555084\n",
      "0.44758933782577515\n",
      "0.449554979801178\n",
      "0.44984182715415955\n",
      "0.44691428542137146\n",
      "0.446407288312912\n",
      "0.4504999816417694\n",
      "0.4449073374271393\n",
      "0.4475595951080322\n",
      "0.44802966713905334\n",
      "0.44622543454170227\n",
      "0.4453504681587219\n",
      "0.44666171073913574\n",
      "0.4424470365047455\n",
      "0.4489700198173523\n",
      "0.4462628960609436\n",
      "0.44442370533943176\n",
      "0.4507051110267639\n",
      "0.4449162781238556\n",
      "0.44729340076446533\n",
      "0.44615665078163147\n",
      "0.448467493057251\n",
      "0.44684311747550964\n",
      "0.45054179430007935\n",
      "0.4455915093421936\n",
      "0.4479997456073761\n",
      "0.4470953643321991\n",
      "0.4521535038948059\n",
      "0.44826850295066833\n",
      "0.44418177008628845\n",
      "0.44656673073768616\n",
      "0.449862003326416\n",
      "0.4455140233039856\n",
      "0.44939911365509033\n",
      "0.4470944106578827\n",
      "0.443913996219635\n",
      "0.4454854130744934\n",
      "0.44614601135253906\n",
      "0.4495217502117157\n",
      "0.4458991587162018\n",
      "0.44837987422943115\n",
      "0.4468989372253418\n",
      "0.45146650075912476\n",
      "0.45215022563934326\n",
      "0.4454127550125122\n",
      "0.4501080811023712\n",
      "0.4498118460178375\n",
      "0.4547504186630249\n",
      "0.45261290669441223\n",
      "0.44803160429000854\n",
      "0.4510604441165924\n",
      "0.4528643488883972\n",
      "0.44962891936302185\n",
      "0.4490651488304138\n",
      "0.44654223322868347\n",
      "0.4528323709964752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.04\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, gaussian_prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=300, batch_size_data=int(X_train.shape[0]), batch_size_Θ=20,\n",
    "    batchnorm=True, device=device, lr=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6857),\n",
       " tensor(0.6337),\n",
       " tensor(0.5941),\n",
       " tensor(0.5701),\n",
       " tensor(0.5517),\n",
       " tensor(0.5362),\n",
       " tensor(0.5273),\n",
       " tensor(0.5124),\n",
       " tensor(0.5099),\n",
       " tensor(0.5017),\n",
       " tensor(0.4982),\n",
       " tensor(0.4926),\n",
       " tensor(0.4815),\n",
       " tensor(0.4760),\n",
       " tensor(0.4729),\n",
       " tensor(0.4715),\n",
       " tensor(0.4700),\n",
       " tensor(0.4716),\n",
       " tensor(0.4732),\n",
       " tensor(0.4723),\n",
       " tensor(0.4675),\n",
       " tensor(0.4682),\n",
       " tensor(0.4696),\n",
       " tensor(0.4662),\n",
       " tensor(0.4665),\n",
       " tensor(0.4636),\n",
       " tensor(0.4617),\n",
       " tensor(0.4556),\n",
       " tensor(0.4605),\n",
       " tensor(0.4603),\n",
       " tensor(0.4603),\n",
       " tensor(0.4591),\n",
       " tensor(0.4511),\n",
       " tensor(0.4551),\n",
       " tensor(0.4482),\n",
       " tensor(0.4504),\n",
       " tensor(0.4518),\n",
       " tensor(0.4468),\n",
       " tensor(0.4460),\n",
       " tensor(0.4471),\n",
       " tensor(0.4469),\n",
       " tensor(0.4473),\n",
       " tensor(0.4379),\n",
       " tensor(0.4450),\n",
       " tensor(0.4417),\n",
       " tensor(0.4483),\n",
       " tensor(0.4438),\n",
       " tensor(0.4396),\n",
       " tensor(0.4443),\n",
       " tensor(0.4379),\n",
       " tensor(0.4419),\n",
       " tensor(0.4387),\n",
       " tensor(0.4404),\n",
       " tensor(0.4437),\n",
       " tensor(0.4423),\n",
       " tensor(0.4441),\n",
       " tensor(0.4370),\n",
       " tensor(0.4428),\n",
       " tensor(0.4387),\n",
       " tensor(0.4425),\n",
       " tensor(0.4443),\n",
       " tensor(0.4450),\n",
       " tensor(0.4413),\n",
       " tensor(0.4401),\n",
       " tensor(0.4407),\n",
       " tensor(0.4372),\n",
       " tensor(0.4431),\n",
       " tensor(0.4394),\n",
       " tensor(0.4396),\n",
       " tensor(0.4418),\n",
       " tensor(0.4370),\n",
       " tensor(0.4440),\n",
       " tensor(0.4390),\n",
       " tensor(0.4444),\n",
       " tensor(0.4380),\n",
       " tensor(0.4484),\n",
       " tensor(0.4378),\n",
       " tensor(0.4411),\n",
       " tensor(0.4392),\n",
       " tensor(0.4355),\n",
       " tensor(0.4386),\n",
       " tensor(0.4394),\n",
       " tensor(0.4399),\n",
       " tensor(0.4411),\n",
       " tensor(0.4426),\n",
       " tensor(0.4382),\n",
       " tensor(0.4419),\n",
       " tensor(0.4385),\n",
       " tensor(0.4437),\n",
       " tensor(0.4411),\n",
       " tensor(0.4417),\n",
       " tensor(0.4421),\n",
       " tensor(0.4386),\n",
       " tensor(0.4419),\n",
       " tensor(0.4399),\n",
       " tensor(0.4397),\n",
       " tensor(0.4407),\n",
       " tensor(0.4433),\n",
       " tensor(0.4408),\n",
       " tensor(0.4416),\n",
       " tensor(0.4404),\n",
       " tensor(0.4408),\n",
       " tensor(0.4389),\n",
       " tensor(0.4375),\n",
       " tensor(0.4416),\n",
       " tensor(0.4386),\n",
       " tensor(0.4420),\n",
       " tensor(0.4400),\n",
       " tensor(0.4401),\n",
       " tensor(0.4392),\n",
       " tensor(0.4384),\n",
       " tensor(0.4403),\n",
       " tensor(0.4396),\n",
       " tensor(0.4423),\n",
       " tensor(0.4429),\n",
       " tensor(0.4388),\n",
       " tensor(0.4405),\n",
       " tensor(0.4433),\n",
       " tensor(0.4428),\n",
       " tensor(0.4433),\n",
       " tensor(0.4382),\n",
       " tensor(0.4393),\n",
       " tensor(0.4354),\n",
       " tensor(0.4382),\n",
       " tensor(0.4431),\n",
       " tensor(0.4407),\n",
       " tensor(0.4385),\n",
       " tensor(0.4396),\n",
       " tensor(0.4447),\n",
       " tensor(0.4418),\n",
       " tensor(0.4398),\n",
       " tensor(0.4366),\n",
       " tensor(0.4403),\n",
       " tensor(0.4436),\n",
       " tensor(0.4440),\n",
       " tensor(0.4412),\n",
       " tensor(0.4407),\n",
       " tensor(0.4436),\n",
       " tensor(0.4426),\n",
       " tensor(0.4439),\n",
       " tensor(0.4431),\n",
       " tensor(0.4430),\n",
       " tensor(0.4449),\n",
       " tensor(0.4408),\n",
       " tensor(0.4426),\n",
       " tensor(0.4450),\n",
       " tensor(0.4421),\n",
       " tensor(0.4411),\n",
       " tensor(0.4415),\n",
       " tensor(0.4450),\n",
       " tensor(0.4438),\n",
       " tensor(0.4453),\n",
       " tensor(0.4418),\n",
       " tensor(0.4431),\n",
       " tensor(0.4440),\n",
       " tensor(0.4435),\n",
       " tensor(0.4432),\n",
       " tensor(0.4384),\n",
       " tensor(0.4454),\n",
       " tensor(0.4428),\n",
       " tensor(0.4418),\n",
       " tensor(0.4421),\n",
       " tensor(0.4403),\n",
       " tensor(0.4424),\n",
       " tensor(0.4476),\n",
       " tensor(0.4432),\n",
       " tensor(0.4460),\n",
       " tensor(0.4465),\n",
       " tensor(0.4432),\n",
       " tensor(0.4442),\n",
       " tensor(0.4424),\n",
       " tensor(0.4456),\n",
       " tensor(0.4485),\n",
       " tensor(0.4451),\n",
       " tensor(0.4419),\n",
       " tensor(0.4436),\n",
       " tensor(0.4456),\n",
       " tensor(0.4444),\n",
       " tensor(0.4443),\n",
       " tensor(0.4408),\n",
       " tensor(0.4449),\n",
       " tensor(0.4462),\n",
       " tensor(0.4434),\n",
       " tensor(0.4489),\n",
       " tensor(0.4410),\n",
       " tensor(0.4464),\n",
       " tensor(0.4452),\n",
       " tensor(0.4444),\n",
       " tensor(0.4392),\n",
       " tensor(0.4411),\n",
       " tensor(0.4443),\n",
       " tensor(0.4412),\n",
       " tensor(0.4430),\n",
       " tensor(0.4435),\n",
       " tensor(0.4394),\n",
       " tensor(0.4457),\n",
       " tensor(0.4473),\n",
       " tensor(0.4405),\n",
       " tensor(0.4455),\n",
       " tensor(0.4466),\n",
       " tensor(0.4453),\n",
       " tensor(0.4449),\n",
       " tensor(0.4452),\n",
       " tensor(0.4477),\n",
       " tensor(0.4487),\n",
       " tensor(0.4454),\n",
       " tensor(0.4446),\n",
       " tensor(0.4506),\n",
       " tensor(0.4454),\n",
       " tensor(0.4442),\n",
       " tensor(0.4437),\n",
       " tensor(0.4479),\n",
       " tensor(0.4481),\n",
       " tensor(0.4475),\n",
       " tensor(0.4439),\n",
       " tensor(0.4398),\n",
       " tensor(0.4434),\n",
       " tensor(0.4424),\n",
       " tensor(0.4434),\n",
       " tensor(0.4453),\n",
       " tensor(0.4552),\n",
       " tensor(0.4456),\n",
       " tensor(0.4496),\n",
       " tensor(0.4471),\n",
       " tensor(0.4465),\n",
       " tensor(0.4476),\n",
       " tensor(0.4441),\n",
       " tensor(0.4444),\n",
       " tensor(0.4492),\n",
       " tensor(0.4518),\n",
       " tensor(0.4514),\n",
       " tensor(0.4490),\n",
       " tensor(0.4553),\n",
       " tensor(0.4535),\n",
       " tensor(0.4509),\n",
       " tensor(0.4545),\n",
       " tensor(0.4458),\n",
       " tensor(0.4502),\n",
       " tensor(0.4488),\n",
       " tensor(0.4448),\n",
       " tensor(0.4464),\n",
       " tensor(0.4462),\n",
       " tensor(0.4490),\n",
       " tensor(0.4467),\n",
       " tensor(0.4489),\n",
       " tensor(0.4476),\n",
       " tensor(0.4496),\n",
       " tensor(0.4498),\n",
       " tensor(0.4469),\n",
       " tensor(0.4464),\n",
       " tensor(0.4505),\n",
       " tensor(0.4449),\n",
       " tensor(0.4476),\n",
       " tensor(0.4480),\n",
       " tensor(0.4462),\n",
       " tensor(0.4454),\n",
       " tensor(0.4467),\n",
       " tensor(0.4424),\n",
       " tensor(0.4490),\n",
       " tensor(0.4463),\n",
       " tensor(0.4444),\n",
       " tensor(0.4507),\n",
       " tensor(0.4449),\n",
       " tensor(0.4473),\n",
       " tensor(0.4462),\n",
       " tensor(0.4485),\n",
       " tensor(0.4468),\n",
       " tensor(0.4505),\n",
       " tensor(0.4456),\n",
       " tensor(0.4480),\n",
       " tensor(0.4471),\n",
       " tensor(0.4522),\n",
       " tensor(0.4483),\n",
       " tensor(0.4442),\n",
       " tensor(0.4466),\n",
       " tensor(0.4499),\n",
       " tensor(0.4455),\n",
       " tensor(0.4494),\n",
       " tensor(0.4471),\n",
       " tensor(0.4439),\n",
       " tensor(0.4455),\n",
       " tensor(0.4461),\n",
       " tensor(0.4495),\n",
       " tensor(0.4459),\n",
       " tensor(0.4484),\n",
       " tensor(0.4469),\n",
       " tensor(0.4515),\n",
       " tensor(0.4522),\n",
       " tensor(0.4454),\n",
       " tensor(0.4501),\n",
       " tensor(0.4498),\n",
       " tensor(0.4548),\n",
       " tensor(0.4526),\n",
       " tensor(0.4480),\n",
       " tensor(0.4511),\n",
       " tensor(0.4529),\n",
       " tensor(0.4496),\n",
       " tensor(0.4491),\n",
       " tensor(0.4465),\n",
       " tensor(0.4528)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f58411e1c70>]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  3.,  3., 13., 24., 22., 13., 15.,  4.,  1.]),\n",
       " array([-0.12461343, -0.10093003, -0.07724663, -0.05356323, -0.02987984,\n",
       "        -0.00619644,  0.01748696,  0.04117035,  0.06485375,  0.08853715,\n",
       "         0.11222055], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7931, device='cuda:0')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7964, device='cuda:0')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
