{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=250, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "net = LinearClassifier(\n",
    "    dim,1, device=device\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 100.0)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431d6271727744a5a768b52c4345b120"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-460-6511ec892fb4>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7800114154815674\n",
      "1.350321888923645\n",
      "1.1223701238632202\n",
      "0.541502058506012\n",
      "0.3938625454902649\n",
      "0.6960066556930542\n",
      "0.7042401432991028\n",
      "0.5165773034095764\n",
      "0.40783625841140747\n",
      "0.4621610641479492\n",
      "0.5202217698097229\n",
      "0.5568899512290955\n",
      "0.5299643278121948\n",
      "0.44273918867111206\n",
      "0.3918544352054596\n",
      "0.42650800943374634\n",
      "0.48108455538749695\n",
      "0.46307945251464844\n",
      "0.41326427459716797\n",
      "0.36806541681289673\n",
      "0.3942098617553711\n",
      "0.4238591492176056\n",
      "0.4415622055530548\n",
      "0.3981708288192749\n",
      "0.36580029129981995\n",
      "0.39190468192100525\n",
      "0.4652920961380005\n",
      "0.3924853801727295\n",
      "0.36425313353538513\n",
      "0.3811483383178711\n",
      "0.4230753183364868\n",
      "0.39709484577178955\n",
      "0.366493821144104\n",
      "0.37786170840263367\n",
      "0.3834759294986725\n",
      "0.3922995328903198\n",
      "0.362172931432724\n",
      "0.3635648787021637\n",
      "0.3621339499950409\n",
      "0.37988245487213135\n",
      "0.3665646016597748\n",
      "0.3597695231437683\n",
      "0.35931292176246643\n",
      "0.365344762802124\n",
      "0.3637769818305969\n",
      "0.35332149267196655\n",
      "0.36173152923583984\n",
      "0.36190396547317505\n",
      "0.3638000190258026\n",
      "0.35479578375816345\n",
      "0.3673415780067444\n",
      "0.36665812134742737\n",
      "0.3622918426990509\n",
      "0.3507339656352997\n",
      "0.3606712520122528\n",
      "0.3561791181564331\n",
      "0.35449159145355225\n",
      "0.35264211893081665\n",
      "0.3551197648048401\n",
      "0.3588336408138275\n",
      "0.35316142439842224\n",
      "0.3608735501766205\n",
      "0.3564034104347229\n",
      "0.3535108268260956\n",
      "0.35445621609687805\n",
      "0.3570193648338318\n",
      "0.3577767610549927\n",
      "0.35536837577819824\n",
      "0.3538837432861328\n",
      "0.3563351035118103\n",
      "0.3537636697292328\n",
      "0.3516392707824707\n",
      "0.3545897603034973\n",
      "0.36661839485168457\n",
      "0.3516160249710083\n",
      "0.36225804686546326\n",
      "0.36145225167274475\n",
      "0.3528473973274231\n",
      "0.35463783144950867\n",
      "0.3561408221721649\n",
      "0.35436078906059265\n",
      "0.34775787591934204\n",
      "0.35789334774017334\n",
      "0.35914403200149536\n",
      "0.350135862827301\n",
      "0.35861849784851074\n",
      "0.3888133466243744\n",
      "0.35190847516059875\n",
      "0.37039265036582947\n",
      "0.3620010316371918\n",
      "0.3562866747379303\n",
      "0.3592439293861389\n",
      "0.35390162467956543\n",
      "0.35019373893737793\n",
      "0.35632315278053284\n",
      "0.35531023144721985\n",
      "0.35906192660331726\n",
      "0.3518112301826477\n",
      "0.35295164585113525\n",
      "0.3656572103500366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=100, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.002, drift=SimpleForwardNetBN_larger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7800),\n",
       " tensor(1.3503),\n",
       " tensor(1.1224),\n",
       " tensor(0.5415),\n",
       " tensor(0.3939),\n",
       " tensor(0.6960),\n",
       " tensor(0.7042),\n",
       " tensor(0.5166),\n",
       " tensor(0.4078),\n",
       " tensor(0.4622),\n",
       " tensor(0.5202),\n",
       " tensor(0.5569),\n",
       " tensor(0.5300),\n",
       " tensor(0.4427),\n",
       " tensor(0.3919),\n",
       " tensor(0.4265),\n",
       " tensor(0.4811),\n",
       " tensor(0.4631),\n",
       " tensor(0.4133),\n",
       " tensor(0.3681),\n",
       " tensor(0.3942),\n",
       " tensor(0.4239),\n",
       " tensor(0.4416),\n",
       " tensor(0.3982),\n",
       " tensor(0.3658),\n",
       " tensor(0.3919),\n",
       " tensor(0.4653),\n",
       " tensor(0.3925),\n",
       " tensor(0.3643),\n",
       " tensor(0.3811),\n",
       " tensor(0.4231),\n",
       " tensor(0.3971),\n",
       " tensor(0.3665),\n",
       " tensor(0.3779),\n",
       " tensor(0.3835),\n",
       " tensor(0.3923),\n",
       " tensor(0.3622),\n",
       " tensor(0.3636),\n",
       " tensor(0.3621),\n",
       " tensor(0.3799),\n",
       " tensor(0.3666),\n",
       " tensor(0.3598),\n",
       " tensor(0.3593),\n",
       " tensor(0.3653),\n",
       " tensor(0.3638),\n",
       " tensor(0.3533),\n",
       " tensor(0.3617),\n",
       " tensor(0.3619),\n",
       " tensor(0.3638),\n",
       " tensor(0.3548),\n",
       " tensor(0.3673),\n",
       " tensor(0.3667),\n",
       " tensor(0.3623),\n",
       " tensor(0.3507),\n",
       " tensor(0.3607),\n",
       " tensor(0.3562),\n",
       " tensor(0.3545),\n",
       " tensor(0.3526),\n",
       " tensor(0.3551),\n",
       " tensor(0.3588),\n",
       " tensor(0.3532),\n",
       " tensor(0.3609),\n",
       " tensor(0.3564),\n",
       " tensor(0.3535),\n",
       " tensor(0.3545),\n",
       " tensor(0.3570),\n",
       " tensor(0.3578),\n",
       " tensor(0.3554),\n",
       " tensor(0.3539),\n",
       " tensor(0.3563),\n",
       " tensor(0.3538),\n",
       " tensor(0.3516),\n",
       " tensor(0.3546),\n",
       " tensor(0.3666),\n",
       " tensor(0.3516),\n",
       " tensor(0.3623),\n",
       " tensor(0.3615),\n",
       " tensor(0.3528),\n",
       " tensor(0.3546),\n",
       " tensor(0.3561),\n",
       " tensor(0.3544),\n",
       " tensor(0.3478),\n",
       " tensor(0.3579),\n",
       " tensor(0.3591),\n",
       " tensor(0.3501),\n",
       " tensor(0.3586),\n",
       " tensor(0.3888),\n",
       " tensor(0.3519),\n",
       " tensor(0.3704),\n",
       " tensor(0.3620),\n",
       " tensor(0.3563),\n",
       " tensor(0.3592),\n",
       " tensor(0.3539),\n",
       " tensor(0.3502),\n",
       " tensor(0.3563),\n",
       " tensor(0.3553),\n",
       " tensor(0.3591),\n",
       " tensor(0.3518),\n",
       " tensor(0.3530),\n",
       " tensor(0.3657)]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f58e9543310>]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  4.,  6., 16., 25., 19., 18.,  9.,  0.,  2.]),\n",
       " array([-0.5054797, -0.3788218, -0.2521639, -0.125506 ,  0.0011519,\n",
       "         0.1278098,  0.2544677,  0.3811256,  0.5077835,  0.6344414,\n",
       "         0.7610993], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8449, device='cuda:0')"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8500, device='cuda:0')"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
