{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "# class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "#         super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "#         self.nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + 1, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, input_dim )\n",
    "#         )\n",
    "        \n",
    "#         self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.softplus\n",
    ")\n",
    "\n",
    "\n",
    "# net = LinearClassifier(\n",
    "#     dim,1, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3751"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim #, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b073c4db5c5e4dff901717f102aac25d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:140: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f = _vmap_internals.vmap(f_)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:141: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f_detached = _vmap_internals.vmap(sde.f_detached)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py(150)stl_relative_entropy_control_cost_xu()\n",
      "-> lng = log_g(ΘT, ln_prior, ln_like_partial, γ_t[-1,0,0], debug=debug)\n",
      "(Pdb) \n",
      "(Pdb) γ_t[:,0,0]\n",
      "tensor([0.1000, 0.1105, 0.1211, 0.1316, 0.1421, 0.1526, 0.1632, 0.1737, 0.1842,\n",
      "        0.1947, 0.1947, 0.1842, 0.1737, 0.1632, 0.1526, 0.1421, 0.1316, 0.1211,\n",
      "        0.1105, 0.1000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.001, drift=SimpleForwardNetBN_larger, schedule=\"linear\",\n",
    "    γ_min=0.1, γ_max=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f37d84e53a0>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 99.]),\n",
       " array([-491.86154 , -445.8174  , -399.77332 , -353.7292  , -307.68506 ,\n",
       "        -261.64093 , -215.59682 , -169.5527  , -123.50858 ,  -77.46446 ,\n",
       "         -31.420343], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7592, device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7638, device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
