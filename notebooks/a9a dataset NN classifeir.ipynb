{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "# class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "#         super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "#         self.nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + 1, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, input_dim )\n",
    "#         )\n",
    "        \n",
    "#         self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.softplus\n",
    ")\n",
    "\n",
    "\n",
    "# net = LinearClassifier(\n",
    "#     dim,1, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3751"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim #, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af75e0b297451f846301c1ee955311"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-351a45896b77>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8186215758323669\n",
      "5.638932228088379\n",
      "0.5418741703033447\n",
      "0.6541256308555603\n",
      "0.6400624513626099\n",
      "0.5644543766975403\n",
      "0.5981833338737488\n",
      "0.49437445402145386\n",
      "0.5374773740768433\n",
      "0.5087399482727051\n",
      "0.5647051334381104\n",
      "0.4778379499912262\n",
      "0.47798651456832886\n",
      "0.5912554860115051\n",
      "0.45249292254447937\n",
      "0.4463699162006378\n",
      "0.5466119050979614\n",
      "0.4480563700199127\n",
      "0.45469486713409424\n",
      "0.5323811769485474\n",
      "0.4488906264305115\n",
      "0.43734070658683777\n",
      "0.4567638635635376\n",
      "0.4145519435405731\n",
      "0.4052802622318268\n",
      "0.4412153363227844\n",
      "0.5198233723640442\n",
      "0.4141315221786499\n",
      "0.435180127620697\n",
      "0.4186280071735382\n",
      "0.4025278389453888\n",
      "0.4790552258491516\n",
      "0.428882360458374\n",
      "0.4186936616897583\n",
      "0.42308685183525085\n",
      "0.39044421911239624\n",
      "0.4012851119041443\n",
      "0.3850614130496979\n",
      "0.3821543753147125\n",
      "0.37665992975234985\n",
      "0.386222243309021\n",
      "0.41276970505714417\n",
      "0.401311457157135\n",
      "0.3895525336265564\n",
      "0.38219791650772095\n",
      "0.3844696879386902\n",
      "0.3816739022731781\n",
      "0.3813711404800415\n",
      "0.43510550260543823\n",
      "0.38153186440467834\n",
      "0.37503668665885925\n",
      "0.3692121207714081\n",
      "0.3704233467578888\n",
      "0.36836573481559753\n",
      "0.36742275953292847\n",
      "0.36421212553977966\n",
      "0.381272554397583\n",
      "0.533523440361023\n",
      "0.7504604458808899\n",
      "0.371772438287735\n",
      "0.5566031336784363\n",
      "0.38649430871009827\n",
      "0.41877105832099915\n",
      "0.4385775029659271\n",
      "0.41132447123527527\n",
      "0.40038254857063293\n",
      "0.41889339685440063\n",
      "0.38765305280685425\n",
      "0.383657306432724\n",
      "0.388083279132843\n",
      "0.37704992294311523\n",
      "0.3753645122051239\n",
      "0.3840242624282837\n",
      "0.37490642070770264\n",
      "0.3776940107345581\n",
      "0.37365761399269104\n",
      "0.38952910900115967\n",
      "0.3757241368293762\n",
      "0.380151629447937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.001, drift=SimpleForwardNetBN_larger, schedule=\"linear\",\n",
    "    γ_min=1e-5, γ_max=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8186),\n",
       " tensor(5.6389),\n",
       " tensor(0.5419),\n",
       " tensor(0.6541),\n",
       " tensor(0.6401),\n",
       " tensor(0.5645),\n",
       " tensor(0.5982),\n",
       " tensor(0.4944),\n",
       " tensor(0.5375),\n",
       " tensor(0.5087),\n",
       " tensor(0.5647),\n",
       " tensor(0.4778),\n",
       " tensor(0.4780),\n",
       " tensor(0.5913),\n",
       " tensor(0.4525),\n",
       " tensor(0.4464),\n",
       " tensor(0.5466),\n",
       " tensor(0.4481),\n",
       " tensor(0.4547),\n",
       " tensor(0.5324),\n",
       " tensor(0.4489),\n",
       " tensor(0.4373),\n",
       " tensor(0.4568),\n",
       " tensor(0.4146),\n",
       " tensor(0.4053),\n",
       " tensor(0.4412),\n",
       " tensor(0.5198),\n",
       " tensor(0.4141),\n",
       " tensor(0.4352),\n",
       " tensor(0.4186),\n",
       " tensor(0.4025),\n",
       " tensor(0.4791),\n",
       " tensor(0.4289),\n",
       " tensor(0.4187),\n",
       " tensor(0.4231),\n",
       " tensor(0.3904),\n",
       " tensor(0.4013),\n",
       " tensor(0.3851),\n",
       " tensor(0.3822),\n",
       " tensor(0.3767),\n",
       " tensor(0.3862),\n",
       " tensor(0.4128),\n",
       " tensor(0.4013),\n",
       " tensor(0.3896),\n",
       " tensor(0.3822),\n",
       " tensor(0.3845),\n",
       " tensor(0.3817),\n",
       " tensor(0.3814),\n",
       " tensor(0.4351),\n",
       " tensor(0.3815),\n",
       " tensor(0.3750),\n",
       " tensor(0.3692),\n",
       " tensor(0.3704),\n",
       " tensor(0.3684),\n",
       " tensor(0.3674),\n",
       " tensor(0.3642),\n",
       " tensor(0.3813),\n",
       " tensor(0.5335),\n",
       " tensor(0.7505),\n",
       " tensor(0.3718),\n",
       " tensor(0.5566),\n",
       " tensor(0.3865),\n",
       " tensor(0.4188),\n",
       " tensor(0.4386),\n",
       " tensor(0.4113),\n",
       " tensor(0.4004),\n",
       " tensor(0.4189),\n",
       " tensor(0.3877),\n",
       " tensor(0.3837),\n",
       " tensor(0.3881),\n",
       " tensor(0.3770),\n",
       " tensor(0.3754),\n",
       " tensor(0.3840),\n",
       " tensor(0.3749),\n",
       " tensor(0.3777),\n",
       " tensor(0.3737),\n",
       " tensor(0.3895),\n",
       " tensor(0.3757),\n",
       " tensor(0.3802)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3176ab34c0>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  4.,  8., 15., 22., 19., 10., 13.,  2.,  4.]),\n",
       " array([-0.5917652 , -0.5007367 , -0.4097082 , -0.3186797 , -0.22765116,\n",
       "        -0.13662265, -0.04559414,  0.04543438,  0.1364629 ,  0.22749141,\n",
       "         0.31851992], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8440, device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8473, device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
