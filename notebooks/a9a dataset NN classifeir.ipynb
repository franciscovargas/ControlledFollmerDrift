{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 109.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /local/scratch/home/fav25/.local/lib/python3.8/site-packages (from scikit-learn) (1.7.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class OnedRegressionForwardNet(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = OnedRegressionForwardNet(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=5.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3751, 100.0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1451ca9072f44d68aaedd1039f53b8b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-132-eab220e940ad>:29: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7775377035140991\n",
      "0.6617360711097717\n",
      "0.5914770364761353\n",
      "0.5549614429473877\n",
      "0.5228423476219177\n",
      "0.5054814219474792\n",
      "0.49772799015045166\n",
      "0.4903304874897003\n",
      "0.4751688838005066\n",
      "0.47012147307395935\n",
      "0.4642406404018402\n",
      "0.4563952088356018\n",
      "0.4567808508872986\n",
      "0.4535273313522339\n",
      "0.4600086808204651\n",
      "0.45139989256858826\n",
      "0.4516918659210205\n",
      "0.4553230106830597\n",
      "0.44834116101264954\n",
      "0.4479217529296875\n",
      "0.4490167498588562\n",
      "0.44591209292411804\n",
      "0.44803789258003235\n",
      "0.45017486810684204\n",
      "0.44485414028167725\n",
      "0.447875440120697\n",
      "0.44428735971450806\n",
      "0.4421639144420624\n",
      "0.44146284461021423\n",
      "0.44344156980514526\n",
      "0.4362752139568329\n",
      "0.43735194206237793\n",
      "0.43900230526924133\n",
      "0.44028928875923157\n",
      "0.43556347489356995\n",
      "0.435276597738266\n",
      "0.4322590231895447\n",
      "0.4306997060775757\n",
      "0.43282967805862427\n",
      "0.4298889636993408\n",
      "0.43345215916633606\n",
      "0.43189066648483276\n",
      "0.4321964979171753\n",
      "0.43277594447135925\n",
      "0.43500393629074097\n",
      "0.43126657605171204\n",
      "0.43196678161621094\n",
      "0.43247321248054504\n",
      "0.43245506286621094\n",
      "0.4337146282196045\n",
      "0.43484392762184143\n",
      "0.4347977042198181\n",
      "0.43839240074157715\n",
      "0.43300431966781616\n",
      "0.43283262848854065\n",
      "0.43632927536964417\n",
      "0.43577733635902405\n",
      "0.43811503052711487\n",
      "0.437276154756546\n",
      "0.43967077136039734\n",
      "0.4365417957305908\n",
      "0.4423297047615051\n",
      "0.43811383843421936\n",
      "0.4370211958885193\n",
      "0.4396037459373474\n",
      "0.43745240569114685\n",
      "0.43953821063041687\n",
      "0.4361904561519623\n",
      "0.43698781728744507\n",
      "0.4356900453567505\n",
      "0.4365461766719818\n",
      "0.4369448721408844\n",
      "0.43931013345718384\n",
      "0.4413394033908844\n",
      "0.4415164291858673\n",
      "0.4427492022514343\n",
      "0.4375269412994385\n",
      "0.4402953088283539\n",
      "0.44001856446266174\n",
      "0.43668413162231445\n",
      "0.44183456897735596\n",
      "0.4391675293445587\n",
      "0.43278011679649353\n",
      "0.43861955404281616\n",
      "0.4404413402080536\n",
      "0.4385763108730316\n",
      "0.43789756298065186\n",
      "0.44134271144866943\n",
      "0.4438718557357788\n",
      "0.44115036725997925\n",
      "0.44206738471984863\n",
      "0.4430544078350067\n",
      "0.43796306848526\n",
      "0.44378748536109924\n",
      "0.44430437684059143\n",
      "0.4405969977378845\n",
      "0.4355525076389313\n",
      "0.4364893138408661\n",
      "0.44094258546829224\n",
      "0.44125986099243164\n",
      "0.443492591381073\n",
      "0.44681617617607117\n",
      "0.44462326169013977\n",
      "0.4428853988647461\n",
      "0.43756571412086487\n",
      "0.44127488136291504\n",
      "0.44194772839546204\n",
      "0.4386233687400818\n",
      "0.439015656709671\n",
      "0.43736717104911804\n",
      "0.43817251920700073\n",
      "0.44270631670951843\n",
      "0.44240379333496094\n",
      "0.4390431046485901\n",
      "0.43935540318489075\n",
      "0.4414798617362976\n",
      "0.43848681449890137\n",
      "0.4394218623638153\n",
      "0.44050899147987366\n",
      "0.4411693811416626\n",
      "0.4438149929046631\n",
      "0.4376688301563263\n",
      "0.4415353238582611\n",
      "0.44317278265953064\n",
      "0.4389542043209076\n",
      "0.44343653321266174\n",
      "0.4428945481777191\n",
      "0.4388834238052368\n",
      "0.4363088011741638\n",
      "0.441810667514801\n",
      "0.43823349475860596\n",
      "0.44083884358406067\n",
      "0.4407675266265869\n",
      "0.4383329153060913\n",
      "0.4421825706958771\n",
      "0.44904467463493347\n",
      "0.43704721331596375\n",
      "0.440417617559433\n",
      "0.4409789741039276\n",
      "0.4427230954170227\n",
      "0.44129952788352966\n",
      "0.43816107511520386\n",
      "0.43830499053001404\n",
      "0.4411719739437103\n",
      "0.441702276468277\n",
      "0.4394693076610565\n",
      "0.4408400356769562\n",
      "0.4412139058113098\n",
      "0.4406302869319916\n",
      "0.4383719265460968\n",
      "0.4423041343688965\n",
      "0.44061779975891113\n",
      "0.439911425113678\n",
      "0.44216886162757874\n",
      "0.4378124475479126\n",
      "0.43783947825431824\n",
      "0.442639023065567\n",
      "0.43790847063064575\n",
      "0.440532922744751\n",
      "0.43922582268714905\n",
      "0.4422409236431122\n",
      "0.44221583008766174\n",
      "0.43667086958885193\n",
      "0.4403049349784851\n",
      "0.43843597173690796\n",
      "0.44077742099761963\n",
      "0.4424222707748413\n",
      "0.43966203927993774\n",
      "0.4364170432090759\n",
      "0.43682095408439636\n",
      "0.4379805624485016\n",
      "0.4347565770149231\n",
      "0.43675822019577026\n",
      "0.438877671957016\n",
      "0.43898558616638184\n",
      "0.43673962354660034\n",
      "0.4421785771846771\n",
      "0.4388273358345032\n",
      "0.43880411982536316\n",
      "0.43499723076820374\n",
      "0.436430960893631\n",
      "0.4388161897659302\n",
      "0.4370901882648468\n",
      "0.434353232383728\n",
      "0.4352724254131317\n",
      "0.43522337079048157\n",
      "0.43698421120643616\n",
      "0.43347758054733276\n",
      "0.4348636567592621\n",
      "0.44151267409324646\n",
      "0.44388169050216675\n",
      "0.4314509928226471\n",
      "0.4304235875606537\n",
      "0.4361954629421234\n",
      "0.4359481930732727\n",
      "0.43536075949668884\n",
      "0.43303540349006653\n",
      "0.4373980164527893\n",
      "0.43087324500083923\n",
      "0.4332750737667084\n",
      "0.4332236349582672\n",
      "0.43105167150497437\n",
      "0.4348788857460022\n",
      "0.4347953200340271\n",
      "0.4364936351776123\n",
      "0.4421772062778473\n",
      "0.4384726583957672\n",
      "0.4386519193649292\n",
      "0.43832385540008545\n",
      "0.43624210357666016\n",
      "0.4335578680038452\n",
      "0.42887061834335327\n",
      "0.4310009777545929\n",
      "0.4346611797809601\n",
      "0.4303784966468811\n",
      "0.4280848801136017\n",
      "0.42548584938049316\n",
      "0.42482811212539673\n",
      "0.42705237865448\n",
      "0.42437148094177246\n",
      "0.4229872524738312\n",
      "0.4268750548362732\n",
      "0.4324260950088501\n",
      "0.42786896228790283\n",
      "0.42560330033302307\n",
      "0.4275955259799957\n",
      "0.43177875876426697\n",
      "0.4365687966346741\n",
      "0.4270809292793274\n",
      "0.42924442887306213\n",
      "0.4282780885696411\n",
      "0.42681390047073364\n",
      "0.42133137583732605\n",
      "0.4277150630950928\n",
      "0.4277667999267578\n",
      "0.4350421130657196\n",
      "0.43795570731163025\n",
      "0.4371687173843384\n",
      "0.4397631585597992\n",
      "0.4361194372177124\n",
      "0.4381650686264038\n",
      "0.4442303776741028\n",
      "0.4566074013710022\n",
      "0.4438460171222687\n",
      "0.4597529470920563\n",
      "0.44037705659866333\n",
      "0.43760183453559875\n",
      "0.4360359311103821\n",
      "0.43355292081832886\n",
      "0.4374987781047821\n",
      "0.43756103515625\n",
      "0.44480398297309875\n",
      "0.4377635419368744\n",
      "0.43458423018455505\n",
      "0.4358181655406952\n",
      "0.42741650342941284\n",
      "0.4373263716697693\n",
      "0.43112292885780334\n",
      "0.44343939423561096\n",
      "0.44002565741539\n",
      "0.43466612696647644\n",
      "0.4320003092288971\n",
      "0.4361489415168762\n",
      "0.43939444422721863\n",
      "0.44015294313430786\n",
      "0.4373253881931305\n",
      "0.4374442398548126\n",
      "0.43547382950782776\n",
      "0.4305644631385803\n",
      "0.4344225227832794\n",
      "0.43644028902053833\n",
      "0.43818485736846924\n",
      "0.43326425552368164\n",
      "0.4296233057975769\n",
      "0.43051159381866455\n",
      "0.43307754397392273\n",
      "0.42904528975486755\n",
      "0.4250909984111786\n",
      "0.4316664934158325\n",
      "0.4270974099636078\n",
      "0.4236762821674347\n",
      "0.4236828088760376\n",
      "0.42677316069602966\n",
      "0.4208444654941559\n",
      "0.4392906129360199\n",
      "0.4256395399570465\n",
      "0.4199092984199524\n",
      "0.428080677986145\n",
      "0.42691248655319214\n",
      "0.4289984703063965\n",
      "0.429490327835083\n",
      "0.42489132285118103\n",
      "0.41723117232322693\n",
      "0.41656652092933655\n",
      "0.4190366864204407\n",
      "0.42553213238716125\n",
      "0.4160151481628418\n",
      "0.41523343324661255\n",
      "0.418074369430542\n",
      "0.4178645610809326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.04\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, gaussian_prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_sharper_nik\", adjoint=False, optimizer=None,\n",
    "    num_steps=300, batch_size_data=int(X_train.shape[0]), batch_size_Θ=20,\n",
    "    batchnorm=True, device=device, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7775),\n",
       " tensor(0.6617),\n",
       " tensor(0.5915),\n",
       " tensor(0.5550),\n",
       " tensor(0.5228),\n",
       " tensor(0.5055),\n",
       " tensor(0.4977),\n",
       " tensor(0.4903),\n",
       " tensor(0.4752),\n",
       " tensor(0.4701),\n",
       " tensor(0.4642),\n",
       " tensor(0.4564),\n",
       " tensor(0.4568),\n",
       " tensor(0.4535),\n",
       " tensor(0.4600),\n",
       " tensor(0.4514),\n",
       " tensor(0.4517),\n",
       " tensor(0.4553),\n",
       " tensor(0.4483),\n",
       " tensor(0.4479),\n",
       " tensor(0.4490),\n",
       " tensor(0.4459),\n",
       " tensor(0.4480),\n",
       " tensor(0.4502),\n",
       " tensor(0.4449),\n",
       " tensor(0.4479),\n",
       " tensor(0.4443),\n",
       " tensor(0.4422),\n",
       " tensor(0.4415),\n",
       " tensor(0.4434),\n",
       " tensor(0.4363),\n",
       " tensor(0.4374),\n",
       " tensor(0.4390),\n",
       " tensor(0.4403),\n",
       " tensor(0.4356),\n",
       " tensor(0.4353),\n",
       " tensor(0.4323),\n",
       " tensor(0.4307),\n",
       " tensor(0.4328),\n",
       " tensor(0.4299),\n",
       " tensor(0.4335),\n",
       " tensor(0.4319),\n",
       " tensor(0.4322),\n",
       " tensor(0.4328),\n",
       " tensor(0.4350),\n",
       " tensor(0.4313),\n",
       " tensor(0.4320),\n",
       " tensor(0.4325),\n",
       " tensor(0.4325),\n",
       " tensor(0.4337),\n",
       " tensor(0.4348),\n",
       " tensor(0.4348),\n",
       " tensor(0.4384),\n",
       " tensor(0.4330),\n",
       " tensor(0.4328),\n",
       " tensor(0.4363),\n",
       " tensor(0.4358),\n",
       " tensor(0.4381),\n",
       " tensor(0.4373),\n",
       " tensor(0.4397),\n",
       " tensor(0.4365),\n",
       " tensor(0.4423),\n",
       " tensor(0.4381),\n",
       " tensor(0.4370),\n",
       " tensor(0.4396),\n",
       " tensor(0.4375),\n",
       " tensor(0.4395),\n",
       " tensor(0.4362),\n",
       " tensor(0.4370),\n",
       " tensor(0.4357),\n",
       " tensor(0.4365),\n",
       " tensor(0.4369),\n",
       " tensor(0.4393),\n",
       " tensor(0.4413),\n",
       " tensor(0.4415),\n",
       " tensor(0.4427),\n",
       " tensor(0.4375),\n",
       " tensor(0.4403),\n",
       " tensor(0.4400),\n",
       " tensor(0.4367),\n",
       " tensor(0.4418),\n",
       " tensor(0.4392),\n",
       " tensor(0.4328),\n",
       " tensor(0.4386),\n",
       " tensor(0.4404),\n",
       " tensor(0.4386),\n",
       " tensor(0.4379),\n",
       " tensor(0.4413),\n",
       " tensor(0.4439),\n",
       " tensor(0.4412),\n",
       " tensor(0.4421),\n",
       " tensor(0.4431),\n",
       " tensor(0.4380),\n",
       " tensor(0.4438),\n",
       " tensor(0.4443),\n",
       " tensor(0.4406),\n",
       " tensor(0.4356),\n",
       " tensor(0.4365),\n",
       " tensor(0.4409),\n",
       " tensor(0.4413),\n",
       " tensor(0.4435),\n",
       " tensor(0.4468),\n",
       " tensor(0.4446),\n",
       " tensor(0.4429),\n",
       " tensor(0.4376),\n",
       " tensor(0.4413),\n",
       " tensor(0.4419),\n",
       " tensor(0.4386),\n",
       " tensor(0.4390),\n",
       " tensor(0.4374),\n",
       " tensor(0.4382),\n",
       " tensor(0.4427),\n",
       " tensor(0.4424),\n",
       " tensor(0.4390),\n",
       " tensor(0.4394),\n",
       " tensor(0.4415),\n",
       " tensor(0.4385),\n",
       " tensor(0.4394),\n",
       " tensor(0.4405),\n",
       " tensor(0.4412),\n",
       " tensor(0.4438),\n",
       " tensor(0.4377),\n",
       " tensor(0.4415),\n",
       " tensor(0.4432),\n",
       " tensor(0.4390),\n",
       " tensor(0.4434),\n",
       " tensor(0.4429),\n",
       " tensor(0.4389),\n",
       " tensor(0.4363),\n",
       " tensor(0.4418),\n",
       " tensor(0.4382),\n",
       " tensor(0.4408),\n",
       " tensor(0.4408),\n",
       " tensor(0.4383),\n",
       " tensor(0.4422),\n",
       " tensor(0.4490),\n",
       " tensor(0.4370),\n",
       " tensor(0.4404),\n",
       " tensor(0.4410),\n",
       " tensor(0.4427),\n",
       " tensor(0.4413),\n",
       " tensor(0.4382),\n",
       " tensor(0.4383),\n",
       " tensor(0.4412),\n",
       " tensor(0.4417),\n",
       " tensor(0.4395),\n",
       " tensor(0.4408),\n",
       " tensor(0.4412),\n",
       " tensor(0.4406),\n",
       " tensor(0.4384),\n",
       " tensor(0.4423),\n",
       " tensor(0.4406),\n",
       " tensor(0.4399),\n",
       " tensor(0.4422),\n",
       " tensor(0.4378),\n",
       " tensor(0.4378),\n",
       " tensor(0.4426),\n",
       " tensor(0.4379),\n",
       " tensor(0.4405),\n",
       " tensor(0.4392),\n",
       " tensor(0.4422),\n",
       " tensor(0.4422),\n",
       " tensor(0.4367),\n",
       " tensor(0.4403),\n",
       " tensor(0.4384),\n",
       " tensor(0.4408),\n",
       " tensor(0.4424),\n",
       " tensor(0.4397),\n",
       " tensor(0.4364),\n",
       " tensor(0.4368),\n",
       " tensor(0.4380),\n",
       " tensor(0.4348),\n",
       " tensor(0.4368),\n",
       " tensor(0.4389),\n",
       " tensor(0.4390),\n",
       " tensor(0.4367),\n",
       " tensor(0.4422),\n",
       " tensor(0.4388),\n",
       " tensor(0.4388),\n",
       " tensor(0.4350),\n",
       " tensor(0.4364),\n",
       " tensor(0.4388),\n",
       " tensor(0.4371),\n",
       " tensor(0.4344),\n",
       " tensor(0.4353),\n",
       " tensor(0.4352),\n",
       " tensor(0.4370),\n",
       " tensor(0.4335),\n",
       " tensor(0.4349),\n",
       " tensor(0.4415),\n",
       " tensor(0.4439),\n",
       " tensor(0.4315),\n",
       " tensor(0.4304),\n",
       " tensor(0.4362),\n",
       " tensor(0.4359),\n",
       " tensor(0.4354),\n",
       " tensor(0.4330),\n",
       " tensor(0.4374),\n",
       " tensor(0.4309),\n",
       " tensor(0.4333),\n",
       " tensor(0.4332),\n",
       " tensor(0.4311),\n",
       " tensor(0.4349),\n",
       " tensor(0.4348),\n",
       " tensor(0.4365),\n",
       " tensor(0.4422),\n",
       " tensor(0.4385),\n",
       " tensor(0.4387),\n",
       " tensor(0.4383),\n",
       " tensor(0.4362),\n",
       " tensor(0.4336),\n",
       " tensor(0.4289),\n",
       " tensor(0.4310),\n",
       " tensor(0.4347),\n",
       " tensor(0.4304),\n",
       " tensor(0.4281),\n",
       " tensor(0.4255),\n",
       " tensor(0.4248),\n",
       " tensor(0.4271),\n",
       " tensor(0.4244),\n",
       " tensor(0.4230),\n",
       " tensor(0.4269),\n",
       " tensor(0.4324),\n",
       " tensor(0.4279),\n",
       " tensor(0.4256),\n",
       " tensor(0.4276),\n",
       " tensor(0.4318),\n",
       " tensor(0.4366),\n",
       " tensor(0.4271),\n",
       " tensor(0.4292),\n",
       " tensor(0.4283),\n",
       " tensor(0.4268),\n",
       " tensor(0.4213),\n",
       " tensor(0.4277),\n",
       " tensor(0.4278),\n",
       " tensor(0.4350),\n",
       " tensor(0.4380),\n",
       " tensor(0.4372),\n",
       " tensor(0.4398),\n",
       " tensor(0.4361),\n",
       " tensor(0.4382),\n",
       " tensor(0.4442),\n",
       " tensor(0.4566),\n",
       " tensor(0.4438),\n",
       " tensor(0.4598),\n",
       " tensor(0.4404),\n",
       " tensor(0.4376),\n",
       " tensor(0.4360),\n",
       " tensor(0.4336),\n",
       " tensor(0.4375),\n",
       " tensor(0.4376),\n",
       " tensor(0.4448),\n",
       " tensor(0.4378),\n",
       " tensor(0.4346),\n",
       " tensor(0.4358),\n",
       " tensor(0.4274),\n",
       " tensor(0.4373),\n",
       " tensor(0.4311),\n",
       " tensor(0.4434),\n",
       " tensor(0.4400),\n",
       " tensor(0.4347),\n",
       " tensor(0.4320),\n",
       " tensor(0.4361),\n",
       " tensor(0.4394),\n",
       " tensor(0.4402),\n",
       " tensor(0.4373),\n",
       " tensor(0.4374),\n",
       " tensor(0.4355),\n",
       " tensor(0.4306),\n",
       " tensor(0.4344),\n",
       " tensor(0.4364),\n",
       " tensor(0.4382),\n",
       " tensor(0.4333),\n",
       " tensor(0.4296),\n",
       " tensor(0.4305),\n",
       " tensor(0.4331),\n",
       " tensor(0.4290),\n",
       " tensor(0.4251),\n",
       " tensor(0.4317),\n",
       " tensor(0.4271),\n",
       " tensor(0.4237),\n",
       " tensor(0.4237),\n",
       " tensor(0.4268),\n",
       " tensor(0.4208),\n",
       " tensor(0.4393),\n",
       " tensor(0.4256),\n",
       " tensor(0.4199),\n",
       " tensor(0.4281),\n",
       " tensor(0.4269),\n",
       " tensor(0.4290),\n",
       " tensor(0.4295),\n",
       " tensor(0.4249),\n",
       " tensor(0.4172),\n",
       " tensor(0.4166),\n",
       " tensor(0.4190),\n",
       " tensor(0.4255),\n",
       " tensor(0.4160),\n",
       " tensor(0.4152),\n",
       " tensor(0.4181),\n",
       " tensor(0.4179)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5841a19550>]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  8., 20., 20., 18., 15.,  7.,  3.,  3.,  1.]),\n",
       " array([-0.10870295, -0.08848834, -0.06827373, -0.04805912, -0.02784451,\n",
       "        -0.00762989,  0.01258472,  0.03279933,  0.05301394,  0.07322855,\n",
       "         0.09344316], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8107, device='cuda:0')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8137, device='cuda:0')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
