{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer\n",
    "from cfollmer.sampler_utils import ResNetScoreNetwork, AbstractDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# mod = sklearn.neural_network.MLPClassifier((100,100),random_state=0).fit(X_train, y_train)\n",
    "# print(mod.score(X_train, y_train))\n",
    "# print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x, t):\n",
    "#         x = torch.cat((x, t), dim=-1)\n",
    "#         return self.nn(x)\n",
    "\n",
    "\n",
    "# class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "#     def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "#         super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "#         self.nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + 1, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, input_dim )\n",
    "#         )\n",
    "        \n",
    "#         self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "##     def forward(self, x):\n",
    "##         return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=2, width=100, activation=F.softplus\n",
    ")\n",
    "\n",
    "\n",
    "# net = LinearClassifier(\n",
    "#     dim,1, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22601, 123)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim,  X_train.shape[1] #, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.layers import ResBlock, get_timestep_embedding\n",
    "\n",
    "class ResNetScoreNetwork_(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int = 123,\n",
    "                 pos_dim: int = 16,\n",
    "                 res_dim: int = 650,\n",
    "                 res_block_initial_widths=None,\n",
    "                 res_block_final_widths=None,\n",
    "                 res_block_inner_layers=None,\n",
    "                 activation=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        if res_block_initial_widths is None:\n",
    "            res_block_initial_widths = [res_dim, res_dim, res_dim,res_dim]\n",
    "        if res_block_final_widths is None:\n",
    "            res_block_final_widths = [res_dim, res_dim, res_dim, res_dim]\n",
    "        if res_block_inner_layers is None:\n",
    "            res_block_inner_layers = [128, 128]\n",
    "\n",
    "        self.temb_dim = pos_dim\n",
    "\n",
    "        # ResBlock Sequence\n",
    "        res_layers = []\n",
    "        initial_dim = input_dim\n",
    "        for initial, final in zip(res_block_initial_widths, res_block_final_widths):\n",
    "            res_layers.append(ResBlock(initial_dim, initial, final, res_block_inner_layers, activation))\n",
    "            initial_dim = initial + final\n",
    "        self.res_sequence = torch.nn.Sequential(*res_layers)\n",
    "\n",
    "        # Time FCBlock\n",
    "        self.time_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim, self.temb_dim * 2), activation)\n",
    "\n",
    "        # Final_block\n",
    "        self.final_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim * 2 + initial_dim, input_dim))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t needs the same shape as x (except for the final dim, which is 1)\n",
    "        t_emb = get_timestep_embedding(t, self.temb_dim)\n",
    "        t_emb = self.time_block(t_emb)\n",
    "        x_emb = self.res_sequence(x)\n",
    "        h = torch.cat([x_emb, t_emb], -1)\n",
    "        return self.final_block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetScoreNetwork(\n",
       "  (res_sequence): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_block): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (1): SiLU()\n",
       "  )\n",
       "  (final_block): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ResNetScoreNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5c7fd688d7483cb293c0138d1e1919"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:143: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f = _vmap_internals.vmap(f_)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:144: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f_detached = _vmap_internals.vmap(sde.f_detached)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:152: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  g = _vmap_internals.vmap(sde.g)\n",
      "<ipython-input-16-f8dc0d413ba5>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.508732795715332\n",
      "18.55075454711914\n",
      "1.6584545373916626\n",
      "0.8690106272697449\n",
      "1.0092272758483887\n",
      "1.1212632656097412\n",
      "1.1372182369232178\n",
      "0.8284947276115417\n",
      "0.6076087355613708\n",
      "0.5051792860031128\n",
      "0.4624505043029785\n",
      "0.4820045828819275\n",
      "0.41415882110595703\n",
      "0.3824295103549957\n",
      "0.362121045589447\n",
      "0.34983381628990173\n",
      "0.32733139395713806\n",
      "0.34101602435112\n",
      "0.314826101064682\n",
      "0.30960533022880554\n",
      "0.31268662214279175\n",
      "0.2955743074417114\n",
      "0.34417030215263367\n",
      "0.31188952922821045\n",
      "0.30961480736732483\n",
      "0.31690701842308044\n",
      "0.31611883640289307\n",
      "0.31219249963760376\n",
      "0.3096579909324646\n",
      "0.31544679403305054\n",
      "0.33479300141334534\n",
      "0.3364640176296234\n",
      "0.31962764263153076\n",
      "0.3253071904182434\n",
      "0.32002249360084534\n",
      "0.31961536407470703\n",
      "0.33313941955566406\n",
      "0.3304208517074585\n",
      "0.34137576818466187\n",
      "0.3341197669506073\n",
      "0.3418159484863281\n",
      "0.3462827801704407\n",
      "0.3647210896015167\n",
      "0.3331320583820343\n",
      "0.3339003622531891\n",
      "0.3306185305118561\n",
      "0.322042316198349\n",
      "0.33549395203590393\n",
      "0.3302409052848816\n",
      "0.34631145000457764\n",
      "0.33015576004981995\n",
      "0.3330751359462738\n",
      "0.3383212983608246\n",
      "0.33709388971328735\n",
      "0.33392876386642456\n",
      "0.3586220443248749\n",
      "0.3332780599594116\n",
      "0.32002466917037964\n",
      "0.3456052243709564\n",
      "0.3295571208000183\n",
      "0.3254571855068207\n",
      "0.3356708288192749\n",
      "0.3189588487148285\n",
      "0.3402811586856842\n",
      "0.330298513174057\n",
      "0.3119589388370514\n",
      "0.3268645107746124\n",
      "0.3251493573188782\n",
      "0.31790339946746826\n",
      "0.3194085359573364\n",
      "0.33504581451416016\n",
      "0.3277229368686676\n",
      "0.3327717185020447\n",
      "0.3225914537906647\n",
      "0.3305392563343048\n",
      "0.33538636565208435\n",
      "0.34084948897361755\n",
      "0.3323386311531067\n",
      "0.34384778141975403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v\n",
    "\n",
    "\n",
    "class SimpleForwardNetBN_larger(AbstractDrift):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=2500, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.nn(x)\n",
    "\n",
    "\n",
    "\n",
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"linear\",\n",
    "    γ_min=0.2**2, γ_max=0.5**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.5087),\n",
       " tensor(18.5508),\n",
       " tensor(1.6585),\n",
       " tensor(0.8690),\n",
       " tensor(1.0092),\n",
       " tensor(1.1213),\n",
       " tensor(1.1372),\n",
       " tensor(0.8285),\n",
       " tensor(0.6076),\n",
       " tensor(0.5052),\n",
       " tensor(0.4625),\n",
       " tensor(0.4820),\n",
       " tensor(0.4142),\n",
       " tensor(0.3824),\n",
       " tensor(0.3621),\n",
       " tensor(0.3498),\n",
       " tensor(0.3273),\n",
       " tensor(0.3410),\n",
       " tensor(0.3148),\n",
       " tensor(0.3096),\n",
       " tensor(0.3127),\n",
       " tensor(0.2956),\n",
       " tensor(0.3442),\n",
       " tensor(0.3119),\n",
       " tensor(0.3096),\n",
       " tensor(0.3169),\n",
       " tensor(0.3161),\n",
       " tensor(0.3122),\n",
       " tensor(0.3097),\n",
       " tensor(0.3154),\n",
       " tensor(0.3348),\n",
       " tensor(0.3365),\n",
       " tensor(0.3196),\n",
       " tensor(0.3253),\n",
       " tensor(0.3200),\n",
       " tensor(0.3196),\n",
       " tensor(0.3331),\n",
       " tensor(0.3304),\n",
       " tensor(0.3414),\n",
       " tensor(0.3341),\n",
       " tensor(0.3418),\n",
       " tensor(0.3463),\n",
       " tensor(0.3647),\n",
       " tensor(0.3331),\n",
       " tensor(0.3339),\n",
       " tensor(0.3306),\n",
       " tensor(0.3220),\n",
       " tensor(0.3355),\n",
       " tensor(0.3302),\n",
       " tensor(0.3463),\n",
       " tensor(0.3302),\n",
       " tensor(0.3331),\n",
       " tensor(0.3383),\n",
       " tensor(0.3371),\n",
       " tensor(0.3339),\n",
       " tensor(0.3586),\n",
       " tensor(0.3333),\n",
       " tensor(0.3200),\n",
       " tensor(0.3456),\n",
       " tensor(0.3296),\n",
       " tensor(0.3255),\n",
       " tensor(0.3357),\n",
       " tensor(0.3190),\n",
       " tensor(0.3403),\n",
       " tensor(0.3303),\n",
       " tensor(0.3120),\n",
       " tensor(0.3269),\n",
       " tensor(0.3251),\n",
       " tensor(0.3179),\n",
       " tensor(0.3194),\n",
       " tensor(0.3350),\n",
       " tensor(0.3277),\n",
       " tensor(0.3328),\n",
       " tensor(0.3226),\n",
       " tensor(0.3305),\n",
       " tensor(0.3354),\n",
       " tensor(0.3408),\n",
       " tensor(0.3323),\n",
       " tensor(0.3438)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd3388c040>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  6., 11., 19., 14., 20., 13.,  7.,  8.,  1.]),\n",
       " array([-1.4535437 , -1.2866066 , -1.1196696 , -0.9527325 , -0.78579545,\n",
       "        -0.61885834, -0.4519213 , -0.28498426, -0.11804719,  0.04888987,\n",
       "         0.21582693], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8426, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8442, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
