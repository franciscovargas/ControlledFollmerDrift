{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=250, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.tanh\n",
    ")\n",
    "\n",
    "\n",
    "net = LinearClassifier(\n",
    "    dim,1, device=device\n",
    ")\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 100.0)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a23845c5604a70b40a9b56f555f6d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-460-6511ec892fb4>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7533348202705383\n",
      "1.3288285732269287\n",
      "1.0855001211166382\n",
      "0.5205703377723694\n",
      "0.4099160134792328\n",
      "0.6881771087646484\n",
      "0.7859485745429993\n",
      "0.5115951299667358\n",
      "0.41827741265296936\n",
      "0.46622371673583984\n",
      "0.5500982999801636\n",
      "0.6066301465034485\n",
      "0.5604962110519409\n",
      "0.43330395221710205\n",
      "0.39532479643821716\n",
      "0.42984074354171753\n",
      "0.4664943814277649\n",
      "0.47527876496315\n",
      "0.41772395372390747\n",
      "0.37468576431274414\n",
      "0.40581822395324707\n",
      "0.4311576783657074\n",
      "0.45866620540618896\n",
      "0.3916395902633667\n",
      "0.3658023476600647\n",
      "0.3970146179199219\n",
      "0.43644651770591736\n",
      "0.3916533291339874\n",
      "0.3597484529018402\n",
      "0.4072249233722687\n",
      "0.42455360293388367\n",
      "0.3885144889354706\n",
      "0.37544044852256775\n",
      "0.37342578172683716\n",
      "0.3947857916355133\n",
      "0.39697471261024475\n",
      "0.36064448952674866\n",
      "0.3717248737812042\n",
      "0.37650036811828613\n",
      "0.37192386388778687\n",
      "0.3656264543533325\n",
      "0.352934330701828\n",
      "0.3553847372531891\n",
      "0.36172977089881897\n",
      "0.355183482170105\n",
      "0.36002546548843384\n",
      "0.35896193981170654\n",
      "0.35662615299224854\n",
      "0.3577728569507599\n",
      "0.351711243391037\n",
      "0.3560577630996704\n",
      "0.355038046836853\n",
      "0.3603132665157318\n",
      "0.3573964834213257\n",
      "0.35980647802352905\n",
      "0.3629550039768219\n",
      "0.35137954354286194\n",
      "0.35690397024154663\n",
      "0.36388176679611206\n",
      "0.357084721326828\n",
      "0.3539896309375763\n",
      "0.35548678040504456\n",
      "0.3638109862804413\n",
      "0.35515156388282776\n",
      "0.3513162434101105\n",
      "0.35678496956825256\n",
      "0.35207322239875793\n",
      "0.3574218153953552\n",
      "0.3532070815563202\n",
      "0.3549209237098694\n",
      "0.35265159606933594\n",
      "0.35791414976119995\n",
      "0.353384405374527\n",
      "0.35255783796310425\n",
      "0.35248860716819763\n",
      "0.3496672213077545\n",
      "0.36115244030952454\n",
      "0.35056939721107483\n",
      "0.3538883328437805\n",
      "0.35382211208343506\n",
      "0.3531385064125061\n",
      "0.35847729444503784\n",
      "0.35515129566192627\n",
      "0.35119009017944336\n",
      "0.3491741716861725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=85, batch_size_data=int(X_train.shape[0]), batch_size_Θ=20,\n",
    "    batchnorm=True, device=device, lr=0.002, drift=SimpleForwardNetBN_larger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7533),\n",
       " tensor(1.3288),\n",
       " tensor(1.0855),\n",
       " tensor(0.5206),\n",
       " tensor(0.4099),\n",
       " tensor(0.6882),\n",
       " tensor(0.7859),\n",
       " tensor(0.5116),\n",
       " tensor(0.4183),\n",
       " tensor(0.4662),\n",
       " tensor(0.5501),\n",
       " tensor(0.6066),\n",
       " tensor(0.5605),\n",
       " tensor(0.4333),\n",
       " tensor(0.3953),\n",
       " tensor(0.4298),\n",
       " tensor(0.4665),\n",
       " tensor(0.4753),\n",
       " tensor(0.4177),\n",
       " tensor(0.3747),\n",
       " tensor(0.4058),\n",
       " tensor(0.4312),\n",
       " tensor(0.4587),\n",
       " tensor(0.3916),\n",
       " tensor(0.3658),\n",
       " tensor(0.3970),\n",
       " tensor(0.4364),\n",
       " tensor(0.3917),\n",
       " tensor(0.3597),\n",
       " tensor(0.4072),\n",
       " tensor(0.4246),\n",
       " tensor(0.3885),\n",
       " tensor(0.3754),\n",
       " tensor(0.3734),\n",
       " tensor(0.3948),\n",
       " tensor(0.3970),\n",
       " tensor(0.3606),\n",
       " tensor(0.3717),\n",
       " tensor(0.3765),\n",
       " tensor(0.3719),\n",
       " tensor(0.3656),\n",
       " tensor(0.3529),\n",
       " tensor(0.3554),\n",
       " tensor(0.3617),\n",
       " tensor(0.3552),\n",
       " tensor(0.3600),\n",
       " tensor(0.3590),\n",
       " tensor(0.3566),\n",
       " tensor(0.3578),\n",
       " tensor(0.3517),\n",
       " tensor(0.3561),\n",
       " tensor(0.3550),\n",
       " tensor(0.3603),\n",
       " tensor(0.3574),\n",
       " tensor(0.3598),\n",
       " tensor(0.3630),\n",
       " tensor(0.3514),\n",
       " tensor(0.3569),\n",
       " tensor(0.3639),\n",
       " tensor(0.3571),\n",
       " tensor(0.3540),\n",
       " tensor(0.3555),\n",
       " tensor(0.3638),\n",
       " tensor(0.3552),\n",
       " tensor(0.3513),\n",
       " tensor(0.3568),\n",
       " tensor(0.3521),\n",
       " tensor(0.3574),\n",
       " tensor(0.3532),\n",
       " tensor(0.3549),\n",
       " tensor(0.3527),\n",
       " tensor(0.3579),\n",
       " tensor(0.3534),\n",
       " tensor(0.3526),\n",
       " tensor(0.3525),\n",
       " tensor(0.3497),\n",
       " tensor(0.3612),\n",
       " tensor(0.3506),\n",
       " tensor(0.3539),\n",
       " tensor(0.3538),\n",
       " tensor(0.3531),\n",
       " tensor(0.3585),\n",
       " tensor(0.3552),\n",
       " tensor(0.3512),\n",
       " tensor(0.3492)]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f58e6fa5ee0>]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4., 10., 17., 16., 23., 13., 11.,  1.,  2.,  3.]),\n",
       " array([-0.24733813, -0.15700565, -0.06667317,  0.02365931,  0.11399179,\n",
       "         0.20432428,  0.29465675,  0.38498923,  0.4753217 ,  0.56565416,\n",
       "         0.65598667], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8454, device='cuda:0')"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8501, device='cuda:0')"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
