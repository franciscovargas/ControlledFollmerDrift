{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer\n",
    "from cfollmer.sampler_utils import ResNetScoreNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# mod = sklearn.neural_network.MLPClassifier((100,100),random_state=0).fit(X_train, y_train)\n",
    "# print(mod.score(X_train, y_train))\n",
    "# print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "# class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "#         super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "#         self.nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + 1, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, width), activation(),\n",
    "#             torch.nn.Linear(width, input_dim )\n",
    "#         )\n",
    "        \n",
    "#         self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=2, width=100, activation=F.softplus\n",
    ")\n",
    "\n",
    "\n",
    "# net = LinearClassifier(\n",
    "#     dim,1, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22601, 123)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim,  X_train.shape[1] #, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.layers import ResBlock, get_timestep_embedding\n",
    "\n",
    "class ResNetScoreNetwork_(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int = 123,\n",
    "                 pos_dim: int = 16,\n",
    "                 res_dim: int = 650,\n",
    "                 res_block_initial_widths=None,\n",
    "                 res_block_final_widths=None,\n",
    "                 res_block_inner_layers=None,\n",
    "                 activation=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        if res_block_initial_widths is None:\n",
    "            res_block_initial_widths = [res_dim, res_dim, res_dim,res_dim]\n",
    "        if res_block_final_widths is None:\n",
    "            res_block_final_widths = [res_dim, res_dim, res_dim, res_dim]\n",
    "        if res_block_inner_layers is None:\n",
    "            res_block_inner_layers = [128, 128]\n",
    "\n",
    "        self.temb_dim = pos_dim\n",
    "\n",
    "        # ResBlock Sequence\n",
    "        res_layers = []\n",
    "        initial_dim = input_dim\n",
    "        for initial, final in zip(res_block_initial_widths, res_block_final_widths):\n",
    "            res_layers.append(ResBlock(initial_dim, initial, final, res_block_inner_layers, activation))\n",
    "            initial_dim = initial + final\n",
    "        self.res_sequence = torch.nn.Sequential(*res_layers)\n",
    "\n",
    "        # Time FCBlock\n",
    "        self.time_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim, self.temb_dim * 2), activation)\n",
    "\n",
    "        # Final_block\n",
    "        self.final_block = torch.nn.Sequential(torch.nn.Linear(self.temb_dim * 2 + initial_dim, input_dim))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t needs the same shape as x (except for the final dim, which is 1)\n",
    "        t_emb = get_timestep_embedding(t, self.temb_dim)\n",
    "        t_emb = self.time_block(t_emb)\n",
    "        x_emb = self.res_sequence(x)\n",
    "        h = torch.cat([x_emb, t_emb], -1)\n",
    "        return self.final_block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetScoreNetwork(\n",
       "  (res_sequence): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (first_layer): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): SiLU()\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_block): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (1): SiLU()\n",
       "  )\n",
       "  (final_block): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ResNetScoreNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489d939a21de45189f93811018a0be7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:141: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f = _vmap_internals.vmap(f_)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:142: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  f_detached = _vmap_internals.vmap(sde.f_detached)\n",
      "/local/scratch/home/fav25/ControlledFollmerDrift/cfollmer/objectives.py:150: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  g = _vmap_internals.vmap(sde.g)\n",
      "<ipython-input-11-f8dc0d413ba5>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.757448196411133\n",
      "3.9710893630981445\n",
      "2.4238905906677246\n",
      "2.8419854640960693\n",
      "2.06982421875\n",
      "2.179858446121216\n",
      "1.8367462158203125\n",
      "1.7762255668640137\n",
      "1.3274050951004028\n",
      "1.241607666015625\n",
      "1.0098347663879395\n",
      "0.9237085580825806\n",
      "0.8387624025344849\n",
      "0.8155657649040222\n",
      "0.8261422514915466\n",
      "0.7190550565719604\n",
      "0.7705386877059937\n",
      "0.6436424851417542\n",
      "0.6812188625335693\n",
      "0.6998549699783325\n",
      "0.6992924809455872\n",
      "0.7290688157081604\n",
      "0.6403883099555969\n",
      "0.6190033555030823\n",
      "0.6376720666885376\n",
      "0.6291783452033997\n",
      "0.616996169090271\n",
      "0.6278215050697327\n",
      "0.5909305810928345\n",
      "0.5886972546577454\n",
      "0.5723456740379333\n",
      "0.6202760338783264\n",
      "0.5874497890472412\n",
      "0.6252084970474243\n",
      "0.576433539390564\n",
      "0.6057628989219666\n",
      "0.5830933451652527\n",
      "0.6228150725364685\n",
      "0.6036190390586853\n",
      "0.5550096035003662\n",
      "0.6289268136024475\n",
      "0.5941263437271118\n",
      "0.6023337244987488\n",
      "0.5645768642425537\n",
      "0.6439412832260132\n",
      "0.6021217107772827\n",
      "0.6551634073257446\n",
      "0.6371065974235535\n",
      "0.6532573103904724\n",
      "0.6251009106636047\n",
      "0.5924004316329956\n",
      "0.6004409193992615\n",
      "0.6184086203575134\n",
      "0.5997728109359741\n",
      "0.6210125088691711\n",
      "0.6208422780036926\n",
      "0.613774299621582\n",
      "0.6569475531578064\n",
      "0.7424929141998291\n",
      "0.6556393504142761\n",
      "0.6857609748840332\n",
      "0.6975797414779663\n",
      "0.6822542548179626\n",
      "0.6875731348991394\n",
      "0.6066086292266846\n",
      "0.6366488337516785\n",
      "0.6752079725265503\n",
      "0.6772543787956238\n",
      "0.6191598773002625\n",
      "0.7126191854476929\n",
      "0.5954809784889221\n",
      "0.6257772445678711\n",
      "0.6246156096458435\n",
      "0.6226410865783691\n",
      "0.6438247561454773\n",
      "0.5699453949928284\n",
      "0.6502740979194641\n",
      "0.6155497431755066\n",
      "0.6424781680107117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=2500, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "\n",
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.0001, drift=SimpleForwardNetBN_larger, schedule=\"linear\",\n",
    "    γ_min=0.2**2, γ_max=0.5**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4.7574),\n",
       " tensor(3.9711),\n",
       " tensor(2.4239),\n",
       " tensor(2.8420),\n",
       " tensor(2.0698),\n",
       " tensor(2.1799),\n",
       " tensor(1.8367),\n",
       " tensor(1.7762),\n",
       " tensor(1.3274),\n",
       " tensor(1.2416),\n",
       " tensor(1.0098),\n",
       " tensor(0.9237),\n",
       " tensor(0.8388),\n",
       " tensor(0.8156),\n",
       " tensor(0.8261),\n",
       " tensor(0.7191),\n",
       " tensor(0.7705),\n",
       " tensor(0.6436),\n",
       " tensor(0.6812),\n",
       " tensor(0.6999),\n",
       " tensor(0.6993),\n",
       " tensor(0.7291),\n",
       " tensor(0.6404),\n",
       " tensor(0.6190),\n",
       " tensor(0.6377),\n",
       " tensor(0.6292),\n",
       " tensor(0.6170),\n",
       " tensor(0.6278),\n",
       " tensor(0.5909),\n",
       " tensor(0.5887),\n",
       " tensor(0.5723),\n",
       " tensor(0.6203),\n",
       " tensor(0.5874),\n",
       " tensor(0.6252),\n",
       " tensor(0.5764),\n",
       " tensor(0.6058),\n",
       " tensor(0.5831),\n",
       " tensor(0.6228),\n",
       " tensor(0.6036),\n",
       " tensor(0.5550),\n",
       " tensor(0.6289),\n",
       " tensor(0.5941),\n",
       " tensor(0.6023),\n",
       " tensor(0.5646),\n",
       " tensor(0.6439),\n",
       " tensor(0.6021),\n",
       " tensor(0.6552),\n",
       " tensor(0.6371),\n",
       " tensor(0.6533),\n",
       " tensor(0.6251),\n",
       " tensor(0.5924),\n",
       " tensor(0.6004),\n",
       " tensor(0.6184),\n",
       " tensor(0.5998),\n",
       " tensor(0.6210),\n",
       " tensor(0.6208),\n",
       " tensor(0.6138),\n",
       " tensor(0.6569),\n",
       " tensor(0.7425),\n",
       " tensor(0.6556),\n",
       " tensor(0.6858),\n",
       " tensor(0.6976),\n",
       " tensor(0.6823),\n",
       " tensor(0.6876),\n",
       " tensor(0.6066),\n",
       " tensor(0.6366),\n",
       " tensor(0.6752),\n",
       " tensor(0.6773),\n",
       " tensor(0.6192),\n",
       " tensor(0.7126),\n",
       " tensor(0.5955),\n",
       " tensor(0.6258),\n",
       " tensor(0.6246),\n",
       " tensor(0.6226),\n",
       " tensor(0.6438),\n",
       " tensor(0.5699),\n",
       " tensor(0.6503),\n",
       " tensor(0.6155),\n",
       " tensor(0.6425)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc01385cd30>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  3., 11., 12., 23., 18., 16., 10.,  4.,  1.]),\n",
       " array([-0.94686   , -0.7584331 , -0.5700062 , -0.38157925, -0.19315232,\n",
       "        -0.0047254 ,  0.18370153,  0.37212846,  0.5605554 ,  0.7489823 ,\n",
       "         0.9374092 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7592, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7638, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
