{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsde\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import _vmap_internals\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfollmer.objectives import log_g, relative_entropy_control_cost, stl_relative_entropy_control_cost_xu\n",
    "from cfollmer.sampler_utils import FollmerSDE\n",
    "from cfollmer.trainers import basic_batched_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathcal{N}(\\theta | 0, \\sigma_w^2 \\mathbb{I}) \\\\\n",
    "y_i | x_i, \\theta &\\sim  \\mathrm{Bernouli}\\left[\\mathrm{NN}_{\\theta}\\left(x_i \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "We want samples from $p(\\theta | \\{(y_i, x_i)\\})$. Note $f(x; \\theta)$ is a neural net with params $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9a_train = pd.read_csv(\"../data/a9a.csv\", header=None)\n",
    "a9a_test = pd.read_csv(\"../data/a9a_t.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490832591136636\n",
      "0.8497635280388183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fav25/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = a9a_train.values[:,:-1],  a9a_train.values[:,-1]\n",
    "X_test, y_test = a9a_test.values[:,:-1],  a9a_test.values[:,-1]\n",
    "\n",
    "# X_train = np.concatenate((X_train, np.ones((X_train.shape[0],X_train.shape[1]))), axis=1)\n",
    "# X_test = np.concatenate((X_test, np.ones((X_test.shape[0],X_train.shape[1]))), axis=1)\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = sklearn.linear_model.LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(mod.score(X_train, y_train))\n",
    "print(mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    torch.tensor(X_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(X_test, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_train, dtype=torch.float32, device=device), \\\n",
    "    torch.tensor(y_test, dtype=torch.float32, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmin}{arg\\,min}$$\n",
    "$$\\def\\E{{\\mathbb{E}}}$$\n",
    "$$\\def\\rvu{{\\mathbf{u}}}$$\n",
    "$$\\def\\rvTheta{{\\bm{\\Theta}}}$$\n",
    "$$\\def\\gU{{\\mathcal{U}}}$$\n",
    "$$\\def\\mX{{\\mathbf{X}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled Schrodinger Follmer Sampler\n",
    "\n",
    "The objevtive we are trying to implement is:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{u}_t^{*}=  \\argmin_{\\rvu_t \\in \\mathcal{U}}\\mathbb{E}\\left[\\frac{1}{2\\gamma}\\int_0^1||\\rvu(t, \\Theta_t)||^2 dt - \\ln\\left(\\frac{ p(\\mX | \\Theta_1)p(\\Theta_1)}{\\mathcal{N}(\\Theta_1|\\mathbf{0}, \\gamma \\mathbb{I} )}\\right)\\right] \\\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\\begin{align}\n",
    "d\\Theta_t = \\rvu(t, \\Theta_t)dt + \\sqrt{\\gamma} dB_t\n",
    "\\end{align}\n",
    "\n",
    "To do so we use the EM discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassificationNetwork(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, depth=None,\n",
    "        width=20, width_seq=None, device=\"cpu\", activation=F.relu\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.depth = depth\n",
    "        if not self.depth:\n",
    "            self.depth = 1\n",
    "        if not width_seq:\n",
    "            self.width = width\n",
    "            self.width_seq = [self.width] * (self.depth + 1)\n",
    "            self.shapes = [(self.width_seq[i-1], self.width_seq[i])  for i in range(1,self.depth)]\n",
    "            self.shapes += [(self.width_seq[-1], self.output_dim)]\n",
    "            self.shapes = [(self.input_dim, self.width_seq[0])] + self.shapes\n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        index = 0\n",
    "        n, d = x.shape\n",
    "        \n",
    "#         dim_bl =  sum([wx * wy + wy for wx, wy in self.shapes[:-1]])\n",
    "#         Θ[:dim_bl] = (Θ[:dim_bl] - Θ[:dim_bl].mean()) / Θ[:dim_bl].std()\n",
    "#         σ_Θ, μ_Θ = Θ.std(), Θ.mean()\n",
    "#         Θ = (Θ - μ_Θ) / σ_Θ\n",
    "\n",
    "        for wx, wy in self.shapes[:-1]:\n",
    "            x = F.linear(\n",
    "                x,\n",
    "                Θ[index: index + wx * wy].reshape(wy, wx),\n",
    "                Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy)\n",
    "            )\n",
    "            x = self.activation(x)\n",
    "            index += wx * wy  + wy\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[index: index + wx * wy].reshape(wy, wx), #* σ_Θ + μ_Θ,\n",
    "            Θ[index + wx * wy: index + wx * wy + wy].reshape(1,wy) # * σ_Θ + μ_Θ\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=1, output_dim=1, device=\"gpu\",\n",
    "    ):\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "\n",
    "        self.shapes = [(self.input_dim, self.output_dim)] \n",
    "        \n",
    "        self.dim = sum([wx * wy + wy for wx, wy in self.shapes])\n",
    "        \n",
    "    def forward(self, x, Θ):\n",
    "        wx, wy = self.shapes[-1]\n",
    "        x = F.linear(\n",
    "            x,\n",
    "            Θ[:wx * wy].reshape(wy, wx),\n",
    "            Θ[wx * wy: wx * wy + wy].reshape(1,wy)\n",
    "        )\n",
    "        return x.to(self.device)\n",
    "    \n",
    "    def map_forward(self, x, Θ):\n",
    "        preds_func = lambda θ: self.forward(x, θ)\n",
    "        batched_preds = torch._vmap_internals.vmap(preds_func)\n",
    "        preds = torch.hstack(list(map(preds_func, Θ)))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForwardNetBN_larger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=1, width=650, activation=torch.nn.Softplus):\n",
    "        super(SimpleForwardNetBN_larger, self).__init__()\n",
    "        \n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + 1, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, width), torch.nn.BatchNorm1d(width, affine=False), activation(),\n",
    "            torch.nn.Linear(width, input_dim )\n",
    "        )\n",
    "        \n",
    "        self.nn[-1].weight.data.fill_(0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "net = ClassificationNetwork(\n",
    "    dim,1, device=device, depth=1, width=30, activation=F.softplus\n",
    ")\n",
    "\n",
    "\n",
    "# net = LinearClassifier(\n",
    "#     dim,1, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "def gaussian_prior(Θ, σ_w=2.8):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -0.5 * (Θ**2).sum(axis=1) / σ_w\n",
    "\n",
    "def laplace_prior(Θ, σ_w=1):\n",
    "    \"\"\"\n",
    "    Logistic regresion bayesian prior\n",
    "    \"\"\"\n",
    "    return -(torch.abs(Θ) ).sum(axis=1) / σ_w\n",
    "\n",
    "\n",
    "def log_likelihood_vmap_nn(Θ, X, y, net=net):\n",
    "    \"\"\"\n",
    "    Hoping this implementation is less buggy / faster\n",
    "    \n",
    "    still feels a bit slow.\n",
    "    \"\"\"\n",
    "    pos_weights = torch.ones(X.shape[0], device=device)\n",
    "    \n",
    "    def loss(θ):\n",
    "        preds = net.forward(X, θ)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights, reduction=\"sum\")\n",
    "        ll_bcs = -1.0 * bce(preds.reshape(-1), y.reshape(-1))\n",
    "        return ll_bcs\n",
    "    \n",
    "    batched_loss =  torch._vmap_internals.vmap(loss)\n",
    "\n",
    "    return batched_loss(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3751, 100.0)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.dim, 1/Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e29cc372d541a8b17caf57a0ed2697"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-244-351a45896b77>:40: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816074788570404\n",
      "3.0133981704711914\n",
      "0.5571591854095459\n",
      "0.6471439003944397\n",
      "0.6257203221321106\n",
      "0.6001624464988708\n",
      "0.5495566129684448\n",
      "0.6284526586532593\n",
      "0.4882146418094635\n",
      "0.5149649977684021\n",
      "0.5171689987182617\n",
      "0.49882805347442627\n",
      "0.4705171287059784\n",
      "0.45915040373802185\n",
      "0.4422745406627655\n",
      "0.43776261806488037\n",
      "0.4341631531715393\n",
      "0.4229431748390198\n",
      "0.41461727023124695\n",
      "0.41823074221611023\n",
      "0.4118596911430359\n",
      "0.41221433877944946\n",
      "0.394908607006073\n",
      "0.3886967897415161\n",
      "0.3799595534801483\n",
      "0.3824955224990845\n",
      "0.382280558347702\n",
      "0.38810408115386963\n",
      "0.502236008644104\n",
      "0.6647531390190125\n",
      "0.4242742955684662\n",
      "1.0258734226226807\n",
      "0.4292885959148407\n",
      "0.5891388654708862\n",
      "0.42216867208480835\n",
      "0.814784824848175\n",
      "0.43518170714378357\n",
      "0.5274824500083923\n",
      "0.4142681956291199\n",
      "0.5380106568336487\n",
      "0.4420761466026306\n",
      "0.5162664651870728\n",
      "0.3972163200378418\n",
      "0.5325965285301208\n",
      "0.3837307095527649\n",
      "0.4121493995189667\n",
      "0.4135412871837616\n",
      "0.3895512521266937\n",
      "0.47040942311286926\n",
      "0.3790765404701233\n",
      "0.4065176546573639\n",
      "0.384374737739563\n",
      "0.3903485834598541\n",
      "0.387775182723999\n",
      "0.37708383798599243\n",
      "0.38730454444885254\n",
      "0.38512489199638367\n",
      "0.3697701096534729\n",
      "0.3667621612548828\n",
      "0.36977219581604004\n",
      "0.38189801573753357\n",
      "0.36546844244003296\n",
      "0.38058534264564514\n",
      "0.36167895793914795\n",
      "0.38534143567085266\n",
      "0.3669975996017456\n",
      "0.37722450494766235\n",
      "0.37139177322387695\n",
      "0.3940418064594269\n",
      "0.36021125316619873\n",
      "0.39776304364204407\n",
      "0.36972343921661377\n",
      "0.3824012577533722\n",
      "0.36554160714149475\n",
      "0.3759472370147705\n",
      "0.3685574233531952\n",
      "0.38814467191696167\n",
      "0.35791510343551636\n",
      "0.3808758556842804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "γ =  0.2\n",
    "Δt=0.01\n",
    "\n",
    "dim= net.dim\n",
    "\n",
    "prior = laplace_prior\n",
    "\n",
    "sde, losses = basic_batched_trainer(\n",
    "    γ, Δt, prior, log_likelihood_vmap_nn, dim, X_train, y_train,\n",
    "    method=\"euler\", stl=\"stl_xu\", adjoint=False, optimizer=None,\n",
    "    num_steps=79, batch_size_data=int(X_train.shape[0]), batch_size_Θ=30,\n",
    "    batchnorm=True, device=device, lr=0.001, drift=SimpleForwardNetBN_larger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8161),\n",
       " tensor(3.0134),\n",
       " tensor(0.5572),\n",
       " tensor(0.6471),\n",
       " tensor(0.6257),\n",
       " tensor(0.6002),\n",
       " tensor(0.5496),\n",
       " tensor(0.6285),\n",
       " tensor(0.4882),\n",
       " tensor(0.5150),\n",
       " tensor(0.5172),\n",
       " tensor(0.4988),\n",
       " tensor(0.4705),\n",
       " tensor(0.4592),\n",
       " tensor(0.4423),\n",
       " tensor(0.4378),\n",
       " tensor(0.4342),\n",
       " tensor(0.4229),\n",
       " tensor(0.4146),\n",
       " tensor(0.4182),\n",
       " tensor(0.4119),\n",
       " tensor(0.4122),\n",
       " tensor(0.3949),\n",
       " tensor(0.3887),\n",
       " tensor(0.3800),\n",
       " tensor(0.3825),\n",
       " tensor(0.3823),\n",
       " tensor(0.3881),\n",
       " tensor(0.5022),\n",
       " tensor(0.6648),\n",
       " tensor(0.4243),\n",
       " tensor(1.0259),\n",
       " tensor(0.4293),\n",
       " tensor(0.5891),\n",
       " tensor(0.4222),\n",
       " tensor(0.8148),\n",
       " tensor(0.4352),\n",
       " tensor(0.5275),\n",
       " tensor(0.4143),\n",
       " tensor(0.5380),\n",
       " tensor(0.4421),\n",
       " tensor(0.5163),\n",
       " tensor(0.3972),\n",
       " tensor(0.5326),\n",
       " tensor(0.3837),\n",
       " tensor(0.4121),\n",
       " tensor(0.4135),\n",
       " tensor(0.3896),\n",
       " tensor(0.4704),\n",
       " tensor(0.3791),\n",
       " tensor(0.4065),\n",
       " tensor(0.3844),\n",
       " tensor(0.3903),\n",
       " tensor(0.3878),\n",
       " tensor(0.3771),\n",
       " tensor(0.3873),\n",
       " tensor(0.3851),\n",
       " tensor(0.3698),\n",
       " tensor(0.3668),\n",
       " tensor(0.3698),\n",
       " tensor(0.3819),\n",
       " tensor(0.3655),\n",
       " tensor(0.3806),\n",
       " tensor(0.3617),\n",
       " tensor(0.3853),\n",
       " tensor(0.3670),\n",
       " tensor(0.3772),\n",
       " tensor(0.3714),\n",
       " tensor(0.3940),\n",
       " tensor(0.3602),\n",
       " tensor(0.3978),\n",
       " tensor(0.3697),\n",
       " tensor(0.3824),\n",
       " tensor(0.3655),\n",
       " tensor(0.3759),\n",
       " tensor(0.3686),\n",
       " tensor(0.3881),\n",
       " tensor(0.3579),\n",
       " tensor(0.3809)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5b39c5d3a0>]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAApzUlEQVR4nO3dd3hc5Zn38e89Td2S1Vxk2XLDFWyDcOxQYkwwZUnMJoTAJlnCsmE3L9kku7BvApslJNl3c+0mm7IJKQQIsJsACSUYYsAUA8EB23LvRrhIstV7sTTtef+YM6M2qshIZ879uS5dnjlzLD3SjH565n7KEWMMSiml7M813g1QSik1NjTQlVIqQWigK6VUgtBAV0qpBKGBrpRSCcIzXl84NzfXFBUVjdeXV0opW9qxY0edMSYv3mPjFuhFRUWUlJSM15dXSilbEpGTAz2mJRellEoQGuhKKZUgNNCVUipBaKArpVSC0EBXSqkEoYGulFIJYshAF5FkEdkmIntE5ICIfCvOOUki8oSIlIrIVhEpOiutVUopNaDh9NC7gLXGmGXAcuAqEVnV55xbgUZjzDzgh8B/jGkrB9HY7uf5vac/qC+nlFIT1pCBbiLarLte66PvJurrgUes208Cl4uIjFkrB/Hs7lN86be7aO4IfBBfTimlJqxh1dBFxC0iu4Ea4GVjzNY+pxQA5QDGmCDQDOTE+Ty3iUiJiJTU1ta+r4ZHdQbDAHQFQ2Py+ZRSyq6GFejGmJAxZjkwA1gpIktH88WMMfcbY4qNMcV5eXG3IhixgBXogbBeeUkp5WwjmuVijGkCNgNX9XnoFFAIICIeIBOoH4P2DckfsgLdCnallHKq4cxyyRORLOt2CnAFcLjPaRuAm63b1wOvmQ/oYqXRQA+GNdCVUs42nN0WpwGPiIibyB+A3xljnheRbwMlxpgNwIPA/4hIKdAA3HjWWtxHIBj5u+EPaslFKeVsQwa6MWYvsCLO8Xt63O4EPjW2TRuegPbQlVIKSICVotFAD4S0h66UcjbbB3psUDSkPXSllLPZPtCjPfOg9tCVUg5n/0APag9dKaUgEQJdSy5KKQUkQKD7dVBUKaWARAj0oE5bVEopSIBAj5Za/Lr0XynlcAkQ6NYsF92cSynlcAkQ6DooqpRSkACBroOiSikVYftA1x66UkpF2D/Qg9GVohroSilns3+ga8lFKaWABAh03ZxLKaUi7B/osYVF2kNXSjmb7QNdFxYppVSErQM9FDZEO+a69F8p5XS2DvSedfOAXlNUKeVwtg50f89A1x66UsrhbB3ogR51c522qJRyOnsHeo8Q14VFSimns3mgh+PeVkopJ7J1oPeqoWvJRSnlcLYOdO2hK6VUN1sHes/FREHtoSulHM7WgR7QaYtKKRVj60D3W4uJUn1uLbkopRxvyEAXkUIR2SwiB0XkgIh8Jc45a0SkWUR2Wx/3nJ3m9hYN8VSfW0suSinH8wzjnCBwhzFmp4hkADtE5GVjzME+5/3JGHPt2DdxYN2B7uk140UppZxoyB66MabSGLPTut0KHAIKznbDhkN76Eop1W1ENXQRKQJWAFvjPLxaRPaIyAsismSA/3+biJSISEltbe3IW9uHP6Q1dKWUihp2oItIOvAU8FVjTEufh3cCs4wxy4CfAH+I9zmMMfcbY4qNMcV5eXmjbHK36F4uaUkeXViklHK8YQW6iHiJhPlvjDFP933cGNNijGmzbm8EvCKSO6YtjaNnyUV76EoppxvOLBcBHgQOGWN+MMA5U63zEJGV1uetH8uGxtNzUFQ351JKOd1wZrlcBHwO2Cciu61jdwMzAYwxvwCuB74oIkHgDHCjMeas10C6gj176FpyUUo525CBbox5C5Ahzvkp8NOxatRwBXoOiobDGGOw3igopZTj2HqlaM+SizGRa4wqpZRT2T7QXQJJ3si3EdRAV0o5mK0D3R8K43W78Lkj34bOdFFKOZmtAz0QNPjcLjyuSN1cB0aVUk5m70APhfF6XHg9VslFe+hKKQezfaD73C68rsi3oRt0KaWczNaB7g+F8XoEjztSctENupRSTmbvQA9GBkW9OiiqlFL2DvRYycWtg6JKKWXzQDfaQ1dKKYvNAz2M1y143NGFRRroSinnsnWgd9fQxbqvJRellHPZOtADoTA+T3fJRXvoSikns3mgaw1dKaWibB7oYV36r5RSFlsHut9a+u+LLf3XQFdKOZetAz02yyXWQ9eSi1LKuWwd6P5gdGGR1tCVUsrWgd5/UFRLLkop57J3oPeZh67TFpVSTmbrQO/ebdHaPjeoga6Uci5bB3rfzbn0mqJKKSezbaCHwoawoXcNXXvoSikHs22gR2e0eHsuLNIeulLKwWwb6NHLzfk8LkQEr1t02qJSytFsG+jR8orPqp97XC69SLRSytFsG+j+HiWXyL+i89CVUo5m20APWHufdwe6S0suSilHs22gx3ronu5A1825lFJONmSgi0ihiGwWkYMickBEvhLnHBGR/xaRUhHZKyLnn53mdov2xmM1dB0UVUo5nGcY5wSBO4wxO0UkA9ghIi8bYw72OOdqYL718SHg59a/Z02gTw3d53bptEWllKMN2UM3xlQaY3Zat1uBQ0BBn9PWA4+aiHeALBGZNuat7aFvoHvcoguLlFKONqIauogUASuArX0eKgDKe9yvoH/oIyK3iUiJiJTU1taOsKm9+eMMiurmXEopJxt2oItIOvAU8FVjTMtovpgx5n5jTLExpjgvL280nyIm0GNhEYDH7cKvg6JKKQcbVqCLiJdImP/GGPN0nFNOAYU97s+wjp013YOi0Rq66MIipZSjDWeWiwAPAoeMMT8Y4LQNwF9bs11WAc3GmMoxbGc/sRq6p3ulqM5yUUo52XBmuVwEfA7YJyK7rWN3AzMBjDG/ADYC1wClQAdwy5i3tI+uYP9B0TMBLbkopZxryEA3xrwFyBDnGOD2sWrUcESX+ft6TlvUHrpSysFsu1I03rRFXSmqlHKyBAh0sf7VHrpSytlsG+jR64f23MsloPPQlVIOZttA71tD92rJRSnlcDYO9L41dC25KKWczdaB7nYJbut6opFZLtpDV0o5l20D3R8KxwZEATwu3T5XKeVstg30QNDEyi0QGRzVGrpSyslsG+j+UCg2IArgdQn+UJjIGiellHIe2wZ6vx66dTukF7lQSjmUfQM9FI5tzAWRWS6R4xroSilnsm2gRwZFe/bQI+Gui4uUUk5l20APhMK9a+jRHrpehk4p5VA2DnQTu1oRRDbnAghqDV0p5VA2DvS+JZfIbb/20JVSDmXbQPcHey8s8moPXSnlcLYN9IF66HpdUaWUU9k20P19BkU9LlfsuFJKOZFtA73vwiKfNSddl/8rpZzKvoEeCscubgHdPXTdoEsp5VS2DfS+uy16daWoUsrhbBvo/RcWSey4Uko5kY0DPf7mXEFd+q+Ucij7BnowHHelqD+oJRellDPZNtD7bs7l0x66UsrhbBvokRp6vO1zNdCVUs5ky0APhQ1hQ68euscVHRTVkotSyplsGejRDbh6zkOP1tO1h66UcqohA11EHhKRGhHZP8Dja0SkWUR2Wx/3jH0ze4su74/XQ9eVokopp/IM45yHgZ8Cjw5yzp+MMdeOSYuGIdoL71lD92oPXSnlcEP20I0xbwINH0Bbhi0Qp4fudelKUaWUs41VDX21iOwRkRdEZMlAJ4nIbSJSIiIltbW1o/5iAWuuebxriur2uUoppxqLQN8JzDLGLAN+AvxhoBONMfcbY4qNMcV5eXmj/oLRGnrPhUVuly79V0o52/sOdGNMizGmzbq9EfCKSO77btkg4pVcRASf20VAr1iklHKo9x3oIjJVRMS6vdL6nPXv9/MOJjYo6pFexz1uIaDXFFVKOdSQs1xE5DFgDZArIhXANwEvgDHmF8D1wBdFJAicAW40xpzVbnK8Hnr0vl5TVCnlVEMGujHmpiEe/ymRaY0fmK7gQIEuegk6pZRj2XKlaHRqYtweuga6Usqh7BnowejCot7N97hF56ErpRzLnoEeraH3GRT1ul06bVEp5Vi2DPR4e7lAZLWoBrpSyqlsGejRskq8kotuzqWUciqbBnr/laJglVx02qJSyqFsHejxpi3qwiKllFPZMtBjF7hw9x8U1WuKKqWcypaBPtA8dI/bhV9r6Eoph7JloPsHWCnqc4suLFJKOZYtAz0QCuN2SWzL3CiPTltUSjmYbQO9b/0cIpeh02mLSimnsmWg+0PhfuUWAK9LN+dSSjmXLQM9EAqT5IkT6G7toSulnMuegR40cXvokc25tIeulHImewb6QCUX3ZxLKeVgtgx0/0CDorp9rlLKwWwZ6IP10HWlqFLKqWwa6KbfxlwQWSkaCBnO8iVNlVJqQrJloPuDA09bBPRC0UopR7JnoA+ysAjQqYtKKUeyZaAPVEP3WD10XVyklHIi2wZ636sVQfcFL3SDLqWUE9kz0IMDDIq6Isd06qJSyonsGegDTluU2ONKKeU0tgz0ATfnckd76BroSinnsWWgB0JhfJ54K0WtGrpOW1RKOZBNA33gzbmg+4pGSinlJEMGuog8JCI1IrJ/gMdFRP5bREpFZK+InD/2zextoIVFPu2hJ4TmMwH+5+0TuuJXqREaTg/9YeCqQR6/GphvfdwG/Pz9N2twA9XQPToomhA27qvkX589wHu17ePdFKVsZchAN8a8CTQMcsp64FET8Q6QJSLTxqqBcdpjzUMfuIaugW5vda1dANRa/yqlhmcsaugFQHmP+xXWsX5E5DYRKRGRktra2lF9sVDYYAyDTlvUpf/2VtcWCfL6dg10pUbiAx0UNcbcb4wpNsYU5+XljepzRBcNxVtYpD30xFDX7o/8qz10pUZkLAL9FFDY4/4M69hZEd2nJf5eLrpSNBHUx3ro/nFuiVL2MhaBvgH4a2u2yyqg2RhTOQafN65o79sbt4eug6KJoL7N6qG3aQ9dqZHwDHWCiDwGrAFyRaQC+CbgBTDG/ALYCFwDlAIdwC1nq7HQHdaDDYrqVYvsLRrkdW3aQ1dqJIYMdGPMTUM8boDbx6xFQwgEI+WUQactBrXkYlfBUJjGjgCgPXSlRsp2K0UHq6FHFxYFtIduWw0d3b3yeu2hKzUi9gv04CCDotFA16X/tlXXGgnxgqwU7aErNUK2C/RYDT3u5lx6TVG7i849XzA1gw5/iA5/cJxbpJR92DbQB9s+Vy9BZ1/RMsuCqRm97iulhma7QPfHZrkMHOi6UtS+omWWhVaga9lFqeGzXaBHFw3Fm4fudgkiOg/dzura/PjcLopy0mL3lVLDY79ADw7cQ4dIL11XitpXfVsXOek+8jKSYveVUsNjv0AfpIYO4HUJQe2h21Z9u5+cdB/ZaT5gdCWXX7zxHr9689hYN02pCc92gT4zJ5VbLiqK/cL35fW4tORiY3VtXeSkJZHsdZOR7BlVyeV3JeX8fkf50CcqlWCGXCk60SyZnsmS6ZkDPu5xuQjotEXbqm/zMz8/MiCam5404h56OGyoaDiDSOS2y9V/eqtSicp2PfSheN2iC4tsyhhDXVsXuemRd1+56b4RT1usaunEHwrTFQxTo9vvKodJwEB36cIim2rrCtIVDJNjBXpOWtKIL3JR1tARu32iXi9hp5wl4QLd4xZdWGRT0d54bnpkhktuhm/ENfSegV5W3zHImUolnoQLdJ/bpbNcbCraG8+xAj0nLYnGDv+Ins/yhg7cLsHjEk42aA9dOYvtBkWH4nGLzkO3qVprY64cawZTbkYSxkR2YMzPSB7W5yhr6GB6VjIuEU5oD105TMIFemRhkfbQ7SjaQ48uKsq1gr2+bfiBfrK+g5nZqbhdLi25KMdJuJKL16WBblfRGvrk1O4eOoxscVF5Qwczs9OYlZ3Kifp2ItdfUcoZEq+H7hG6AhrodlTf1kVmiheftU9PTo8e+nC0dQWpb/czMzsVr1to7QzS1BFg8gCL0JRKNAnXQ/doD33CO+MP8e3nDlLT2tnreF2bPzZlEUbeQy+3ZrjMzE5llrW518kGLbso50i4QO+7OVdVcydbj9UT1rnpE8amg1U8tOU4z+2p7HU8sqgoKXY/I8mDz+0a9tTFsl6BngrASZ2Lrhwk8UoubiFoXVM0FDbc+sh2DpxuYVZOKn+1ciafKi4ccB8Y9cF4cX8VALvKGoHZseP17X7OmZIeuy8i5Kb7ht1Djw6CzsxOJckb6auc1IFR5SAJ3UP/XUk5B0638DcXzWZKRjLffeEwq777Knf+fg/HatuG/FwN7X7W/fANntlVcbab7Rhn/CFeP1ILwK6ypl6PRTfm6iknPWnYW+iWNXSQmeIlM9VLstfN1EnJGujKURKuhx6Zhx6muSPA9146wsqibP712kWICEeqWvnfd07yu5Jynt5ZwbXnTedLa+dxzpSMuJ/r3g0HOFrdxoNvHecvV8z4gL+TxPTG0VrOBEJctiCPzUdqqWnpJH9SMoFQmKaOQK+SC2D10IdfcpmZnRq7PzMnVUsuylESrofus+ah/+jVozR2+LnnY4sRiey4t2BqBt+5bilvfW0tX7h0Dq8cqmbdD9/km8/uJ9Snxv7i/io27DnNOVPS2X+qhcNVLePx7SScF/dXkpXq5e8/MheAXeVNADS2W4uK0nuXw0bSQy/vE+hFOak6KKocJeEC3eMWGtsDPPr2SW68cCZLC/pvtZuXkcRdVy9iy9fW8vkPF/HI2ye58/d7YkvMG9v9fOMP+1k8bRL/e+uH8LqFp3Zo2eX98gfDvHqohisWTWFZYRZet7DbCvRaK7Rz+wR6ZAtdf6/55PdtLmXdD9/oNdAdChsqGs9Q2CPQZ+WkUdvaRYc/eBa/K6UmjsQLdJcLfyhMms/NnevOGfTcyWk+7v34Eu5cdw7P7DrFVx7fTSAU5t7nDtDU4ef7n1pG/qRk1i7M55ldp3U65Pu05b06WruCXH3uVJK9bhZPm2QNjPbfmCsqN92HPxSmtSsSysYYntheztHqtljvHrq3ze1VcsmOznTRXrpyhoQL9OiilH+84pzYJk9D+dLa+XzjLxbxx32VXHffFp7dfZovrZ3H4umTALj+gkLq2rp482jtWWu3E7y4r4r0JA8XzcsFYHlhFnsrmgmGwv025oqKBnydtbf5gdMtsemJmw5Wxc7rOcMlKnqhaQ105RQJF+ir5+Zw7XnT+OyqWSP6f397yRy+s34JB063sGjaJP7Pmnmxx9YsyCMnzceTWnYZtWAozMuHqlm7MJ8kjxuAFTMn0+EPcbS6LdZD719Dt1aLWjX2F/ZX4nYJ583IZNOB6lgpJrqoKDr/HCKDogBlI9x18USdbhmg7GlYgS4iV4nIEREpFZGvx3n88yJSKyK7rY+/HfumDs9lC/L56V+dP+BFpAfzudVFPPn3q3nklgtjPX2ITIVcv7yAVw/VxAbv1MhsO9FAQ7ufq5dOjR1bMTMLgF3ljdS2deHzuMhI6j3xKjqNsa61C2MMG/dVsXpODp8qLuR4XTulNZHpp2XWtrnTMrs38cpM8ZKV6u236+LmwzWcajoTt517yptY8/3XefCt46P6Pl/YV8lXH9+lfxDUuBgy9UTEDdwHXA0sBm4SkcVxTn3CGLPc+nhgjNv5gSkuyiZ/Uv+d/a6/YAb+UJjn9p6OHTvVdIYfv/Iu9244wNee3Ms/PLaLLz+2i6d3VtDWpQNxPb20v4pkr4uPLMiLHZuZnUp2mo/dZU3Ut/nJTfPFZiRF5WZEeuh17X6OVLdyvK6dq8+dyhWLpgCw6WA1EAn0gqwUPH3+kM/KSeu16+L+U838zSPbufvpfXHb+fCfTwDwk9dKae4I9Hu8svkMP3u9lK5gqN9jxhh++MpR/rD7NHsrmof6kSg15oYzD30lUGqMOQYgIo8D64GDZ7NhE83i6ZNYPG0ST+6oYN3iqdy3uZTHt5cRDBvSkzyk+tyk+jx0+INs2HOaJM8+PrpoCtetKODyhfmOvlhxOGx46UA1Hzknj1Rf90tORFhRmMWu8iYKJ6fEHfPITvUhEumhb9xXhUtg3eKp5GUksbwwi00Hqrj9snmc7DNlMWpWdio7rYFXYwzffv4gxkTmwx883RIbJwGobe3ij3sruWR+Lm+V1vGz10u565pFsceDoTBf+u0udpxsJCfNx6cvnNnra+0ub+JodeQdw7O7T7OsMOt9/dyUGqnh1CUKgPIe9yusY319UkT2isiTIlIY7xOJyG0iUiIiJbW19htg/OQFM9hb0cyl39vMY9vKuP6CQt762lr23XslW+/+KJvvXMM7d13OU19czY0XFvLOsXq+8GgJ6+/bwpbSuvFu/rh57XANVS2dXHve9H6PrZiZRWlNG8fq2vtNWQTwuF1MTvVR397FC/sqWTk7O7Zf+rolU9hT0Uxl8xnKGzp6TVmMKspJ5XTTGfzBMC8dqGLb8Qb++coFpPnc/PLN93qd+/i2MvyhMPd+fAmfPH8Gv/7zCSoau3v3P3v9PXacbCQzxctDb53oV1b5XUk5KV43F8/L5bm9p/utbVDqbBurQdHngCJjzHnAy8Aj8U4yxtxvjCk2xhTn5eXFO2VCu275dObkpbF+2XReu2MN3/3EuRRkpfQ6R0S4YFY231q/lK13X84PblhGQ7ufzzywlc89uJUdJxsd94v+yzffoyArhat61M+jlhdOBiIzUQaalZST5uOdYw28W9PGNedOix2/cknk8z298xQN7f5eA6JRM3PSCBs4XtfOv288zIIpGfzdpXO4aeVMnt9bGRtMDYTC/GZrGZfMz2VuXjr/dMU5CPCDTUeBSO/7x6++y/rl0/nGXyziSHUrW0rrY1+nwx/kuT2VXHPuND7zoZnUtnbx9nv1/dozUg3tfq3Hq2EbTqCfAnr2uGdYx2KMMfXGmOhyvgeAC8ameRNLTnoSr92xhu99allsBsVgPG4Xnzh/Bq/e8RG+8ReL2HeqmU/+/M+ce+9L3Hj/2/zHi4fZfLgmbj02Uew42cj2E43cevHsuAPV5xVmEi2b953hEpWbnkRpTRsi3SEOMDcvnbl5aTz69gmA+CUX63n69vMHKGvo4BvXLsLjdnHrJbMRiA1+bjpQTVVLJzevLgJgelYKt1w0m2d2n6LkRANffXwXUycl8+31S/nYsunkpvt48K1jsa/zx72VtHUF+fSFhVy2MJ+MJA/P7j7VtzkjcriqhVXffZV/33jofX0e5RzDCfTtwHwRmS0iPuBGYEPPE0RkWo+7Hwf0FdhDstfN314yhzf++TJ+9Onl3FBcyBl/iF+9eYxbHt7OBd95hS8/touN+yqpb+tKqK1+73/zPTJTvHz6wrhVOCYle5mfH9lhMW+gHroV9MWzJjOlz4D1uiVTqW6J9CUGC/QtpfWsXZjPJfMj7wynZaawfnkBj28vo6HdzyNvn6AwO4XLFubH/u8X18wlM8XLZx7YysmGDv7rhmVkpkQ2/vrsqllsPlIbm2XzxPZy5uSmcWHRZJK9bq5cOpUX91fRGRjdH+tQ2PC1J/fiD4b59ZYTsa+j1GCGHBQ1xgRF5EvAS4AbeMgYc0BEvg2UGGM2AF8WkY8DQaAB+PxZbLNtZaZ4uW5FAdetiAxBdAZCvHOsnhf3V7HpYDUb9kRm0LhdwuRUH7npPian+mLT7zJTvMzOTWP5zCzm52fgnuADrcdq29h0sJrb18wjLWngl9qKwskcrW4btIcOcPXSaf0eu3LJVH7+eqQWHq+GnpeeRKrPjT8Y5u4eA5wAf/eROTy1s4Jv/GEf2443cPc1C3v9TDNTvPzD2vl85/mDfHHNXFbNyYk99tlVs/jZ6+/x6y3HueWi2ZScbOTrVy+MzdJZv3w6T+6o4PUjNVwVp91D+fWW4+ypaObejy3mvzYd5f/98SC/vmXliD+PcpZh7bZojNkIbOxz7J4et+8C7hrbpiW+ZK+bNQvyWbMgn3+7Lsz2E40cqWqhvt1PXZufurYumjsCHKtro6kjQNOZAP5gZPuBNJ+bc2dksnDqJObmpzMvL505eWkEw4amDj/NZwK0d4XITfdRMDmFvPSkflMCz7Zf/ek4XreLmz9cNOh5K2Zm8URJeb+tc6MKslJwCXFr8OcVZDJlUhKdgTCZKd5+j4sI1543jcLJqczLT+/12DlTMrh8YT4b90WmVN5Q3P9dxOc/XMTCqRl8aHZ2r+O56Ulct3w6T+2sIBgyuF3CJ87vniuwek4OuelJPLv79IgD/WR9O9/fdISPLprCzR8uwh8K8+8bD/P6kRrWLMgf+hMox0q47XPtyuN2sXpuDqvn5gx4jjGGE/Ud7C5vZHdZE7srmvl9STnt/qHf1vs8LqZMSsItQthA2BhSvG4uW5jPNedOY9mMzDEN/JrWTp7aWcH1F8yIzUoZyJVLprKrrInzZ02O+/hffWgmq+fmML3PADSAyyXcduncXrNR+vrP65cN+Njfr5nLq4druG55AVmp/d8huF0S26qgr7+5eDa/K6ngiZJyrlg8hfyM7nKQx+3i2vOm8dttZbR0BpiU7OVodSvff+kI+0414w+GIx+hMIunT+ILl8zhyiVTcQl8/al9eF0u/u26pYgIn//wbH67tYzvPH+Qi+blxsYiGtv9vHa4hlVzc/oNzitnkvEaQS8uLjYlJSXj8rUTiTGGyuZOSmvaOFnfjtftssozPtKS3NS2dnGq6QynGs9Q3dKJAVwiiETmXb9zrJ5AyFCQlcJlC/OYlZ3G9KwUCianxBb+jKZN33vpCD9/4z1eu2MNs3PTxv4bHyPGGP64r5LVc3KGvfdPT599YCtvldbx4M3FXG4tdoraVdbIX/7sz9y57hzKGjp4ckcFaUkerlg8hRSvG5/HhUuEVw5Vc7K+g1k5qVxYlM2TOyr47ifO5aaV3fPcXz5YzRceLeGbH1vMp4oLefBPx/nVn47R1hXEJXDF4incvLqI1XNzEBHau4JUNp+hsSNAitdNWpKHtCQ3mSne2NYL8X4WxmD7NRP1bV28sL+K5YVZcXdb3XGygR+98i5XLZ3KDcWFo1pVPphQ2OASzto7YhHZYYwpjvuYBrqzNXcEePlQNS/sq2Tr8YZ+K1xz0nzMy0/nnCkZZKf56PAHaesK0eEPEgwZEBAiL96mDj+VzZ1UNp2h3R/iyiVT+OXn4r7uEsbeiiYe21bGd9Yv7bdK1RjDmu+/zsn6DnxuF3+9eha3XzaPyX3+SIbChpcOVPHLN4+xp7yJVXOyeewLq3oFgjGGzz24jT3lTXg9Lhra/Vy5JFKSeevdOh7fXk5Du58pk5I44w/R0hl/pbLbJczPT2fZjCzOnZFJRrKHA6db2H+qmf2nmmnpDJLkcZHqc5PidTM3P501C/JZuzB/WH+Yw2FDaW0b2080cLSqleUzs1i7YAqZqd3lsPauIFtK6zhR387ahVP6lcL2lDfxwFvHOd10hquWTOXaZdOYltn9DqQrGOLd6jb8oTDTM1PIy0jC7RIOnG7m4S0neHbPafzBMC6BWy+ezT9dsYAUn5tgKMxPXivlJ6+9S5LHzZlAiNm5ady5bgHXnDuVQMiwt6KJrccbqGg8w8XzclmzIK/f+E/zmQDNHQGmZyXHnnNjDHsqmnlyRznP7akkI9nDv1yziKuWTu31PNa1dfGbd8ooLpo84Du/oWigq2ExxtDSGeS01aM/UR/ZK+VodSvvVrfR2hWMrYhNS3LjcQkGwIABJqV4mTYpmWlZyUzPTOGG4sJev8hOtHFfJX9+r46/u3Ru3EHbnowxHDjdQmF2atzxgCNVrVx33xaKiyZz57oFvVaidgZC/HFvJZuP1JCd5mNaZgrTs5KZnOqjMxCi3fpDXN3cyd5TzeyraKLR2trA53GxaGoGSwoyyUtPojMQosMf+T97ypt4rzayudns3DTm56eTkewlI9lDRrIHfzBMS2eQ1s4ATR0B9lY0xf6Y+Dwu/MEwHpewak4O58/MYkdZI9uON/S6kPt5MzK5bnkB07OSeWjLCbYdbyAjyUNhdioHK1sQgZVF2czMTuXA6RaOVrcS7DETzO0SstN81LZ2keJ188kLCrj+gkKe2F7OY9vKmJmdyj9fuYCH/3yCHScb+cSKAr61fglbjzXwny8d5mh1G4XZKdS0dNFljVGlJ3lo6wri87i4dH4e5xZkcrS6lX2nmmO7fXpcwozJKczKSeNU0xlKa9pI8rhYt2Qq71a3criqlVVzsrnn2iWIwENvHY/9sfmHtfO4Y92CUbyiNNDVGEiUt+N25w+Ge20cN1rGRC4I0u4PMjcvfdCyQ1l9B68druaNo7VUNnfS2hmkpTMQCTy3Kxbwk5I9LJo2ieKibIpnTWZmdip7KprYdLCalw5Ucay2nXOmpHOZNRFgVk4qL+yv4pldFew/FbkiWEFWCrdcVMSnLywkI9nL8bp2Nuw+zXN7T9PY7mfx9EksLchkyfRJpPrcnG7qpKq5k8rmThZOzejXiXjnWD1ff2ovJ+o7yEjy8G9/uZT1y7sHr0NhwzO7TvHs7lOcMyWDlbOzubAom8wULyUnGnhhfxUvHaiisrmTGZNTOLcgk6UFmeSm+yhviHR6TtS3k+rz8IkVBVxz3jQmJXsJhsI8tr2cH2w6QtOZAMYQ+2Pz+Q/P7veuZCQ00JVSY84YM6I6cVtXkPQBpq++W93KqaZImaNv6er96gyEeGpnBZfOzxvyXVI8xhja/aEB2z6Y5o4AD205TlqSmxuKC+MOvI+UBrpSSiWIwQI94S5woZRSTqWBrpRSCUIDXSmlEoQGulJKJQgNdKWUShAa6EoplSA00JVSKkFooCulVIIYt4VFIlILnBzlf88FJupVl7VtozOR2wYTu33attGxa9tmGWPiXpR53AL9/RCRkoFWSo03bdvoTOS2wcRun7ZtdBKxbVpyUUqpBKGBrpRSCcKugX7/eDdgENq20ZnIbYOJ3T5t2+gkXNtsWUNXSinVn1176EoppfrQQFdKqQRhu0AXkatE5IiIlIrI18e5LQ+JSI2I7O9xLFtEXhaRd61/J49T2wpFZLOIHBSRAyLylYnSPhFJFpFtIrLHatu3rOOzRWSr9dw+ISLv//Iuo2+jW0R2icjzE6ltInJCRPaJyG4RKbGOjftzarUjS0SeFJHDInJIRFZPhLaJyALr5xX9aBGRr06Etlnt+0fr92C/iDxm/X6M6vVmq0AXETdwH3A1sBi4SUQWj2OTHgau6nPs68Crxpj5wKvW/fEQBO4wxiwGVgG3Wz+ridC+LmCtMWYZsBy4SkRWAf8B/NAYMw9oBG4dh7ZFfQU41OP+RGrbZcaY5T3mKU+E5xTgx8CLxpiFwDIiP79xb5sx5oj181oOXAB0AM9MhLaJSAHwZaDYGLMUcAM3MtrXW+Tiv/b4AFYDL/W4fxdw1zi3qQjY3+P+EWCadXsacGS8f25WW54Frpho7QNSgZ3Ah4isjPPEe64/4DbNIPILvhZ4HpAJ1LYTQG6fY+P+nAKZwHGsiRYTqW192rMO2DJR2gYUAOVANuCxXm9Xjvb1ZqseOt3ffFSFdWwimWKMqbRuVwFTxrMxACJSBKwAtjJB2meVNHYDNcDLwHtAkzEmaJ0yns/tj4D/C4St+zlMnLYZYJOI7BCR26xjE+E5nQ3UAr+2SlUPiEjaBGlbTzcCj1m3x71txphTwPeBMqASaAZ2MMrXm90C3VZM5M/ruM4LFZF04Cngq8aYlp6PjWf7jDEhE3kLPANYCSwcj3b0JSLXAjXGmB3j3ZYBXGyMOZ9I2fF2Ebm054Pj+Jx6gPOBnxtjVgDt9ClhjPfvg1WH/jjw+76PjVfbrLr9eiJ/EKcDafQv4w6b3QL9FFDY4/4M69hEUi0i0wCsf2vGqyEi4iUS5r8xxjw90doHYIxpAjYTeVuZJSIe66Hxem4vAj4uIieAx4mUXX48QdoW7dFhjKkhUgdeycR4TiuACmPMVuv+k0QCfiK0LepqYKcxptq6PxHa9lHguDGm1hgTAJ4m8hoc1evNboG+HZhvjQD7iLx92jDObeprA3CzdftmIrXrD5yICPAgcMgY84MeD417+0QkT0SyrNspRGr7h4gE+/Xj2TZjzF3GmBnGmCIir6/XjDGfmQhtE5E0EcmI3iZSD97PBHhOjTFVQLmILLAOXQ4cnAht6+EmusstMDHaVgasEpFU63c2+nMb3ettPAcoRjmIcA1wlEjN9V/GuS2PEal7BYj0UG4lUm99FXgXeAXIHqe2XUzkLeReYLf1cc1EaB9wHrDLatt+4B7r+BxgG1BK5G1x0jg/v2uA5ydK26w27LE+DkRf/xPhObXasRwosZ7XPwCTJ1Db0oB6ILPHsYnStm8Bh63fhf8Bkkb7etOl/0oplSDsVnJRSik1AA10pZRKEBroSimVIDTQlVIqQWigK6VUgtBAV0qpBKGBrpRSCeL/A6MKKFkYyA8NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32561, 123])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size = int(math.ceil(1.0/Δt))\n",
    "ts = torch.linspace(0, 1, t_size).to(device)\n",
    "no_posterior_samples = 100\n",
    "Θ_0 = torch.zeros((no_posterior_samples, net.dim)).to(device)\n",
    "\n",
    "Θ_1 = torchsde.sdeint(sde, Θ_0, ts, dt=Δt)[-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  4.,  6., 13., 23., 17., 20.,  7.,  3.,  5.]),\n",
       " array([-0.585412  , -0.47058666, -0.35576126, -0.24093589, -0.12611051,\n",
       "        -0.01128513,  0.10354026,  0.21836564,  0.333191  ,  0.4480164 ,\n",
       "         0.5628418 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "\n",
    "ax1.hist(Θ_1[:,0].cpu().detach().numpy())\n",
    "ax2.hist(Θ_1[:,1].cpu().detach().numpy())\n",
    "ax3.hist(Θ_1[:,2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predc(X, Θ):\n",
    "    return torch.vstack([torch.sigmoid(net.forward(X, θ)[None,...]) for θ in Θ]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predc(X_train, Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8438, device='cuda:0')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "((pred > 0.5).float().flatten()== y_train).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predc(X_test.float(), Θ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8461, device='cuda:0')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_test > 0.5).float().flatten() == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Baseline\n",
    "\n",
    "We run the point estimate approximation (Maximum a posteriori) to double check what the learned weights look like.  We get the  exact same training accuracy as with the controlled model and similarly large weights for the non bias weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc7ed322024c32800b99807fcd8cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vargf\\AppData\\Local\\Temp/ipykernel_2136/381526447.py:36: UserWarning: torch.vmap is an experimental prototype that is subject to change and/or deletion. Please use at your own risk. There may be unexpected performance cliffs due to certain operators not being implemented. To see detailed performance warnings please use `torch._C._debug_only_display_vmap_fallback_warnings(True) before the call to `vmap`.\n",
      "  batched_loss =  torch._vmap_internals.vmap(loss_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45177459716797\n",
      "52.67886734008789\n",
      "50.03776931762695\n",
      "47.52959442138672\n",
      "45.154483795166016\n",
      "42.91158676147461\n",
      "40.79913330078125\n",
      "38.81449508666992\n",
      "36.95428466796875\n",
      "35.21445083618164\n",
      "33.590423583984375\n",
      "32.07719421386719\n",
      "30.66946029663086\n",
      "29.36173439025879\n",
      "28.148420333862305\n",
      "27.023902893066406\n",
      "25.982643127441406\n",
      "25.019207000732422\n",
      "24.128328323364258\n",
      "23.30495262145996\n",
      "22.54424476623535\n",
      "21.841611862182617\n",
      "21.192726135253906\n",
      "20.593515396118164\n",
      "20.040164947509766\n",
      "19.529117584228516\n",
      "19.057064056396484\n",
      "18.620927810668945\n",
      "18.21786880493164\n",
      "17.845247268676758\n",
      "17.500642776489258\n",
      "17.181812286376953\n",
      "16.886699676513672\n",
      "16.613401412963867\n",
      "16.360179901123047\n",
      "16.12543487548828\n",
      "15.907696723937988\n",
      "15.705615997314453\n",
      "15.517951011657715\n",
      "15.343571662902832\n",
      "15.181438446044922\n",
      "15.030595779418945\n",
      "14.890161514282227\n",
      "14.759340286254883\n",
      "14.637393951416016\n",
      "14.523643493652344\n",
      "14.417470932006836\n",
      "14.318305969238281\n",
      "14.225625991821289\n",
      "14.13895034790039\n",
      "14.057838439941406\n",
      "13.981886863708496\n",
      "13.910720825195312\n",
      "13.843996047973633\n",
      "13.781397819519043\n",
      "13.722637176513672\n",
      "13.66744327545166\n",
      "13.61557388305664\n",
      "13.566798210144043\n",
      "13.520904541015625\n",
      "13.477703094482422\n",
      "13.437012672424316\n",
      "13.398666381835938\n",
      "13.362512588500977\n",
      "13.328409194946289\n",
      "13.296223640441895\n",
      "13.265838623046875\n",
      "13.23713493347168\n",
      "13.210012435913086\n",
      "13.184370040893555\n",
      "13.160122871398926\n",
      "13.13718318939209\n",
      "13.115469932556152\n",
      "13.094917297363281\n",
      "13.07545280456543\n",
      "13.057011604309082\n",
      "13.039539337158203\n",
      "13.022977828979492\n",
      "13.00727653503418\n",
      "12.99238395690918\n",
      "12.978260040283203\n",
      "12.964859008789062\n",
      "12.952140808105469\n",
      "12.940072059631348\n",
      "12.92861557006836\n",
      "12.91773796081543\n",
      "12.907407760620117\n",
      "12.897597312927246\n",
      "12.88828182220459\n",
      "12.879429817199707\n",
      "12.87102222442627\n",
      "12.863033294677734\n",
      "12.855443000793457\n",
      "12.848231315612793\n",
      "12.841375350952148\n",
      "12.834863662719727\n",
      "12.8286714553833\n",
      "12.822789192199707\n",
      "12.81719970703125\n",
      "12.81188678741455\n",
      "12.80683708190918\n",
      "12.802037239074707\n",
      "12.797475814819336\n",
      "12.793142318725586\n",
      "12.78902530670166\n",
      "12.785109519958496\n",
      "12.781390190124512\n",
      "12.777854919433594\n",
      "12.774497032165527\n",
      "12.771309852600098\n",
      "12.768280029296875\n",
      "12.765399932861328\n",
      "12.762666702270508\n",
      "12.76007080078125\n",
      "12.757606506347656\n",
      "12.755267143249512\n",
      "12.753044128417969\n",
      "12.750932693481445\n",
      "12.748931884765625\n",
      "12.747032165527344\n",
      "12.745229721069336\n",
      "12.743517875671387\n",
      "12.741894721984863\n",
      "12.740355491638184\n",
      "12.738895416259766\n",
      "12.737510681152344\n",
      "12.736200332641602\n",
      "12.734954833984375\n",
      "12.73377513885498\n",
      "12.732657432556152\n",
      "12.731599807739258\n",
      "12.730597496032715\n",
      "12.729646682739258\n",
      "12.72874641418457\n",
      "12.727895736694336\n",
      "12.727088928222656\n",
      "12.726325988769531\n",
      "12.725605964660645\n",
      "12.724923133850098\n",
      "12.72427749633789\n",
      "12.723665237426758\n",
      "12.723087310791016\n",
      "12.722541809082031\n",
      "12.722025871276855\n",
      "12.721537590026855\n",
      "12.721076965332031\n",
      "12.720643997192383\n",
      "12.720233917236328\n",
      "12.719844818115234\n",
      "12.719480514526367\n",
      "12.719133377075195\n",
      "12.71881103515625\n",
      "12.718502044677734\n",
      "12.718212127685547\n",
      "12.717939376831055\n",
      "12.717683792114258\n",
      "12.717440605163574\n",
      "12.717211723327637\n",
      "12.716997146606445\n",
      "12.71679401397705\n",
      "12.716605186462402\n",
      "12.716425895690918\n",
      "12.716257095336914\n",
      "12.71609878540039\n",
      "12.715950012207031\n",
      "12.715808868408203\n",
      "12.715678215026855\n",
      "12.715555191040039\n",
      "12.715438842773438\n",
      "12.715330123901367\n",
      "12.715227127075195\n",
      "12.715131759643555\n",
      "12.715041160583496\n",
      "12.714957237243652\n",
      "12.71487808227539\n",
      "12.714803695678711\n",
      "12.714734077453613\n",
      "12.714668273925781\n",
      "12.714609146118164\n",
      "12.71455192565918\n",
      "12.714496612548828\n",
      "12.714447021484375\n",
      "12.714401245117188\n",
      "12.714357376098633\n",
      "12.714315414428711\n",
      "12.714277267456055\n",
      "12.714242935180664\n",
      "12.714208602905273\n",
      "12.714178085327148\n",
      "12.714149475097656\n",
      "12.714122772216797\n",
      "12.714097023010254\n",
      "12.714075088500977\n",
      "12.7140531539917\n",
      "12.714031219482422\n",
      "12.71401309967041\n",
      "12.713996887207031\n",
      "12.713979721069336\n",
      "12.713964462280273\n",
      "12.713949203491211\n",
      "12.713935852050781\n",
      "12.713923454284668\n",
      "12.713912963867188\n",
      "12.71390151977539\n",
      "12.713892936706543\n",
      "12.713883399963379\n",
      "12.713874816894531\n",
      "12.7138671875\n",
      "12.713861465454102\n",
      "12.71385383605957\n",
      "12.713848114013672\n",
      "12.71384048461914\n",
      "12.713836669921875\n",
      "12.713830947875977\n",
      "12.713824272155762\n",
      "12.713821411132812\n",
      "12.713817596435547\n",
      "12.713814735412598\n",
      "12.713810920715332\n",
      "12.713809967041016\n",
      "12.713804244995117\n",
      "12.7138032913208\n",
      "12.713800430297852\n",
      "12.713799476623535\n",
      "12.713797569274902\n",
      "12.713794708251953\n",
      "12.71379280090332\n",
      "12.713790893554688\n",
      "12.713790893554688\n",
      "12.713788986206055\n",
      "12.713787078857422\n",
      "12.713786125183105\n",
      "12.713785171508789\n",
      "12.713784217834473\n",
      "12.713783264160156\n",
      "12.713784217834473\n",
      "12.713781356811523\n",
      "12.713783264160156\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713780403137207\n",
      "12.71377944946289\n",
      "12.713780403137207\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713778495788574\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713776588439941\n",
      "12.713777542114258\n",
      "12.713777542114258\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713773727416992\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713773727416992\n",
      "12.713774681091309\n",
      "12.713773727416992\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713774681091309\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713776588439941\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n",
      "12.713775634765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor([[ 0.0000,  2.4463, -2.1438]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ_map = torch.zeros((1, dim), requires_grad=True, device=device)\n",
    "optimizer_map = torch.optim.Adam([Θ_map], lr=0.05)\n",
    "#     optimizer = torch.optim.LBFGS(gpr.parameters(), lr=0.01)\n",
    "\n",
    "losses_map = []\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    if isinstance(optimizer_map, torch.optim.LBFGS):\n",
    "        def closure_map():\n",
    "            loss_map = log_likelihood_vmap()\n",
    "            optimizer_map.zero_grad()\n",
    "            loss_map.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_map.step(closure_map)\n",
    "        losses_map.append(closure_map().item())\n",
    "    else:\n",
    "        loss_map = -(log_likelihood_vmap(Θ_map, X_train, y_train) + gaussian_prior(Θ_map))\n",
    "        optimizer_map.zero_grad()\n",
    "        loss_map.backward()\n",
    "        print(loss_map.item())\n",
    "        optimizer_map.step()\n",
    "        losses_map.append(loss_map.item())\n",
    "\n",
    "Θ_map\n",
    "pred_map = torch.sigmoid(X_train.mm(Θ_map.T)).mean(axis=1)\n",
    "((pred_map < 0.5).float() == y_train).float().mean(), Θ_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
